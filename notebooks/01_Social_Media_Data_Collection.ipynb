{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Social Media Data Collection\n",
    "## Retail Investor Sentiment from Reddit r/WallStreetBets\n",
    "\n",
    "---\n",
    "\n",
    "**Research Project:** Retail Sentiment, Earnings Quality, and Stock Returns\n",
    "\n",
    "**Purpose:** Collect and structure social media posts from Reddit's r/WallStreetBets and related retail investor forums for the period 2018-2023.\n",
    "\n",
    "**Data Sources:**\n",
    "- Reddit API (PRAW)\n",
    "- Pushshift.io Archives (Academic Reddit Data)\n",
    "- BigQuery Reddit Dataset\n",
    "\n",
    "**Output:** Post-level table with `[platform_post_id, ticker, date, time, text, score, author, subreddit]`\n",
    "\n",
    "---\n",
    "\n",
    "**Citation:** If using this code, please cite appropriately.\n",
    "\n",
    "**Last Updated:** 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# INSTALL REQUIRED PACKAGES\n",
    "# =============================================================================\n",
    "# Run this cell once to install all dependencies\n",
    "\n",
    "!pip install praw==7.7.1\n",
    "!pip install pmaw==3.0.0\n",
    "!pip install pandas==2.0.3\n",
    "!pip install numpy==1.24.3\n",
    "!pip install tqdm==4.66.1\n",
    "!pip install requests==2.31.0\n",
    "!pip install pyarrow==14.0.1\n",
    "!pip install google-cloud-bigquery==3.13.0\n",
    "!pip install db-dtypes==1.2.0\n",
    "\n",
    "print(\"All packages installed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORT LIBRARIES\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Set, Optional, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "\n",
    "# Reddit API\n",
    "import praw\n",
    "from pmaw import PushshiftAPI\n",
    "\n",
    "# Google BigQuery (for large-scale Reddit data)\n",
    "from google.cloud import bigquery\n",
    "from google.colab import auth\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(f\"Environment setup complete. Timestamp: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Parameters\n",
    "\n",
    "### 2.1 Research Design Parameters\n",
    "\n",
    "Following established literature on social media and asset pricing (e.g., Cookson & Niessner, 2020; Bradley et al., 2021), we define our sample parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RESEARCH PARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "class ResearchConfig:\n",
    "    \"\"\"Configuration class for research parameters.\n",
    "    \n",
    "    This centralizes all parameters for reproducibility and sensitivity analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sample Period\n",
    "    START_DATE = \"2018-01-01\"\n",
    "    END_DATE = \"2023-12-31\"\n",
    "    \n",
    "    # Target Subreddits (Primary and Extension)\n",
    "    PRIMARY_SUBREDDITS = [\"wallstreetbets\"]\n",
    "    EXTENSION_SUBREDDITS = [\"stocks\", \"investing\", \"options\"]  # Optional\n",
    "    \n",
    "    # Minimum thresholds for data quality\n",
    "    MIN_POST_LENGTH = 10  # Characters\n",
    "    MIN_SCORE_THRESHOLD = -100  # Include downvoted but not spam\n",
    "    \n",
    "    # Data storage paths (Google Drive mount)\n",
    "    BASE_PATH = \"/content/drive/MyDrive/Research/RetailSentiment/\"\n",
    "    RAW_DATA_PATH = BASE_PATH + \"data/raw/\"\n",
    "    PROCESSED_DATA_PATH = BASE_PATH + \"data/processed/\"\n",
    "    \n",
    "    # API Rate Limits\n",
    "    REDDIT_RATE_LIMIT = 60  # Requests per minute\n",
    "    SLEEP_BETWEEN_REQUESTS = 1.0  # Seconds\n",
    "    \n",
    "    @classmethod\n",
    "    def print_config(cls):\n",
    "        \"\"\"Print current configuration for documentation.\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"RESEARCH CONFIGURATION\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Sample Period: {cls.START_DATE} to {cls.END_DATE}\")\n",
    "        print(f\"Primary Subreddits: {cls.PRIMARY_SUBREDDITS}\")\n",
    "        print(f\"Extension Subreddits: {cls.EXTENSION_SUBREDDITS}\")\n",
    "        print(f\"Minimum Post Length: {cls.MIN_POST_LENGTH} chars\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "config = ResearchConfig()\n",
    "config.print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Mount Google Drive for Data Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MOUNT GOOGLE DRIVE\n",
    "# =============================================================================\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directory structure\n",
    "os.makedirs(config.RAW_DATA_PATH, exist_ok=True)\n",
    "os.makedirs(config.PROCESSED_DATA_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"Data directories created at: {config.BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ticker Universe Construction\n",
    "\n",
    "### 3.1 Build Comprehensive Ticker Dictionary\n",
    "\n",
    "We construct a ticker universe based on S&P 500 and Russell 1000 constituents, with careful handling of:\n",
    "- Ambiguous tickers (common words)\n",
    "- Ticker changes over time\n",
    "- Multiple share classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TICKER UNIVERSE CONSTRUCTION\n",
    "# =============================================================================\n",
    "\n",
    "class TickerUniverse:\n",
    "    \"\"\"Manages the ticker universe for text-to-ticker mapping.\n",
    "    \n",
    "    Following Cookson, Engelberg, and Mullins (2020), we carefully construct\n",
    "    a ticker dictionary that excludes ambiguous symbols.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ambiguous tickers to exclude (common words/abbreviations)\n",
    "    AMBIGUOUS_TICKERS = {\n",
    "        'IT', 'ALL', 'A', 'AN', 'ARE', 'AT', 'BE', 'BY', 'FOR', 'HAS',\n",
    "        'HE', 'HIM', 'HIS', 'HOW', 'IF', 'IN', 'IS', 'IT', 'ITS', 'ME',\n",
    "        'MY', 'NEW', 'NOW', 'OF', 'ON', 'ONE', 'OR', 'OUR', 'OUT', 'SO',\n",
    "        'THE', 'TO', 'UP', 'US', 'WAS', 'WE', 'WHO', 'YOU', 'DD', 'TD',\n",
    "        'CEO', 'CFO', 'COO', 'IPO', 'ATH', 'ATL', 'EOD', 'EPS', 'ETF',\n",
    "        'GDP', 'IMO', 'LOL', 'NYC', 'USA', 'WSB', 'YOLO', 'FD', 'TA',\n",
    "        'IV', 'OI', 'PM', 'AM', 'PT', 'ER', 'PE', 'PS', 'PB', 'EV',\n",
    "        'AI', 'AR', 'VR', 'UK', 'EU', 'UN', 'FBI', 'SEC', 'FED', 'GO',\n",
    "        'LOVE', 'LIFE', 'REAL', 'TRUE', 'WELL', 'GOOD', 'BEST', 'NEXT',\n",
    "        'VERY', 'JUST', 'LIKE', 'EVEN', 'ONLY', 'OVER', 'SUCH', 'MOST',\n",
    "        'BEEN', 'WERE', 'WHAT', 'WHEN', 'CASH', 'GAIN', 'LOSS', 'HOLD',\n",
    "        'PUMP', 'DUMP', 'MOON', 'BEAR', 'BULL', 'CALL', 'POST', 'EDIT',\n",
    "        'INFO', 'TECH', 'FUND', 'RISK', 'GOLD', 'OPEN', 'SAVE', 'FAST',\n",
    "        'HUGE', 'PEAK', 'CARE', 'HOPE', 'STAY', 'PLAY', 'BIG', 'LOW',\n",
    "        'HIGH', 'AWAY', 'DONT', 'CANT', 'WONT', 'FREE', 'EASY', 'HARD'\n",
    "    }\n",
    "    \n",
    "    # WSB-specific slang that might be confused with tickers\n",
    "    WSB_SLANG = {\n",
    "        'YOLO', 'FOMO', 'HODL', 'ROPE', 'MOON', 'APES', 'WIFE',\n",
    "        'FANS', 'GANG', 'TEAM', 'KING', 'BABY', 'RICH', 'POOR'\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tickers = {}\n",
    "        self.ticker_to_company = {}\n",
    "        self.excluded = self.AMBIGUOUS_TICKERS.union(self.WSB_SLANG)\n",
    "        \n",
    "    def load_sp500(self) -> pd.DataFrame:\n",
    "        \"\"\"Load current S&P 500 constituents from Wikipedia.\"\"\"\n",
    "        url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "        tables = pd.read_html(url)\n",
    "        sp500 = tables[0]\n",
    "        return sp500[['Symbol', 'Security', 'GICS Sector', 'GICS Sub-Industry']]\n",
    "    \n",
    "    def load_russell1000(self) -> pd.DataFrame:\n",
    "        \"\"\"Load Russell 1000 constituents.\n",
    "        \n",
    "        Note: Russell 1000 list requires alternative source.\n",
    "        We use a combination of sources for comprehensive coverage.\n",
    "        \"\"\"\n",
    "        # Alternative: Load from saved file or API\n",
    "        # For now, we extend S&P 500 with additional large caps\n",
    "        pass\n",
    "    \n",
    "    def build_universe(self) -> Dict[str, str]:\n",
    "        \"\"\"Build the complete ticker universe.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping ticker to company name\n",
    "        \"\"\"\n",
    "        print(\"Building ticker universe...\")\n",
    "        \n",
    "        # Load S&P 500\n",
    "        try:\n",
    "            sp500 = self.load_sp500()\n",
    "            print(f\"Loaded {len(sp500)} S&P 500 constituents\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load S&P 500 from Wikipedia: {e}\")\n",
    "            sp500 = pd.DataFrame(columns=['Symbol', 'Security'])\n",
    "        \n",
    "        # Process tickers\n",
    "        for _, row in sp500.iterrows():\n",
    "            ticker = row['Symbol'].replace('.', '-').upper().strip()\n",
    "            if ticker not in self.excluded and len(ticker) >= 1:\n",
    "                self.ticker_to_company[ticker] = row['Security']\n",
    "        \n",
    "        # Add major stocks that might be missing\n",
    "        additional_tickers = {\n",
    "            'GME': 'GameStop Corp',\n",
    "            'AMC': 'AMC Entertainment',\n",
    "            'BB': 'BlackBerry Limited',\n",
    "            'NOK': 'Nokia Corporation',\n",
    "            'PLTR': 'Palantir Technologies',\n",
    "            'SPCE': 'Virgin Galactic',\n",
    "            'WISH': 'ContextLogic Inc',\n",
    "            'CLOV': 'Clover Health',\n",
    "            'SOFI': 'SoFi Technologies',\n",
    "            'RIVN': 'Rivian Automotive',\n",
    "            'LCID': 'Lucid Group'\n",
    "        }\n",
    "        \n",
    "        for ticker, company in additional_tickers.items():\n",
    "            if ticker not in self.excluded:\n",
    "                self.ticker_to_company[ticker] = company\n",
    "        \n",
    "        print(f\"Final universe: {len(self.ticker_to_company)} tickers\")\n",
    "        print(f\"Excluded ambiguous: {len(self.excluded)} symbols\")\n",
    "        \n",
    "        return self.ticker_to_company\n",
    "    \n",
    "    def get_regex_pattern(self) -> str:\n",
    "        \"\"\"Generate regex pattern for ticker extraction.\n",
    "        \n",
    "        Pattern matches:\n",
    "        - $TICKER format (most reliable)\n",
    "        - Standalone TICKER with word boundaries\n",
    "        \"\"\"\n",
    "        tickers = list(self.ticker_to_company.keys())\n",
    "        # Sort by length (longest first) to avoid partial matches\n",
    "        tickers_sorted = sorted(tickers, key=len, reverse=True)\n",
    "        \n",
    "        # Pattern: $TICKER or word-boundary TICKER\n",
    "        pattern = r'(?:\\$(' + '|'.join(tickers_sorted) + r')\\b|\\b(' + '|'.join(tickers_sorted) + r')\\b)'\n",
    "        return pattern\n",
    "\n",
    "# Initialize ticker universe\n",
    "ticker_universe = TickerUniverse()\n",
    "ticker_dict = ticker_universe.build_universe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TICKER EXTRACTION FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "class TickerExtractor:\n",
    "    \"\"\"Extracts stock tickers from social media text.\n",
    "    \n",
    "    Methodology follows Cookson & Niessner (2020) and Bradley et al. (2021):\n",
    "    1. Prefer $TICKER cashtag format (highest precision)\n",
    "    2. Fall back to standalone ticker with context validation\n",
    "    3. Return primary ticker (first mentioned in title, then body)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ticker_dict: Dict[str, str]):\n",
    "        self.ticker_dict = ticker_dict\n",
    "        self.valid_tickers = set(ticker_dict.keys())\n",
    "        \n",
    "        # Compile regex patterns\n",
    "        tickers_sorted = sorted(self.valid_tickers, key=len, reverse=True)\n",
    "        \n",
    "        # Cashtag pattern: $TICKER\n",
    "        self.cashtag_pattern = re.compile(\n",
    "            r'\\$(' + '|'.join(tickers_sorted) + r')\\b',\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        \n",
    "        # Standalone pattern: TICKER (all caps, word boundary)\n",
    "        self.standalone_pattern = re.compile(\n",
    "            r'\\b(' + '|'.join(tickers_sorted) + r')\\b'\n",
    "        )\n",
    "        \n",
    "    def extract_tickers(self, text: str, prefer_cashtag: bool = True) -> List[str]:\n",
    "        \"\"\"Extract all valid tickers from text.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text (title or body)\n",
    "            prefer_cashtag: If True, prioritize $TICKER matches\n",
    "            \n",
    "        Returns:\n",
    "            List of extracted tickers in order of appearance\n",
    "        \"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            return []\n",
    "        \n",
    "        tickers_found = []\n",
    "        positions = {}\n",
    "        \n",
    "        # Extract cashtags first (highest confidence)\n",
    "        for match in self.cashtag_pattern.finditer(text):\n",
    "            ticker = match.group(1).upper()\n",
    "            if ticker in self.valid_tickers:\n",
    "                pos = match.start()\n",
    "                if ticker not in positions or pos < positions[ticker]:\n",
    "                    positions[ticker] = pos\n",
    "        \n",
    "        # Extract standalone tickers (lower confidence)\n",
    "        for match in self.standalone_pattern.finditer(text):\n",
    "            ticker = match.group(1).upper()\n",
    "            if ticker in self.valid_tickers and ticker not in positions:\n",
    "                # Only add if ticker is in ALL CAPS in original text\n",
    "                original = text[match.start():match.end()]\n",
    "                if original.isupper():\n",
    "                    positions[ticker] = match.start()\n",
    "        \n",
    "        # Sort by position\n",
    "        tickers_found = sorted(positions.keys(), key=lambda x: positions[x])\n",
    "        return tickers_found\n",
    "    \n",
    "    def get_primary_ticker(self, title: str, body: str) -> Optional[str]:\n",
    "        \"\"\"Get primary ticker for a post.\n",
    "        \n",
    "        Priority:\n",
    "        1. First cashtag in title\n",
    "        2. First standalone ticker in title\n",
    "        3. First cashtag in body\n",
    "        4. First standalone ticker in body\n",
    "        \n",
    "        Returns:\n",
    "            Primary ticker or None if no valid ticker found\n",
    "        \"\"\"\n",
    "        # Check title first\n",
    "        title_tickers = self.extract_tickers(title or '')\n",
    "        if title_tickers:\n",
    "            return title_tickers[0]\n",
    "        \n",
    "        # Fall back to body\n",
    "        body_tickers = self.extract_tickers(body or '')\n",
    "        if body_tickers:\n",
    "            return body_tickers[0]\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def get_all_tickers(self, title: str, body: str) -> List[str]:\n",
    "        \"\"\"Get all unique tickers mentioned in a post.\"\"\"\n",
    "        title_tickers = self.extract_tickers(title or '')\n",
    "        body_tickers = self.extract_tickers(body or '')\n",
    "        \n",
    "        # Combine while preserving order\n",
    "        all_tickers = []\n",
    "        seen = set()\n",
    "        for ticker in title_tickers + body_tickers:\n",
    "            if ticker not in seen:\n",
    "                all_tickers.append(ticker)\n",
    "                seen.add(ticker)\n",
    "        \n",
    "        return all_tickers\n",
    "\n",
    "# Initialize extractor\n",
    "ticker_extractor = TickerExtractor(ticker_dict)\n",
    "\n",
    "# Test extraction\n",
    "test_cases = [\n",
    "    (\"$GME to the moon! ðŸš€ðŸš€ðŸš€\", \"GameStop is going to squeeze\"),\n",
    "    (\"AAPL earnings play\", \"Thinking of buying $AAPL calls before earnings\"),\n",
    "    (\"My DD on Tesla\", \"TSLA is undervalued because of the energy business\")\n",
    "]\n",
    "\n",
    "print(\"Testing ticker extraction:\")\n",
    "for title, body in test_cases:\n",
    "    primary = ticker_extractor.get_primary_ticker(title, body)\n",
    "    all_tickers = ticker_extractor.get_all_tickers(title, body)\n",
    "    print(f\"  Title: '{title}'\")\n",
    "    print(f\"  Primary: {primary}, All: {all_tickers}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reddit API Configuration\n",
    "\n",
    "### 4.1 Setup Reddit API Credentials\n",
    "\n",
    "You need to create a Reddit application at https://www.reddit.com/prefs/apps to obtain credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# REDDIT API CREDENTIALS\n",
    "# =============================================================================\n",
    "# \n",
    "# IMPORTANT: Store credentials securely. Do not commit to version control.\n",
    "#\n",
    "# To obtain credentials:\n",
    "# 1. Go to https://www.reddit.com/prefs/apps\n",
    "# 2. Click \"create another app\"\n",
    "# 3. Select \"script\" as the application type\n",
    "# 4. Fill in name and redirect URI (http://localhost:8080)\n",
    "# 5. Copy client_id (under app name) and client_secret\n",
    "\n",
    "# Option 1: Enter credentials manually (will prompt)\n",
    "from getpass import getpass\n",
    "\n",
    "REDDIT_CLIENT_ID = getpass(\"Enter Reddit Client ID: \")\n",
    "REDDIT_CLIENT_SECRET = getpass(\"Enter Reddit Client Secret: \")\n",
    "REDDIT_USER_AGENT = \"ResearchBot/1.0 (Academic Research on Retail Sentiment)\"\n",
    "\n",
    "print(\"Credentials stored (not displayed for security).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# INITIALIZE REDDIT API CLIENT\n",
    "# =============================================================================\n",
    "\n",
    "def initialize_reddit_client(client_id: str, client_secret: str, \n",
    "                              user_agent: str) -> praw.Reddit:\n",
    "    \"\"\"Initialize and validate Reddit API connection.\n",
    "    \n",
    "    Args:\n",
    "        client_id: Reddit API client ID\n",
    "        client_secret: Reddit API client secret\n",
    "        user_agent: Descriptive user agent string\n",
    "        \n",
    "    Returns:\n",
    "        Authenticated Reddit instance\n",
    "    \"\"\"\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=client_id,\n",
    "        client_secret=client_secret,\n",
    "        user_agent=user_agent\n",
    "    )\n",
    "    \n",
    "    # Validate connection\n",
    "    try:\n",
    "        # Test API access\n",
    "        reddit.user.me()  # Will be None for script apps, but validates credentials\n",
    "        print(\"Reddit API connection successful (read-only mode)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not fully validate API access: {e}\")\n",
    "        print(\"Continuing with read-only public access...\")\n",
    "    \n",
    "    return reddit\n",
    "\n",
    "# Initialize client\n",
    "reddit = initialize_reddit_client(\n",
    "    REDDIT_CLIENT_ID,\n",
    "    REDDIT_CLIENT_SECRET,\n",
    "    REDDIT_USER_AGENT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Collection Methods\n",
    "\n",
    "### 5.1 Method A: Pushshift API (Historical Data)\n",
    "\n",
    "Pushshift.io provides comprehensive historical Reddit data. This is the preferred method for academic research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PUSHSHIFT DATA COLLECTOR\n",
    "# =============================================================================\n",
    "\n",
    "class PushshiftCollector:\n",
    "    \"\"\"Collects historical Reddit data using Pushshift API.\n",
    "    \n",
    "    Pushshift provides access to historical Reddit data that is no longer\n",
    "    available through the official Reddit API.\n",
    "    \n",
    "    Note: Pushshift availability has varied. Check current status at:\n",
    "    https://pushshift.io/\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reddit_client: praw.Reddit = None):\n",
    "        \"\"\"Initialize Pushshift API client.\n",
    "        \n",
    "        Args:\n",
    "            reddit_client: Optional PRAW client for enriching data\n",
    "        \"\"\"\n",
    "        self.api = PushshiftAPI(praw=reddit_client)\n",
    "        self.reddit = reddit_client\n",
    "        \n",
    "    def collect_submissions(\n",
    "        self,\n",
    "        subreddit: str,\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        limit: int = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Collect submissions (posts) from a subreddit.\n",
    "        \n",
    "        Args:\n",
    "            subreddit: Subreddit name (without r/)\n",
    "            start_date: Start date (YYYY-MM-DD)\n",
    "            end_date: End date (YYYY-MM-DD)\n",
    "            limit: Maximum number of submissions (None for all)\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with submission data\n",
    "        \"\"\"\n",
    "        # Convert dates to timestamps\n",
    "        start_ts = int(datetime.strptime(start_date, \"%Y-%m-%d\").timestamp())\n",
    "        end_ts = int(datetime.strptime(end_date, \"%Y-%m-%d\").timestamp())\n",
    "        \n",
    "        print(f\"Collecting submissions from r/{subreddit}\")\n",
    "        print(f\"Period: {start_date} to {end_date}\")\n",
    "        \n",
    "        # Query Pushshift\n",
    "        submissions = self.api.search_submissions(\n",
    "            subreddit=subreddit,\n",
    "            after=start_ts,\n",
    "            before=end_ts,\n",
    "            limit=limit,\n",
    "            filter=['id', 'created_utc', 'title', 'selftext', 'score',\n",
    "                    'num_comments', 'author', 'permalink', 'upvote_ratio']\n",
    "        )\n",
    "        \n",
    "        # Convert to list\n",
    "        posts_list = []\n",
    "        for post in tqdm(submissions, desc=\"Processing submissions\"):\n",
    "            posts_list.append({\n",
    "                'post_id': post.get('id'),\n",
    "                'created_utc': post.get('created_utc'),\n",
    "                'title': post.get('title'),\n",
    "                'selftext': post.get('selftext'),\n",
    "                'score': post.get('score'),\n",
    "                'num_comments': post.get('num_comments'),\n",
    "                'author': str(post.get('author')),\n",
    "                'permalink': post.get('permalink'),\n",
    "                'upvote_ratio': post.get('upvote_ratio'),\n",
    "                'subreddit': subreddit\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(posts_list)\n",
    "        \n",
    "        if len(df) > 0:\n",
    "            # Convert timestamp to datetime\n",
    "            df['datetime'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "            df['date'] = df['datetime'].dt.date\n",
    "            df['time'] = df['datetime'].dt.time\n",
    "            \n",
    "            print(f\"Collected {len(df):,} submissions\")\n",
    "        else:\n",
    "            print(\"No submissions found\")\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    def collect_comments(\n",
    "        self,\n",
    "        subreddit: str,\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        limit: int = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Collect comments from a subreddit.\n",
    "        \n",
    "        Args:\n",
    "            subreddit: Subreddit name (without r/)\n",
    "            start_date: Start date (YYYY-MM-DD)\n",
    "            end_date: End date (YYYY-MM-DD)\n",
    "            limit: Maximum number of comments (None for all)\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with comment data\n",
    "        \"\"\"\n",
    "        start_ts = int(datetime.strptime(start_date, \"%Y-%m-%d\").timestamp())\n",
    "        end_ts = int(datetime.strptime(end_date, \"%Y-%m-%d\").timestamp())\n",
    "        \n",
    "        print(f\"Collecting comments from r/{subreddit}\")\n",
    "        print(f\"Period: {start_date} to {end_date}\")\n",
    "        \n",
    "        comments = self.api.search_comments(\n",
    "            subreddit=subreddit,\n",
    "            after=start_ts,\n",
    "            before=end_ts,\n",
    "            limit=limit,\n",
    "            filter=['id', 'created_utc', 'body', 'score', 'author',\n",
    "                    'link_id', 'parent_id', 'permalink']\n",
    "        )\n",
    "        \n",
    "        comments_list = []\n",
    "        for comment in tqdm(comments, desc=\"Processing comments\"):\n",
    "            comments_list.append({\n",
    "                'comment_id': comment.get('id'),\n",
    "                'created_utc': comment.get('created_utc'),\n",
    "                'body': comment.get('body'),\n",
    "                'score': comment.get('score'),\n",
    "                'author': str(comment.get('author')),\n",
    "                'link_id': comment.get('link_id'),\n",
    "                'parent_id': comment.get('parent_id'),\n",
    "                'permalink': comment.get('permalink'),\n",
    "                'subreddit': subreddit\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(comments_list)\n",
    "        \n",
    "        if len(df) > 0:\n",
    "            df['datetime'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "            df['date'] = df['datetime'].dt.date\n",
    "            df['time'] = df['datetime'].dt.time\n",
    "            \n",
    "            print(f\"Collected {len(df):,} comments\")\n",
    "        else:\n",
    "            print(\"No comments found\")\n",
    "            \n",
    "        return df\n",
    "\n",
    "# Initialize collector\n",
    "pushshift_collector = PushshiftCollector(reddit)\n",
    "print(\"Pushshift collector initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Method B: BigQuery Reddit Dataset\n",
    "\n",
    "Google BigQuery hosts historical Reddit data. This is efficient for large-scale queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BIGQUERY REDDIT COLLECTOR\n",
    "# =============================================================================\n",
    "\n",
    "class BigQueryRedditCollector:\n",
    "    \"\"\"Collects Reddit data from Google BigQuery public datasets.\n",
    "    \n",
    "    The fh-bigquery:reddit datasets contain historical Reddit posts and\n",
    "    comments. This is efficient for large-scale academic research.\n",
    "    \n",
    "    Note: Requires Google Cloud authentication.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, project_id: str = None):\n",
    "        \"\"\"Initialize BigQuery client.\n",
    "        \n",
    "        Args:\n",
    "            project_id: Google Cloud project ID\n",
    "        \"\"\"\n",
    "        # Authenticate with Google Cloud\n",
    "        auth.authenticate_user()\n",
    "        \n",
    "        self.client = bigquery.Client(project=project_id)\n",
    "        self.project_id = project_id\n",
    "        print(\"BigQuery client initialized\")\n",
    "        \n",
    "    def query_submissions(\n",
    "        self,\n",
    "        subreddits: List[str],\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        min_score: int = -100\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Query Reddit submissions from BigQuery.\n",
    "        \n",
    "        Args:\n",
    "            subreddits: List of subreddit names\n",
    "            start_date: Start date (YYYY-MM-DD)\n",
    "            end_date: End date (YYYY-MM-DD)\n",
    "            min_score: Minimum post score filter\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with submission data\n",
    "        \"\"\"\n",
    "        subreddit_list = \"','\".join(subreddits)\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT\n",
    "            id AS post_id,\n",
    "            created_utc,\n",
    "            TIMESTAMP_SECONDS(created_utc) AS datetime,\n",
    "            DATE(TIMESTAMP_SECONDS(created_utc)) AS date,\n",
    "            title,\n",
    "            selftext,\n",
    "            score,\n",
    "            num_comments,\n",
    "            author,\n",
    "            subreddit,\n",
    "            permalink,\n",
    "            upvote_ratio\n",
    "        FROM `fh-bigquery.reddit_posts.*`\n",
    "        WHERE subreddit IN ('{subreddit_list}')\n",
    "            AND DATE(TIMESTAMP_SECONDS(created_utc)) >= '{start_date}'\n",
    "            AND DATE(TIMESTAMP_SECONDS(created_utc)) <= '{end_date}'\n",
    "            AND score >= {min_score}\n",
    "        ORDER BY created_utc\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"Querying BigQuery for submissions...\")\n",
    "        print(f\"Subreddits: {subreddits}\")\n",
    "        print(f\"Period: {start_date} to {end_date}\")\n",
    "        \n",
    "        df = self.client.query(query).to_dataframe()\n",
    "        print(f\"Retrieved {len(df):,} submissions\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def query_comments(\n",
    "        self,\n",
    "        subreddits: List[str],\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        min_score: int = -100\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Query Reddit comments from BigQuery.\n",
    "        \n",
    "        Args:\n",
    "            subreddits: List of subreddit names\n",
    "            start_date: Start date (YYYY-MM-DD)\n",
    "            end_date: End date (YYYY-MM-DD)\n",
    "            min_score: Minimum comment score filter\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with comment data\n",
    "        \"\"\"\n",
    "        subreddit_list = \"','\".join(subreddits)\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT\n",
    "            id AS comment_id,\n",
    "            created_utc,\n",
    "            TIMESTAMP_SECONDS(created_utc) AS datetime,\n",
    "            DATE(TIMESTAMP_SECONDS(created_utc)) AS date,\n",
    "            body,\n",
    "            score,\n",
    "            author,\n",
    "            subreddit,\n",
    "            link_id,\n",
    "            parent_id\n",
    "        FROM `fh-bigquery.reddit_comments.*`\n",
    "        WHERE subreddit IN ('{subreddit_list}')\n",
    "            AND DATE(TIMESTAMP_SECONDS(created_utc)) >= '{start_date}'\n",
    "            AND DATE(TIMESTAMP_SECONDS(created_utc)) <= '{end_date}'\n",
    "            AND score >= {min_score}\n",
    "        ORDER BY created_utc\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"Querying BigQuery for comments...\")\n",
    "        df = self.client.query(query).to_dataframe()\n",
    "        print(f\"Retrieved {len(df):,} comments\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Note: Initialize only when needed due to authentication requirement\n",
    "# bq_collector = BigQueryRedditCollector(project_id=\"your-project-id\")\n",
    "print(\"BigQuery collector class defined (initialize when ready)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Method C: Pre-Existing Academic Datasets\n",
    "\n",
    "For reproducibility, we can also use pre-existing Reddit datasets (e.g., from Kaggle or academic archives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# KAGGLE / ACADEMIC DATASET LOADER\n",
    "# =============================================================================\n",
    "\n",
    "class ExternalDatasetLoader:\n",
    "    \"\"\"Loads pre-existing Reddit datasets from external sources.\n",
    "    \n",
    "    Sources:\n",
    "    - Kaggle WSB datasets\n",
    "    - Academic Reddit archives\n",
    "    - Pushshift data dumps\n",
    "    \"\"\"\n",
    "    \n",
    "    # Known Kaggle datasets for WSB\n",
    "    KAGGLE_DATASETS = {\n",
    "        'wsb_daily': 'unanimad/reddit-rwallstreetbets',\n",
    "        'wsb_2021': 'mattpodolak/rwallstreetbets-posts-and-comments',\n",
    "        'gme_saga': 'gpreda/reddit-wsb-gme-gamestop'\n",
    "    }\n",
    "    \n",
    "    def __init__(self, data_dir: str):\n",
    "        \"\"\"Initialize loader with data directory.\n",
    "        \n",
    "        Args:\n",
    "            data_dir: Directory to store downloaded data\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        os.makedirs(data_dir, exist_ok=True)\n",
    "        \n",
    "    def setup_kaggle(self):\n",
    "        \"\"\"Setup Kaggle API authentication.\n",
    "        \n",
    "        Requires kaggle.json file with API credentials.\n",
    "        \"\"\"\n",
    "        # Install Kaggle if needed\n",
    "        try:\n",
    "            import kaggle\n",
    "        except ImportError:\n",
    "            os.system('pip install kaggle')\n",
    "            import kaggle\n",
    "        \n",
    "        print(\"Kaggle API ready\")\n",
    "        print(\"Note: Place kaggle.json in ~/.kaggle/ for authentication\")\n",
    "        \n",
    "    def download_kaggle_dataset(self, dataset_key: str) -> str:\n",
    "        \"\"\"Download a dataset from Kaggle.\n",
    "        \n",
    "        Args:\n",
    "            dataset_key: Key from KAGGLE_DATASETS or full dataset path\n",
    "            \n",
    "        Returns:\n",
    "            Path to downloaded data\n",
    "        \"\"\"\n",
    "        dataset_path = self.KAGGLE_DATASETS.get(dataset_key, dataset_key)\n",
    "        \n",
    "        output_dir = os.path.join(self.data_dir, dataset_key)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        os.system(f'kaggle datasets download -d {dataset_path} -p {output_dir} --unzip')\n",
    "        \n",
    "        print(f\"Dataset downloaded to: {output_dir}\")\n",
    "        return output_dir\n",
    "    \n",
    "    def load_csv_dataset(self, filepath: str) -> pd.DataFrame:\n",
    "        \"\"\"Load a CSV dataset with standard preprocessing.\n",
    "        \n",
    "        Args:\n",
    "            filepath: Path to CSV file\n",
    "            \n",
    "        Returns:\n",
    "            Preprocessed DataFrame\n",
    "        \"\"\"\n",
    "        print(f\"Loading: {filepath}\")\n",
    "        \n",
    "        # Try different encodings\n",
    "        for encoding in ['utf-8', 'latin-1', 'cp1252']:\n",
    "            try:\n",
    "                df = pd.read_csv(filepath, encoding=encoding, low_memory=False)\n",
    "                break\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "        \n",
    "        print(f\"Loaded {len(df):,} rows\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def standardize_columns(self, df: pd.DataFrame, \n",
    "                           column_mapping: Dict[str, str]) -> pd.DataFrame:\n",
    "        \"\"\"Standardize column names across different datasets.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            column_mapping: Mapping from original to standard names\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with standardized columns\n",
    "        \"\"\"\n",
    "        df_renamed = df.rename(columns=column_mapping)\n",
    "        return df_renamed\n",
    "\n",
    "# Initialize loader\n",
    "external_loader = ExternalDatasetLoader(config.RAW_DATA_PATH)\n",
    "print(\"External dataset loader initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Collection Execution\n",
    "\n",
    "### 6.1 Collect WSB Data by Year\n",
    "\n",
    "We collect data in yearly chunks to manage memory and enable checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA COLLECTION PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "class DataCollectionPipeline:\n",
    "    \"\"\"Orchestrates the data collection process.\n",
    "    \n",
    "    Features:\n",
    "    - Yearly chunked collection for memory management\n",
    "    - Checkpoint saving for interruption recovery\n",
    "    - Multiple source fallback\n",
    "    - Progress tracking\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: ResearchConfig, ticker_extractor: TickerExtractor):\n",
    "        self.config = config\n",
    "        self.ticker_extractor = ticker_extractor\n",
    "        self.collected_data = {}\n",
    "        \n",
    "    def collect_year(\n",
    "        self,\n",
    "        year: int,\n",
    "        subreddits: List[str],\n",
    "        collector: PushshiftCollector,\n",
    "        collect_comments: bool = True\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Collect data for a specific year.\n",
    "        \n",
    "        Args:\n",
    "            year: Year to collect\n",
    "            subreddits: List of subreddits\n",
    "            collector: Data collector instance\n",
    "            collect_comments: Whether to collect comments\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (submissions_df, comments_df)\n",
    "        \"\"\"\n",
    "        start_date = f\"{year}-01-01\"\n",
    "        end_date = f\"{year}-12-31\"\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Collecting data for {year}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        all_submissions = []\n",
    "        all_comments = []\n",
    "        \n",
    "        for subreddit in subreddits:\n",
    "            print(f\"\\nProcessing r/{subreddit}...\")\n",
    "            \n",
    "            # Collect submissions\n",
    "            try:\n",
    "                submissions = collector.collect_submissions(\n",
    "                    subreddit=subreddit,\n",
    "                    start_date=start_date,\n",
    "                    end_date=end_date\n",
    "                )\n",
    "                all_submissions.append(submissions)\n",
    "            except Exception as e:\n",
    "                print(f\"Error collecting submissions: {e}\")\n",
    "            \n",
    "            # Collect comments (optional)\n",
    "            if collect_comments:\n",
    "                try:\n",
    "                    comments = collector.collect_comments(\n",
    "                        subreddit=subreddit,\n",
    "                        start_date=start_date,\n",
    "                        end_date=end_date\n",
    "                    )\n",
    "                    all_comments.append(comments)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error collecting comments: {e}\")\n",
    "            \n",
    "            # Rate limiting\n",
    "            time.sleep(self.config.SLEEP_BETWEEN_REQUESTS)\n",
    "        \n",
    "        # Combine DataFrames\n",
    "        submissions_df = pd.concat(all_submissions, ignore_index=True) if all_submissions else pd.DataFrame()\n",
    "        comments_df = pd.concat(all_comments, ignore_index=True) if all_comments else pd.DataFrame()\n",
    "        \n",
    "        return submissions_df, comments_df\n",
    "    \n",
    "    def add_ticker_mapping(self, df: pd.DataFrame, \n",
    "                          text_col: str, title_col: str = None) -> pd.DataFrame:\n",
    "        \"\"\"Add ticker mapping to DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            text_col: Column containing main text (body/selftext)\n",
    "            title_col: Column containing title (optional)\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with ticker columns added\n",
    "        \"\"\"\n",
    "        print(\"Extracting tickers...\")\n",
    "        \n",
    "        tickers_primary = []\n",
    "        tickers_all = []\n",
    "        \n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Ticker extraction\"):\n",
    "            title = row.get(title_col, '') if title_col else ''\n",
    "            text = row.get(text_col, '') or ''\n",
    "            \n",
    "            primary = self.ticker_extractor.get_primary_ticker(title, text)\n",
    "            all_tickers = self.ticker_extractor.get_all_tickers(title, text)\n",
    "            \n",
    "            tickers_primary.append(primary)\n",
    "            tickers_all.append(all_tickers)\n",
    "        \n",
    "        df['ticker_primary'] = tickers_primary\n",
    "        df['tickers_all'] = tickers_all\n",
    "        df['ticker_count'] = df['tickers_all'].apply(len)\n",
    "        \n",
    "        # Statistics\n",
    "        has_ticker = df['ticker_primary'].notna().sum()\n",
    "        print(f\"Posts with identified ticker: {has_ticker:,} ({100*has_ticker/len(df):.1f}%)\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def save_checkpoint(self, df: pd.DataFrame, name: str, year: int):\n",
    "        \"\"\"Save data checkpoint to disk.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to save\n",
    "            name: Dataset name (submissions/comments)\n",
    "            year: Year of data\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(\n",
    "            self.config.RAW_DATA_PATH,\n",
    "            f\"wsb_{name}_{year}.parquet\"\n",
    "        )\n",
    "        df.to_parquet(filepath, index=False)\n",
    "        print(f\"Saved checkpoint: {filepath}\")\n",
    "        \n",
    "    def load_checkpoint(self, name: str, year: int) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Load data checkpoint from disk.\n",
    "        \n",
    "        Args:\n",
    "            name: Dataset name\n",
    "            year: Year of data\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame or None if not found\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(\n",
    "            self.config.RAW_DATA_PATH,\n",
    "            f\"wsb_{name}_{year}.parquet\"\n",
    "        )\n",
    "        if os.path.exists(filepath):\n",
    "            print(f\"Loading checkpoint: {filepath}\")\n",
    "            return pd.read_parquet(filepath)\n",
    "        return None\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = DataCollectionPipeline(config, ticker_extractor)\n",
    "print(\"Data collection pipeline initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXECUTE DATA COLLECTION\n",
    "# =============================================================================\n",
    "\n",
    "# Define collection years\n",
    "YEARS_TO_COLLECT = [2018, 2019, 2020, 2021, 2022, 2023]\n",
    "\n",
    "# Storage for all collected data\n",
    "all_submissions = []\n",
    "all_comments = []\n",
    "\n",
    "# Collect data year by year\n",
    "for year in YEARS_TO_COLLECT:\n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"# COLLECTING DATA FOR {year}\")\n",
    "    print(f\"{'#'*60}\")\n",
    "    \n",
    "    # Check for existing checkpoint\n",
    "    submissions_checkpoint = pipeline.load_checkpoint('submissions', year)\n",
    "    comments_checkpoint = pipeline.load_checkpoint('comments', year)\n",
    "    \n",
    "    if submissions_checkpoint is not None:\n",
    "        print(f\"Using cached submissions for {year}\")\n",
    "        year_submissions = submissions_checkpoint\n",
    "    else:\n",
    "        # Collect new data\n",
    "        year_submissions, year_comments = pipeline.collect_year(\n",
    "            year=year,\n",
    "            subreddits=config.PRIMARY_SUBREDDITS,\n",
    "            collector=pushshift_collector,\n",
    "            collect_comments=True\n",
    "        )\n",
    "        \n",
    "        # Add ticker mapping\n",
    "        if len(year_submissions) > 0:\n",
    "            year_submissions = pipeline.add_ticker_mapping(\n",
    "                year_submissions, \n",
    "                text_col='selftext', \n",
    "                title_col='title'\n",
    "            )\n",
    "            pipeline.save_checkpoint(year_submissions, 'submissions', year)\n",
    "        \n",
    "        if len(year_comments) > 0:\n",
    "            year_comments = pipeline.add_ticker_mapping(\n",
    "                year_comments,\n",
    "                text_col='body'\n",
    "            )\n",
    "            pipeline.save_checkpoint(year_comments, 'comments', year)\n",
    "            all_comments.append(year_comments)\n",
    "    \n",
    "    if year_submissions is not None and len(year_submissions) > 0:\n",
    "        all_submissions.append(year_submissions)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA COLLECTION COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Quality and Validation\n",
    "\n",
    "### 7.1 Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA QUALITY VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "class DataQualityValidator:\n",
    "    \"\"\"Validates data quality for academic research standards.\n",
    "    \n",
    "    Following best practices from top finance journals.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.validation_results = {}\n",
    "        \n",
    "    def validate_submissions(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Run comprehensive validation on submissions data.\n",
    "        \n",
    "        Args:\n",
    "            df: Submissions DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of validation results\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'total_records': len(df),\n",
    "            'date_range': (df['date'].min(), df['date'].max()),\n",
    "            'missing_values': {},\n",
    "            'ticker_coverage': {},\n",
    "            'temporal_distribution': {},\n",
    "            'quality_flags': []\n",
    "        }\n",
    "        \n",
    "        # Missing values check\n",
    "        for col in df.columns:\n",
    "            missing = df[col].isna().sum()\n",
    "            if missing > 0:\n",
    "                results['missing_values'][col] = {\n",
    "                    'count': missing,\n",
    "                    'pct': 100 * missing / len(df)\n",
    "                }\n",
    "        \n",
    "        # Ticker coverage\n",
    "        if 'ticker_primary' in df.columns:\n",
    "            has_ticker = df['ticker_primary'].notna().sum()\n",
    "            results['ticker_coverage'] = {\n",
    "                'posts_with_ticker': has_ticker,\n",
    "                'coverage_pct': 100 * has_ticker / len(df),\n",
    "                'unique_tickers': df['ticker_primary'].nunique(),\n",
    "                'top_tickers': df['ticker_primary'].value_counts().head(20).to_dict()\n",
    "            }\n",
    "        \n",
    "        # Temporal distribution\n",
    "        if 'date' in df.columns:\n",
    "            daily_counts = df.groupby('date').size()\n",
    "            results['temporal_distribution'] = {\n",
    "                'days_covered': len(daily_counts),\n",
    "                'avg_posts_per_day': daily_counts.mean(),\n",
    "                'min_posts_day': daily_counts.min(),\n",
    "                'max_posts_day': daily_counts.max(),\n",
    "                'days_with_zero_posts': (daily_counts == 0).sum()\n",
    "            }\n",
    "        \n",
    "        # Quality flags\n",
    "        if results['ticker_coverage'].get('coverage_pct', 0) < 10:\n",
    "            results['quality_flags'].append('LOW_TICKER_COVERAGE')\n",
    "        \n",
    "        if len(df) < 10000:\n",
    "            results['quality_flags'].append('SMALL_SAMPLE')\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def print_validation_report(self, results: Dict):\n",
    "        \"\"\"Print formatted validation report.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"DATA QUALITY VALIDATION REPORT\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"\\nTotal Records: {results['total_records']:,}\")\n",
    "        print(f\"Date Range: {results['date_range'][0]} to {results['date_range'][1]}\")\n",
    "        \n",
    "        print(\"\\n--- Missing Values ---\")\n",
    "        if results['missing_values']:\n",
    "            for col, info in results['missing_values'].items():\n",
    "                print(f\"  {col}: {info['count']:,} ({info['pct']:.2f}%)\")\n",
    "        else:\n",
    "            print(\"  No missing values\")\n",
    "        \n",
    "        print(\"\\n--- Ticker Coverage ---\")\n",
    "        tc = results['ticker_coverage']\n",
    "        if tc:\n",
    "            print(f\"  Posts with ticker: {tc['posts_with_ticker']:,} ({tc['coverage_pct']:.1f}%)\")\n",
    "            print(f\"  Unique tickers: {tc['unique_tickers']:,}\")\n",
    "            print(f\"  Top 10 tickers:\")\n",
    "            for ticker, count in list(tc['top_tickers'].items())[:10]:\n",
    "                print(f\"    {ticker}: {count:,}\")\n",
    "        \n",
    "        print(\"\\n--- Temporal Distribution ---\")\n",
    "        td = results['temporal_distribution']\n",
    "        if td:\n",
    "            print(f\"  Days covered: {td['days_covered']:,}\")\n",
    "            print(f\"  Avg posts/day: {td['avg_posts_per_day']:.1f}\")\n",
    "            print(f\"  Range: {td['min_posts_day']} - {td['max_posts_day']}\")\n",
    "        \n",
    "        print(\"\\n--- Quality Flags ---\")\n",
    "        if results['quality_flags']:\n",
    "            for flag in results['quality_flags']:\n",
    "                print(f\"  âš ï¸  {flag}\")\n",
    "        else:\n",
    "            print(\"  âœ“ No quality issues detected\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Initialize validator\n",
    "validator = DataQualityValidator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RUN VALIDATION ON COLLECTED DATA\n",
    "# =============================================================================\n",
    "\n",
    "# Combine all submissions\n",
    "if all_submissions:\n",
    "    combined_submissions = pd.concat(all_submissions, ignore_index=True)\n",
    "    print(f\"Combined submissions: {len(combined_submissions):,} records\")\n",
    "    \n",
    "    # Run validation\n",
    "    validation_results = validator.validate_submissions(combined_submissions)\n",
    "    validator.print_validation_report(validation_results)\n",
    "else:\n",
    "    print(\"No submissions data collected yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Output: Post-Level Table\n",
    "\n",
    "### 8.1 Create Final Output Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CREATE FINAL OUTPUT TABLE\n",
    "# =============================================================================\n",
    "\n",
    "def create_post_level_table(submissions_df: pd.DataFrame, \n",
    "                            comments_df: pd.DataFrame = None) -> pd.DataFrame:\n",
    "    \"\"\"Create the final post-level table for analysis.\n",
    "    \n",
    "    Output schema:\n",
    "    [platform_post_id, ticker, date, time, text, score, author, subreddit]\n",
    "    \n",
    "    Args:\n",
    "        submissions_df: Submissions data\n",
    "        comments_df: Comments data (optional)\n",
    "        \n",
    "    Returns:\n",
    "        Combined post-level DataFrame\n",
    "    \"\"\"\n",
    "    posts = []\n",
    "    \n",
    "    # Process submissions\n",
    "    if submissions_df is not None and len(submissions_df) > 0:\n",
    "        submissions_processed = pd.DataFrame({\n",
    "            'platform_post_id': 'submission_' + submissions_df['post_id'].astype(str),\n",
    "            'ticker': submissions_df['ticker_primary'],\n",
    "            'date': pd.to_datetime(submissions_df['date']),\n",
    "            'time': submissions_df['time'],\n",
    "            'datetime': submissions_df['datetime'],\n",
    "            'title': submissions_df['title'],\n",
    "            'text': submissions_df['selftext'],\n",
    "            'score': submissions_df['score'],\n",
    "            'num_comments': submissions_df['num_comments'],\n",
    "            'author': submissions_df['author'],\n",
    "            'subreddit': submissions_df['subreddit'],\n",
    "            'tickers_all': submissions_df['tickers_all'],\n",
    "            'ticker_count': submissions_df['ticker_count'],\n",
    "            'post_type': 'submission'\n",
    "        })\n",
    "        posts.append(submissions_processed)\n",
    "        print(f\"Processed {len(submissions_processed):,} submissions\")\n",
    "    \n",
    "    # Process comments\n",
    "    if comments_df is not None and len(comments_df) > 0:\n",
    "        comments_processed = pd.DataFrame({\n",
    "            'platform_post_id': 'comment_' + comments_df['comment_id'].astype(str),\n",
    "            'ticker': comments_df['ticker_primary'],\n",
    "            'date': pd.to_datetime(comments_df['date']),\n",
    "            'time': comments_df['time'],\n",
    "            'datetime': comments_df['datetime'],\n",
    "            'title': None,  # Comments don't have titles\n",
    "            'text': comments_df['body'],\n",
    "            'score': comments_df['score'],\n",
    "            'num_comments': 0,\n",
    "            'author': comments_df['author'],\n",
    "            'subreddit': comments_df['subreddit'],\n",
    "            'tickers_all': comments_df['tickers_all'],\n",
    "            'ticker_count': comments_df['ticker_count'],\n",
    "            'post_type': 'comment'\n",
    "        })\n",
    "        posts.append(comments_processed)\n",
    "        print(f\"Processed {len(comments_processed):,} comments\")\n",
    "    \n",
    "    # Combine\n",
    "    if posts:\n",
    "        final_df = pd.concat(posts, ignore_index=True)\n",
    "        \n",
    "        # Sort by datetime\n",
    "        final_df = final_df.sort_values('datetime').reset_index(drop=True)\n",
    "        \n",
    "        print(f\"\\nFinal post-level table: {len(final_df):,} records\")\n",
    "        return final_df\n",
    "    else:\n",
    "        print(\"No data to process\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Create final table (uncomment when data is ready)\n",
    "# post_level_table = create_post_level_table(combined_submissions, combined_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE FINAL OUTPUT\n",
    "# =============================================================================\n",
    "\n",
    "def save_post_level_table(df: pd.DataFrame, output_dir: str):\n",
    "    \"\"\"Save post-level table in multiple formats.\n",
    "    \n",
    "    Saves:\n",
    "    - Parquet (efficient for analysis)\n",
    "    - CSV (for compatibility)\n",
    "    - Data dictionary (documentation)\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Parquet (primary format)\n",
    "    parquet_path = os.path.join(output_dir, 'wsb_posts_raw.parquet')\n",
    "    df.to_parquet(parquet_path, index=False)\n",
    "    print(f\"Saved Parquet: {parquet_path}\")\n",
    "    \n",
    "    # CSV (subset for inspection)\n",
    "    csv_path = os.path.join(output_dir, 'wsb_posts_sample.csv')\n",
    "    df.head(10000).to_csv(csv_path, index=False)\n",
    "    print(f\"Saved CSV sample: {csv_path}\")\n",
    "    \n",
    "    # Data dictionary\n",
    "    data_dict = {\n",
    "        'platform_post_id': 'Unique identifier for post (submission_ or comment_ prefix)',\n",
    "        'ticker': 'Primary stock ticker mentioned (uppercase)',\n",
    "        'date': 'Date of post (YYYY-MM-DD)',\n",
    "        'time': 'Time of post (HH:MM:SS)',\n",
    "        'datetime': 'Full datetime (UTC)',\n",
    "        'title': 'Post title (submissions only)',\n",
    "        'text': 'Post body text',\n",
    "        'score': 'Reddit score (upvotes - downvotes)',\n",
    "        'num_comments': 'Number of comments (submissions only)',\n",
    "        'author': 'Reddit username (pseudonymous)',\n",
    "        'subreddit': 'Source subreddit',\n",
    "        'tickers_all': 'List of all tickers mentioned',\n",
    "        'ticker_count': 'Number of tickers mentioned',\n",
    "        'post_type': 'Type: submission or comment'\n",
    "    }\n",
    "    \n",
    "    dict_path = os.path.join(output_dir, 'data_dictionary.json')\n",
    "    with open(dict_path, 'w') as f:\n",
    "        json.dump(data_dict, f, indent=2)\n",
    "    print(f\"Saved data dictionary: {dict_path}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    summary = {\n",
    "        'total_posts': len(df),\n",
    "        'date_range': [str(df['date'].min()), str(df['date'].max())],\n",
    "        'posts_with_ticker': int(df['ticker'].notna().sum()),\n",
    "        'unique_tickers': int(df['ticker'].nunique()),\n",
    "        'unique_authors': int(df['author'].nunique()),\n",
    "        'submissions': int((df['post_type'] == 'submission').sum()),\n",
    "        'comments': int((df['post_type'] == 'comment').sum()),\n",
    "        'created_at': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    summary_path = os.path.join(output_dir, 'collection_summary.json')\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(f\"Saved summary: {summary_path}\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Save final table (uncomment when ready)\n",
    "# summary = save_post_level_table(post_level_table, config.PROCESSED_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps\n",
    "\n",
    "### 9.1 Collection Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COLLECTION SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘           NOTEBOOK 1: DATA COLLECTION COMPLETE               â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "OUTPUT FILES:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "â€¢ wsb_posts_raw.parquet      - Primary post-level dataset\n",
    "â€¢ wsb_posts_sample.csv       - CSV sample for inspection\n",
    "â€¢ data_dictionary.json       - Variable definitions\n",
    "â€¢ collection_summary.json    - Summary statistics\n",
    "\n",
    "DATA SCHEMA:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "[platform_post_id, ticker, date, time, text, score, author, subreddit]\n",
    "\n",
    "NEXT STEPS:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "â†’ Notebook 2: Text Processing & Sentiment Analysis\n",
    "  - Clean text data\n",
    "  - Apply FinBERT sentiment scoring\n",
    "  - Aggregate to firm-day level\n",
    "\n",
    "IMPORTANT NOTES:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "1. Verify data quality before proceeding\n",
    "2. Check ticker coverage is sufficient (>15% recommended)\n",
    "3. Review temporal gaps in data\n",
    "4. Document any data collection issues\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ENVIRONMENT INFO FOR REPRODUCIBILITY\n",
    "# =============================================================================\n",
    "\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "print(\"Environment Information:\")\n",
    "print(f\"  Python: {sys.version}\")\n",
    "print(f\"  Platform: {platform.platform()}\")\n",
    "print(f\"  Pandas: {pd.__version__}\")\n",
    "print(f\"  NumPy: {np.__version__}\")\n",
    "print(f\"  Timestamp: {datetime.now().isoformat()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

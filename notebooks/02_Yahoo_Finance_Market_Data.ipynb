{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 2: Yahoo Finance Market Data Collection\n",
        "## Social Media-Driven Stock Manipulation and Tail Risk Research\n",
        "\n",
        "---\n",
        "\n",
        "**Research Project:** Social Media-Driven Stock Manipulation and Tail Risk\n",
        "\n",
        "**Purpose:** Collect and process daily OHLCV (Open, High, Low, Close, Volume) data from Yahoo Finance for the stock universe. Compute baseline statistics for episode detection.\n",
        "\n",
        "**Data Source:** Yahoo Finance via yfinance library (no API key required)\n",
        "\n",
        "**Output:** \n",
        "- Daily price-volume data for all universe tickers\n",
        "- Rolling baseline statistics (mean, std, percentiles)\n",
        "- Candidate price-volume spike events\n",
        "\n",
        "---\n",
        "\n",
        "**Last Updated:** 2025"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# INSTALL REQUIRED PACKAGES\n",
        "# =============================================================================\n",
        "\n",
        "!pip install yfinance==0.2.33\n",
        "!pip install pandas==2.0.3\n",
        "!pip install numpy==1.24.3\n",
        "!pip install scipy==1.11.4\n",
        "!pip install tqdm==4.66.1\n",
        "!pip install pyarrow==14.0.1\n",
        "!pip install matplotlib==3.8.2\n",
        "!pip install seaborn==0.13.0\n",
        "\n",
        "print(\"All packages installed successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# IMPORT LIBRARIES\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from tqdm.notebook import tqdm\n",
        "import yfinance as yf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', 100)\n",
        "\n",
        "# Plotting style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette('husl')\n",
        "\n",
        "print(f\"Environment setup complete. Timestamp: {datetime.now()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration and Load Universe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "class ResearchConfig:\n",
        "    \"\"\"Configuration for market data collection.\"\"\"\n",
        "    \n",
        "    # Sample Period\n",
        "    START_DATE = \"2019-01-01\"\n",
        "    END_DATE = \"2025-12-31\"\n",
        "    \n",
        "    # Rolling Statistics Parameters\n",
        "    ROLLING_WINDOW = 60  # days for baseline calculation\n",
        "    MIN_PERIODS = 20     # minimum observations for rolling stats\n",
        "    \n",
        "    # Episode Detection Thresholds (applied in Notebook 4)\n",
        "    RETURN_ZSCORE_THRESHOLD = 3.0\n",
        "    VOLUME_PERCENTILE_THRESHOLD = 95\n",
        "    PRICE_THRESHOLD = 10.0  # Max price for penny stock filter\n",
        "    \n",
        "    # Data Storage Paths\n",
        "    BASE_PATH = \"/content/drive/MyDrive/Research/PumpDump/\"\n",
        "    RAW_DATA_PATH = BASE_PATH + \"data/raw/\"\n",
        "    PROCESSED_DATA_PATH = BASE_PATH + \"data/processed/\"\n",
        "    \n",
        "    # Scraping Parameters\n",
        "    BATCH_SIZE = 20  # tickers per batch for yfinance\n",
        "    SLEEP_BETWEEN_BATCHES = 2  # seconds\n",
        "\n",
        "config = ResearchConfig()\n",
        "\n",
        "# Handle Colab vs local\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    print(\"Not running in Colab - using local paths\")\n",
        "    IN_COLAB = False\n",
        "    config.BASE_PATH = \"./research_data/\"\n",
        "    config.RAW_DATA_PATH = config.BASE_PATH + \"data/raw/\"\n",
        "    config.PROCESSED_DATA_PATH = config.BASE_PATH + \"data/processed/\"\n",
        "\n",
        "os.makedirs(config.RAW_DATA_PATH, exist_ok=True)\n",
        "os.makedirs(config.PROCESSED_DATA_PATH, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# LOAD STOCK UNIVERSE FROM NOTEBOOK 1\n",
        "# =============================================================================\n",
        "\n",
        "def load_universe(data_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Load stock universe from Notebook 1 output.\"\"\"\n",
        "    \n",
        "    universe_path = os.path.join(data_path, 'stock_universe.parquet')\n",
        "    \n",
        "    if os.path.exists(universe_path):\n",
        "        universe = pd.read_parquet(universe_path)\n",
        "        print(f\"Loaded universe: {len(universe)} tickers\")\n",
        "    else:\n",
        "        print(\"Universe file not found - creating sample universe\")\n",
        "        # Create sample universe for demonstration\n",
        "        sample_tickers = [\n",
        "            'GME', 'AMC', 'BB', 'NOK', 'BBBY', 'KOSS', 'CLOV', 'WISH',\n",
        "            'PLTR', 'SPCE', 'TLRY', 'SNDL', 'LCID', 'RIVN', 'MULN',\n",
        "            'FFIE', 'OCGN', 'NVAX', 'INO', 'ATER'\n",
        "        ]\n",
        "        universe = pd.DataFrame({\n",
        "            'ticker': sample_tickers,\n",
        "            'is_confirmed_manipulation': [False] * len(sample_tickers),\n",
        "            'source': ['sample'] * len(sample_tickers)\n",
        "        })\n",
        "    \n",
        "    return universe\n",
        "\n",
        "universe_df = load_universe(config.PROCESSED_DATA_PATH)\n",
        "print(f\"\\nUniverse composition:\")\n",
        "print(universe_df['source'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Yahoo Finance Data Collection\n",
        "\n",
        "### 3.1 Price-Volume Data Scraper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# YAHOO FINANCE DATA COLLECTOR\n",
        "# =============================================================================\n",
        "\n",
        "class YahooFinanceCollector:\n",
        "    \"\"\"Collects daily OHLCV data from Yahoo Finance.\n",
        "    \n",
        "    Uses yfinance library which scrapes Yahoo Finance without API key.\n",
        "    Implements batching and rate limiting for stability.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config: ResearchConfig):\n",
        "        self.config = config\n",
        "        self.failed_tickers = []\n",
        "        \n",
        "    def get_single_ticker_data(self, ticker: str, \n",
        "                                start: str, end: str) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"Get OHLCV data for a single ticker.\n",
        "        \n",
        "        Args:\n",
        "            ticker: Stock ticker symbol\n",
        "            start: Start date (YYYY-MM-DD)\n",
        "            end: End date (YYYY-MM-DD)\n",
        "            \n",
        "        Returns:\n",
        "            DataFrame with OHLCV data or None if failed\n",
        "        \"\"\"\n",
        "        try:\n",
        "            stock = yf.Ticker(ticker)\n",
        "            df = stock.history(start=start, end=end, auto_adjust=True)\n",
        "            \n",
        "            if len(df) == 0:\n",
        "                return None\n",
        "            \n",
        "            # Clean up DataFrame\n",
        "            df = df[['Open', 'High', 'Low', 'Close', 'Volume']].copy()\n",
        "            df['ticker'] = ticker\n",
        "            df = df.reset_index()\n",
        "            df.columns = ['date', 'open', 'high', 'low', 'close', 'volume', 'ticker']\n",
        "            df['date'] = pd.to_datetime(df['date']).dt.date\n",
        "            \n",
        "            return df\n",
        "            \n",
        "        except Exception as e:\n",
        "            return None\n",
        "    \n",
        "    def get_batch_data(self, tickers: List[str], \n",
        "                       start: str, end: str) -> pd.DataFrame:\n",
        "        \"\"\"Get OHLCV data for multiple tickers using yfinance batch download.\n",
        "        \n",
        "        Args:\n",
        "            tickers: List of ticker symbols\n",
        "            start: Start date\n",
        "            end: End date\n",
        "            \n",
        "        Returns:\n",
        "            Combined DataFrame with all ticker data\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Download batch\n",
        "            data = yf.download(\n",
        "                tickers=tickers,\n",
        "                start=start,\n",
        "                end=end,\n",
        "                auto_adjust=True,\n",
        "                progress=False,\n",
        "                threads=True\n",
        "            )\n",
        "            \n",
        "            if len(data) == 0:\n",
        "                return pd.DataFrame()\n",
        "            \n",
        "            # Reshape from multi-index columns to long format\n",
        "            records = []\n",
        "            \n",
        "            # Handle single vs multiple ticker response\n",
        "            if isinstance(data.columns, pd.MultiIndex):\n",
        "                for ticker in tickers:\n",
        "                    try:\n",
        "                        ticker_data = data.xs(ticker, axis=1, level=1)\n",
        "                        ticker_data = ticker_data.reset_index()\n",
        "                        ticker_data['ticker'] = ticker\n",
        "                        records.append(ticker_data)\n",
        "                    except KeyError:\n",
        "                        self.failed_tickers.append(ticker)\n",
        "            else:\n",
        "                # Single ticker case\n",
        "                data = data.reset_index()\n",
        "                data['ticker'] = tickers[0]\n",
        "                records.append(data)\n",
        "            \n",
        "            if not records:\n",
        "                return pd.DataFrame()\n",
        "            \n",
        "            df = pd.concat(records, ignore_index=True)\n",
        "            \n",
        "            # Standardize column names\n",
        "            df.columns = df.columns.str.lower()\n",
        "            if 'date' not in df.columns and 'index' in df.columns:\n",
        "                df = df.rename(columns={'index': 'date'})\n",
        "            \n",
        "            df['date'] = pd.to_datetime(df['date']).dt.date\n",
        "            \n",
        "            return df\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Batch download error: {e}\")\n",
        "            return pd.DataFrame()\n",
        "    \n",
        "    def collect_all_data(self, tickers: List[str]) -> pd.DataFrame:\n",
        "        \"\"\"Collect data for all tickers with batching and rate limiting.\n",
        "        \n",
        "        Args:\n",
        "            tickers: List of all ticker symbols\n",
        "            \n",
        "        Returns:\n",
        "            Combined DataFrame with all price-volume data\n",
        "        \"\"\"\n",
        "        print(f\"Collecting data for {len(tickers)} tickers\")\n",
        "        print(f\"Period: {self.config.START_DATE} to {self.config.END_DATE}\")\n",
        "        print(f\"Batch size: {self.config.BATCH_SIZE}\")\n",
        "        \n",
        "        all_data = []\n",
        "        \n",
        "        # Split into batches\n",
        "        batches = [tickers[i:i+self.config.BATCH_SIZE] \n",
        "                   for i in range(0, len(tickers), self.config.BATCH_SIZE)]\n",
        "        \n",
        "        for batch in tqdm(batches, desc=\"Downloading batches\"):\n",
        "            batch_data = self.get_batch_data(\n",
        "                tickers=batch,\n",
        "                start=self.config.START_DATE,\n",
        "                end=self.config.END_DATE\n",
        "            )\n",
        "            \n",
        "            if len(batch_data) > 0:\n",
        "                all_data.append(batch_data)\n",
        "            \n",
        "            # Rate limiting\n",
        "            time.sleep(self.config.SLEEP_BETWEEN_BATCHES)\n",
        "        \n",
        "        if not all_data:\n",
        "            print(\"No data collected!\")\n",
        "            return pd.DataFrame()\n",
        "        \n",
        "        combined = pd.concat(all_data, ignore_index=True)\n",
        "        \n",
        "        # Remove duplicates\n",
        "        combined = combined.drop_duplicates(subset=['ticker', 'date'])\n",
        "        \n",
        "        # Sort\n",
        "        combined = combined.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
        "        \n",
        "        print(f\"\\nCollection complete:\")\n",
        "        print(f\"  Total records: {len(combined):,}\")\n",
        "        print(f\"  Unique tickers: {combined['ticker'].nunique()}\")\n",
        "        print(f\"  Date range: {combined['date'].min()} to {combined['date'].max()}\")\n",
        "        print(f\"  Failed tickers: {len(self.failed_tickers)}\")\n",
        "        \n",
        "        return combined\n",
        "\n",
        "\n",
        "# Initialize collector\n",
        "yf_collector = YahooFinanceCollector(config)\n",
        "print(\"Yahoo Finance Collector initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# EXECUTE DATA COLLECTION\n",
        "# =============================================================================\n",
        "\n",
        "# Get list of tickers from universe\n",
        "tickers = universe_df['ticker'].unique().tolist()\n",
        "\n",
        "print(f\"Collecting market data for {len(tickers)} tickers...\")\n",
        "print(\"This may take several minutes depending on universe size.\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Collect data\n",
        "price_data = yf_collector.collect_all_data(tickers)\n",
        "\n",
        "# Display sample\n",
        "print(\"\\nSample of collected data:\")\n",
        "print(price_data.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Compute Baseline Statistics\n",
        "\n",
        "### 4.1 Rolling Statistics for Each Stock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# BASELINE STATISTICS CALCULATOR\n",
        "# =============================================================================\n",
        "\n",
        "class BaselineCalculator:\n",
        "    \"\"\"Computes rolling baseline statistics for episode detection.\n",
        "    \n",
        "    For each stock on each day, computes:\n",
        "    - Rolling mean and std of returns (60-day)\n",
        "    - Rolling percentiles of volume (60-day)\n",
        "    - Z-scores for anomaly detection\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, window: int = 60, min_periods: int = 20):\n",
        "        self.window = window\n",
        "        self.min_periods = min_periods\n",
        "        \n",
        "    def compute_returns(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Compute daily returns for each ticker.\"\"\"\n",
        "        df = df.copy()\n",
        "        df = df.sort_values(['ticker', 'date'])\n",
        "        \n",
        "        # Simple return\n",
        "        df['return'] = df.groupby('ticker')['close'].pct_change()\n",
        "        \n",
        "        # Log return (for normality)\n",
        "        df['log_return'] = np.log(df['close'] / df.groupby('ticker')['close'].shift(1))\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def compute_rolling_stats(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Compute rolling statistics for returns and volume.\"\"\"\n",
        "        df = df.copy()\n",
        "        \n",
        "        print(\"Computing rolling statistics...\")\n",
        "        \n",
        "        # Group by ticker and compute rolling stats\n",
        "        for col in tqdm(['return', 'volume'], desc=\"Computing stats\"):\n",
        "            # Rolling mean\n",
        "            df[f'{col}_mean_{self.window}d'] = df.groupby('ticker')[col].transform(\n",
        "                lambda x: x.rolling(window=self.window, min_periods=self.min_periods).mean()\n",
        "            )\n",
        "            \n",
        "            # Rolling std\n",
        "            df[f'{col}_std_{self.window}d'] = df.groupby('ticker')[col].transform(\n",
        "                lambda x: x.rolling(window=self.window, min_periods=self.min_periods).std()\n",
        "            )\n",
        "            \n",
        "            # Rolling median (more robust)\n",
        "            df[f'{col}_median_{self.window}d'] = df.groupby('ticker')[col].transform(\n",
        "                lambda x: x.rolling(window=self.window, min_periods=self.min_periods).median()\n",
        "            )\n",
        "        \n",
        "        # Volume percentiles\n",
        "        for pct in [90, 95, 99]:\n",
        "            df[f'volume_pct{pct}_{self.window}d'] = df.groupby('ticker')['volume'].transform(\n",
        "                lambda x: x.rolling(window=self.window, min_periods=self.min_periods).quantile(pct/100)\n",
        "            )\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def compute_zscores(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Compute z-scores for anomaly detection.\"\"\"\n",
        "        df = df.copy()\n",
        "        \n",
        "        # Return z-score\n",
        "        df['return_zscore'] = (\n",
        "            (df['return'] - df[f'return_mean_{self.window}d']) / \n",
        "            df[f'return_std_{self.window}d']\n",
        "        )\n",
        "        \n",
        "        # Volume z-score\n",
        "        df['volume_zscore'] = (\n",
        "            (df['volume'] - df[f'volume_mean_{self.window}d']) / \n",
        "            df[f'volume_std_{self.window}d']\n",
        "        )\n",
        "        \n",
        "        # Volume as ratio to median (more interpretable)\n",
        "        df['volume_ratio'] = df['volume'] / df[f'volume_median_{self.window}d']\n",
        "        \n",
        "        # Turnover (volume relative to average)\n",
        "        df['turnover_ratio'] = df['volume'] / df[f'volume_mean_{self.window}d']\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def compute_price_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Compute additional price-based features.\"\"\"\n",
        "        df = df.copy()\n",
        "        \n",
        "        # Intraday range\n",
        "        df['intraday_range'] = (df['high'] - df['low']) / df['close']\n",
        "        \n",
        "        # Gap (open vs previous close)\n",
        "        df['gap'] = (df['open'] - df.groupby('ticker')['close'].shift(1)) / df.groupby('ticker')['close'].shift(1)\n",
        "        \n",
        "        # Close position in range\n",
        "        df['close_position'] = (df['close'] - df['low']) / (df['high'] - df['low'] + 1e-10)\n",
        "        \n",
        "        # Cumulative returns\n",
        "        df['return_5d'] = df.groupby('ticker')['return'].transform(\n",
        "            lambda x: x.rolling(5, min_periods=1).sum()\n",
        "        )\n",
        "        df['return_20d'] = df.groupby('ticker')['return'].transform(\n",
        "            lambda x: x.rolling(20, min_periods=1).sum()\n",
        "        )\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def process_all(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Run complete baseline calculation pipeline.\"\"\"\n",
        "        print(\"=\"*60)\n",
        "        print(\"COMPUTING BASELINE STATISTICS\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        # Step 1: Returns\n",
        "        df = self.compute_returns(df)\n",
        "        print(f\"Returns computed: {df['return'].notna().sum():,} observations\")\n",
        "        \n",
        "        # Step 2: Rolling statistics\n",
        "        df = self.compute_rolling_stats(df)\n",
        "        \n",
        "        # Step 3: Z-scores\n",
        "        df = self.compute_zscores(df)\n",
        "        print(f\"Z-scores computed\")\n",
        "        \n",
        "        # Step 4: Price features\n",
        "        df = self.compute_price_features(df)\n",
        "        print(f\"Price features computed\")\n",
        "        \n",
        "        print(\"\\nBaseline computation complete\")\n",
        "        print(f\"Total columns: {len(df.columns)}\")\n",
        "        \n",
        "        return df\n",
        "\n",
        "\n",
        "# Initialize calculator\n",
        "baseline_calc = BaselineCalculator(\n",
        "    window=config.ROLLING_WINDOW, \n",
        "    min_periods=config.MIN_PERIODS\n",
        ")\n",
        "print(\"Baseline Calculator initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# COMPUTE BASELINES\n",
        "# =============================================================================\n",
        "\n",
        "# Process data\n",
        "if len(price_data) > 0:\n",
        "    market_data = baseline_calc.process_all(price_data)\n",
        "    \n",
        "    print(\"\\nData Summary:\")\n",
        "    print(market_data[['return', 'return_zscore', 'volume', 'volume_zscore', 'volume_ratio']].describe())\n",
        "else:\n",
        "    print(\"No price data to process\")\n",
        "    market_data = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Identify Candidate Price-Volume Events\n",
        "\n",
        "### 5.1 Flag Days with Extreme Price-Volume Activity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CANDIDATE EVENT DETECTOR\n",
        "# =============================================================================\n",
        "\n",
        "class CandidateEventDetector:\n",
        "    \"\"\"Identifies candidate pump-and-dump events based on price-volume anomalies.\n",
        "    \n",
        "    A day is flagged as candidate if ALL conditions hold:\n",
        "    1. Return z-score > threshold (extreme positive return)\n",
        "    2. Volume > 95th percentile of own history (unusual volume)\n",
        "    3. Price < $10 (penny stock filter)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, \n",
        "                 return_threshold: float = 3.0,\n",
        "                 volume_percentile: int = 95,\n",
        "                 price_threshold: float = 10.0):\n",
        "        self.return_threshold = return_threshold\n",
        "        self.volume_percentile = volume_percentile\n",
        "        self.price_threshold = price_threshold\n",
        "        \n",
        "    def flag_candidates(self, df: pd.DataFrame, window: int = 60) -> pd.DataFrame:\n",
        "        \"\"\"Flag candidate price-volume spike days.\n",
        "        \n",
        "        Args:\n",
        "            df: DataFrame with baseline statistics\n",
        "            window: Rolling window used for baselines\n",
        "            \n",
        "        Returns:\n",
        "            DataFrame with candidate flags\n",
        "        \"\"\"\n",
        "        df = df.copy()\n",
        "        \n",
        "        # Volume threshold column name\n",
        "        vol_col = f'volume_pct{self.volume_percentile}_{window}d'\n",
        "        \n",
        "        # Previous day close (for price filter)\n",
        "        df['prev_close'] = df.groupby('ticker')['close'].shift(1)\n",
        "        \n",
        "        # Condition 1: Extreme positive return\n",
        "        cond_return = df['return_zscore'] > self.return_threshold\n",
        "        \n",
        "        # Condition 2: Extreme volume\n",
        "        if vol_col in df.columns:\n",
        "            cond_volume = df['volume'] > df[vol_col]\n",
        "        else:\n",
        "            # Fallback to z-score based\n",
        "            cond_volume = df['volume_zscore'] > 2.0\n",
        "        \n",
        "        # Condition 3: Penny stock price\n",
        "        cond_price = df['prev_close'] < self.price_threshold\n",
        "        \n",
        "        # Combined candidate flag\n",
        "        df['is_price_spike'] = cond_return\n",
        "        df['is_volume_spike'] = cond_volume\n",
        "        df['is_penny_stock'] = cond_price\n",
        "        df['is_candidate_event'] = cond_return & cond_volume & cond_price\n",
        "        \n",
        "        # Also flag without penny stock filter (for robustness)\n",
        "        df['is_candidate_any_price'] = cond_return & cond_volume\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def summarize_candidates(self, df: pd.DataFrame) -> Dict:\n",
        "        \"\"\"Generate summary statistics for candidate events.\"\"\"\n",
        "        summary = {\n",
        "            'total_days': len(df),\n",
        "            'unique_tickers': df['ticker'].nunique(),\n",
        "            'price_spikes': int(df['is_price_spike'].sum()),\n",
        "            'volume_spikes': int(df['is_volume_spike'].sum()),\n",
        "            'penny_stock_days': int(df['is_penny_stock'].sum()),\n",
        "            'candidate_events': int(df['is_candidate_event'].sum()),\n",
        "            'candidate_any_price': int(df['is_candidate_any_price'].sum()),\n",
        "            'tickers_with_candidates': int(df[df['is_candidate_event']]['ticker'].nunique())\n",
        "        }\n",
        "        \n",
        "        # Candidates by ticker\n",
        "        ticker_counts = df[df['is_candidate_event']].groupby('ticker').size()\n",
        "        summary['top_candidate_tickers'] = ticker_counts.nlargest(10).to_dict()\n",
        "        \n",
        "        return summary\n",
        "    \n",
        "    def get_candidate_events(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Extract DataFrame of candidate events only.\"\"\"\n",
        "        candidates = df[df['is_candidate_event']].copy()\n",
        "        candidates = candidates.sort_values(['ticker', 'date'])\n",
        "        return candidates\n",
        "\n",
        "\n",
        "# Initialize detector\n",
        "event_detector = CandidateEventDetector(\n",
        "    return_threshold=config.RETURN_ZSCORE_THRESHOLD,\n",
        "    volume_percentile=config.VOLUME_PERCENTILE_THRESHOLD,\n",
        "    price_threshold=config.PRICE_THRESHOLD\n",
        ")\n",
        "print(\"Candidate Event Detector initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# FLAG CANDIDATE EVENTS\n",
        "# =============================================================================\n",
        "\n",
        "if len(market_data) > 0:\n",
        "    # Flag candidates\n",
        "    market_data = event_detector.flag_candidates(market_data, window=config.ROLLING_WINDOW)\n",
        "    \n",
        "    # Summarize\n",
        "    summary = event_detector.summarize_candidates(market_data)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"CANDIDATE EVENT DETECTION RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Total trading days: {summary['total_days']:,}\")\n",
        "    print(f\"Unique tickers: {summary['unique_tickers']}\")\n",
        "    print(f\"\\nSpike Detection:\")\n",
        "    print(f\"  Price spikes (z > {config.RETURN_ZSCORE_THRESHOLD}): {summary['price_spikes']:,}\")\n",
        "    print(f\"  Volume spikes (> {config.VOLUME_PERCENTILE_THRESHOLD}th pct): {summary['volume_spikes']:,}\")\n",
        "    print(f\"  Penny stock days (< ${config.PRICE_THRESHOLD}): {summary['penny_stock_days']:,}\")\n",
        "    print(f\"\\nCandidate Events (joint):\")\n",
        "    print(f\"  With penny stock filter: {summary['candidate_events']:,}\")\n",
        "    print(f\"  Without price filter: {summary['candidate_any_price']:,}\")\n",
        "    print(f\"  Unique tickers with candidates: {summary['tickers_with_candidates']}\")\n",
        "    print(f\"\\nTop tickers by candidate count:\")\n",
        "    for ticker, count in list(summary['top_candidate_tickers'].items())[:10]:\n",
        "        print(f\"    {ticker}: {count}\")\n",
        "else:\n",
        "    print(\"No market data to analyze\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# VISUALIZATIONS\n",
        "# =============================================================================\n",
        "\n",
        "def plot_candidate_distribution(df: pd.DataFrame):\n",
        "    \"\"\"Plot distribution of candidate events over time.\"\"\"\n",
        "    if len(df) == 0:\n",
        "        print(\"No data to plot\")\n",
        "        return\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    \n",
        "    # 1. Return z-score distribution\n",
        "    ax1 = axes[0, 0]\n",
        "    data = df['return_zscore'].dropna()\n",
        "    data = data[data.between(-10, 10)]  # Clip outliers for visualization\n",
        "    ax1.hist(data, bins=100, edgecolor='black', alpha=0.7)\n",
        "    ax1.axvline(x=config.RETURN_ZSCORE_THRESHOLD, color='red', linestyle='--', label=f'Threshold ({config.RETURN_ZSCORE_THRESHOLD})')\n",
        "    ax1.set_xlabel('Return Z-Score')\n",
        "    ax1.set_ylabel('Frequency')\n",
        "    ax1.set_title('Distribution of Return Z-Scores')\n",
        "    ax1.legend()\n",
        "    \n",
        "    # 2. Volume ratio distribution\n",
        "    ax2 = axes[0, 1]\n",
        "    data = df['volume_ratio'].dropna()\n",
        "    data = data[data.between(0, 20)]  # Clip for visualization\n",
        "    ax2.hist(data, bins=100, edgecolor='black', alpha=0.7, color='orange')\n",
        "    ax2.axvline(x=2.0, color='red', linestyle='--', label='2x Median')\n",
        "    ax2.set_xlabel('Volume / Median Volume')\n",
        "    ax2.set_ylabel('Frequency')\n",
        "    ax2.set_title('Distribution of Volume Ratios')\n",
        "    ax2.legend()\n",
        "    \n",
        "    # 3. Candidate events over time\n",
        "    ax3 = axes[1, 0]\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    monthly_candidates = df.groupby(df['date'].dt.to_period('M'))['is_candidate_event'].sum()\n",
        "    monthly_candidates.plot(ax=ax3, kind='bar', color='green', alpha=0.7)\n",
        "    ax3.set_xlabel('Month')\n",
        "    ax3.set_ylabel('Candidate Events')\n",
        "    ax3.set_title('Candidate Events Over Time')\n",
        "    ax3.tick_params(axis='x', rotation=45)\n",
        "    # Show only every 6th label\n",
        "    for i, label in enumerate(ax3.xaxis.get_ticklabels()):\n",
        "        if i % 6 != 0:\n",
        "            label.set_visible(False)\n",
        "    \n",
        "    # 4. Price at candidate events\n",
        "    ax4 = axes[1, 1]\n",
        "    candidates = df[df['is_candidate_event']]\n",
        "    if len(candidates) > 0:\n",
        "        ax4.hist(candidates['prev_close'].dropna(), bins=50, edgecolor='black', alpha=0.7, color='purple')\n",
        "        ax4.set_xlabel('Price ($)')\n",
        "        ax4.set_ylabel('Frequency')\n",
        "        ax4.set_title('Price Distribution at Candidate Events')\n",
        "    else:\n",
        "        ax4.text(0.5, 0.5, 'No candidate events', ha='center', va='center', transform=ax4.transAxes)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(config.PROCESSED_DATA_PATH, 'candidate_distributions.png'), dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_example_spike(df: pd.DataFrame, ticker: str = None):\n",
        "    \"\"\"Plot example of a price-volume spike event.\"\"\"\n",
        "    if len(df) == 0:\n",
        "        print(\"No data to plot\")\n",
        "        return\n",
        "    \n",
        "    # Find a ticker with candidate events\n",
        "    if ticker is None:\n",
        "        candidates = df[df['is_candidate_event']]\n",
        "        if len(candidates) == 0:\n",
        "            print(\"No candidate events to plot\")\n",
        "            return\n",
        "        ticker = candidates['ticker'].value_counts().index[0]\n",
        "    \n",
        "    ticker_data = df[df['ticker'] == ticker].copy()\n",
        "    ticker_data = ticker_data.sort_values('date')\n",
        "    \n",
        "    fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
        "    \n",
        "    # Price\n",
        "    ax1 = axes[0]\n",
        "    ax1.plot(ticker_data['date'], ticker_data['close'], color='blue', linewidth=1)\n",
        "    candidate_days = ticker_data[ticker_data['is_candidate_event']]\n",
        "    ax1.scatter(candidate_days['date'], candidate_days['close'], color='red', s=100, marker='^', label='Candidate Event', zorder=5)\n",
        "    ax1.set_ylabel('Close Price ($)')\n",
        "    ax1.set_title(f'{ticker} - Price with Candidate Events')\n",
        "    ax1.legend()\n",
        "    \n",
        "    # Volume\n",
        "    ax2 = axes[1]\n",
        "    ax2.bar(ticker_data['date'], ticker_data['volume'], color='gray', alpha=0.5, width=1)\n",
        "    ax2.scatter(candidate_days['date'], candidate_days['volume'], color='red', s=100, marker='^', zorder=5)\n",
        "    ax2.set_ylabel('Volume')\n",
        "    ax2.set_title(f'{ticker} - Volume')\n",
        "    \n",
        "    # Return Z-Score\n",
        "    ax3 = axes[2]\n",
        "    ax3.plot(ticker_data['date'], ticker_data['return_zscore'], color='green', linewidth=1)\n",
        "    ax3.axhline(y=config.RETURN_ZSCORE_THRESHOLD, color='red', linestyle='--', label=f'Threshold ({config.RETURN_ZSCORE_THRESHOLD})')\n",
        "    ax3.axhline(y=-config.RETURN_ZSCORE_THRESHOLD, color='red', linestyle='--')\n",
        "    ax3.scatter(candidate_days['date'], candidate_days['return_zscore'], color='red', s=100, marker='^', zorder=5)\n",
        "    ax3.set_ylabel('Return Z-Score')\n",
        "    ax3.set_xlabel('Date')\n",
        "    ax3.set_title(f'{ticker} - Return Z-Score')\n",
        "    ax3.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(config.PROCESSED_DATA_PATH, f'example_spike_{ticker}.png'), dpi=150)\n",
        "    plt.show()\n",
        "    \n",
        "    return ticker\n",
        "\n",
        "\n",
        "# Generate visualizations\n",
        "if len(market_data) > 0:\n",
        "    print(\"Generating visualizations...\")\n",
        "    plot_candidate_distribution(market_data)\n",
        "    example_ticker = plot_example_spike(market_data)\n",
        "    print(f\"\\nExample ticker plotted: {example_ticker}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save Outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SAVE OUTPUTS\n",
        "# =============================================================================\n",
        "\n",
        "def save_market_data(df: pd.DataFrame, output_dir: str):\n",
        "    \"\"\"Save market data with baseline statistics.\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # Save full dataset\n",
        "    full_path = os.path.join(output_dir, 'market_data_with_baselines.parquet')\n",
        "    df.to_parquet(full_path, index=False)\n",
        "    print(f\"Saved full market data: {full_path}\")\n",
        "    \n",
        "    # Save candidate events only\n",
        "    candidates = df[df['is_candidate_event']].copy()\n",
        "    candidates_path = os.path.join(output_dir, 'candidate_price_volume_events.parquet')\n",
        "    candidates.to_parquet(candidates_path, index=False)\n",
        "    print(f\"Saved candidate events: {candidates_path}\")\n",
        "    \n",
        "    # Save summary\n",
        "    summary = {\n",
        "        'total_observations': len(df),\n",
        "        'unique_tickers': int(df['ticker'].nunique()),\n",
        "        'date_range': [str(df['date'].min()), str(df['date'].max())],\n",
        "        'candidate_events': int(df['is_candidate_event'].sum()),\n",
        "        'tickers_with_candidates': int(df[df['is_candidate_event']]['ticker'].nunique()),\n",
        "        'thresholds': {\n",
        "            'return_zscore': config.RETURN_ZSCORE_THRESHOLD,\n",
        "            'volume_percentile': config.VOLUME_PERCENTILE_THRESHOLD,\n",
        "            'price': config.PRICE_THRESHOLD,\n",
        "            'rolling_window': config.ROLLING_WINDOW\n",
        "        },\n",
        "        'created_at': datetime.now().isoformat()\n",
        "    }\n",
        "    \n",
        "    summary_path = os.path.join(output_dir, 'notebook02_summary.json')\n",
        "    with open(summary_path, 'w') as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "    print(f\"Saved summary: {summary_path}\")\n",
        "    \n",
        "    return summary\n",
        "\n",
        "\n",
        "# Save outputs\n",
        "if len(market_data) > 0:\n",
        "    output_summary = save_market_data(market_data, config.PROCESSED_DATA_PATH)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Output Summary:\")\n",
        "    print(json.dumps(output_summary, indent=2))\n",
        "else:\n",
        "    print(\"No data to save\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary and Next Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# NOTEBOOK 2 SUMMARY\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\"\"\n",
        "╔══════════════════════════════════════════════════════════════════════════════╗\n",
        "║          NOTEBOOK 2: MARKET DATA COLLECTION COMPLETE                         ║\n",
        "╚══════════════════════════════════════════════════════════════════════════════╝\n",
        "\n",
        "OUTPUT FILES:\n",
        "─────────────\n",
        "• market_data_with_baselines.parquet   - Full OHLCV data with rolling stats\n",
        "• candidate_price_volume_events.parquet - Flagged candidate events\n",
        "• candidate_distributions.png           - Distribution plots\n",
        "• example_spike_{ticker}.png            - Example spike visualization\n",
        "• notebook02_summary.json               - Summary statistics\n",
        "\n",
        "KEY FEATURES COMPUTED:\n",
        "──────────────────────\n",
        "• Daily returns (simple and log)\n",
        "• 60-day rolling mean/std for returns and volume\n",
        "• Return z-scores\n",
        "• Volume z-scores and ratios\n",
        "• Volume percentiles (90th, 95th, 99th)\n",
        "• Candidate event flags\n",
        "\n",
        "CANDIDATE EVENT CRITERIA:\n",
        "─────────────────────────\n",
        "1. Return z-score > 3.0 (extreme positive return)\n",
        "2. Volume > 95th percentile of 60-day history\n",
        "3. Previous close < $10 (penny stock filter)\n",
        "\n",
        "NEXT STEPS:\n",
        "───────────\n",
        "→ Notebook 3: Yahoo Message Board Scraping\n",
        "  - Scrape social media discussion data\n",
        "  - Compute message volume baselines\n",
        "  - Identify social media bursts\n",
        "\n",
        "NOTE: These candidate events are based on PRICE-VOLUME only.\n",
        "Final episode detection will require joint price-volume AND social conditions.\n",
        "\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ENVIRONMENT INFO FOR REPRODUCIBILITY\n",
        "# =============================================================================\n",
        "\n",
        "import sys\n",
        "import platform\n",
        "\n",
        "print(\"Environment Information:\")\n",
        "print(f\"  Python: {sys.version}\")\n",
        "print(f\"  Platform: {platform.platform()}\")\n",
        "print(f\"  Pandas: {pd.__version__}\")\n",
        "print(f\"  NumPy: {np.__version__}\")\n",
        "print(f\"  yfinance: {yf.__version__}\")\n",
        "print(f\"  Timestamp: {datetime.now().isoformat()}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

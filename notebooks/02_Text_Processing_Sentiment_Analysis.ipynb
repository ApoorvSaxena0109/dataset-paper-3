{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Text Processing & Sentiment Analysis\n",
    "## FinBERT-Based Sentiment Scoring for Retail Investor Posts\n",
    "\n",
    "---\n",
    "\n",
    "**Research Project:** Retail Sentiment, Earnings Quality, and Stock Returns\n",
    "\n",
    "**Purpose:** Process raw social media text and generate sentiment scores using finance-specific transformer models.\n",
    "\n",
    "**Methodology:**\n",
    "- Text cleaning preserving financial context and WSB slang\n",
    "- FinBERT sentiment classification (ProsusAI/finbert)\n",
    "- Daily firm-level aggregation\n",
    "\n",
    "**Input:** `wsb_posts_raw.parquet` from Notebook 1\n",
    "\n",
    "**Output:** Firm-day panel with sentiment and attention metrics\n",
    "\n",
    "---\n",
    "\n",
    "**References:**\n",
    "- Araci, D. (2019). FinBERT: Financial Sentiment Analysis with Pre-trained Language Models\n",
    "- Huang, A. H., Wang, H., & Yang, Y. (2023). FinBERT: A Large Language Model for Extracting Information from Financial Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# INSTALL REQUIRED PACKAGES\n",
    "# =============================================================================\n",
    "\n",
    "!pip install transformers==4.35.0\n",
    "!pip install torch==2.1.0\n",
    "!pip install pandas==2.0.3\n",
    "!pip install numpy==1.24.3\n",
    "!pip install scipy==1.11.3\n",
    "!pip install tqdm==4.66.1\n",
    "!pip install pyarrow==14.0.1\n",
    "!pip install emoji==2.8.0\n",
    "!pip install nltk==3.8.1\n",
    "!pip install scikit-learn==1.3.2\n",
    "\n",
    "print(\"All packages installed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORT LIBRARIES\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# NLP Libraries\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    pipeline\n",
    ")\n",
    "import nltk\n",
    "import emoji\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "print(f\"\\nEnvironment setup complete. Timestamp: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "class SentimentConfig:\n",
    "    \"\"\"Configuration for sentiment analysis pipeline.\"\"\"\n",
    "    \n",
    "    # Data paths (Google Drive)\n",
    "    BASE_PATH = \"/content/drive/MyDrive/Research/RetailSentiment/\"\n",
    "    RAW_DATA_PATH = BASE_PATH + \"data/raw/\"\n",
    "    PROCESSED_DATA_PATH = BASE_PATH + \"data/processed/\"\n",
    "    \n",
    "    # Model selection\n",
    "    FINBERT_MODEL = \"ProsusAI/finbert\"  # Primary model\n",
    "    BACKUP_MODEL = \"yiyanghkust/finbert-tone\"  # Alternative\n",
    "    \n",
    "    # Processing parameters\n",
    "    MAX_SEQUENCE_LENGTH = 512  # FinBERT max tokens\n",
    "    BATCH_SIZE = 32  # Adjust based on GPU memory\n",
    "    \n",
    "    # Text cleaning parameters\n",
    "    MIN_TEXT_LENGTH = 10  # Characters\n",
    "    MAX_TEXT_LENGTH = 5000  # Truncate longer texts\n",
    "    \n",
    "    # Aggregation windows\n",
    "    TRADING_HOURS_START = 9  # EST\n",
    "    TRADING_HOURS_END = 16  # EST\n",
    "    \n",
    "    @classmethod\n",
    "    def print_config(cls):\n",
    "        print(\"=\"*60)\n",
    "        print(\"SENTIMENT ANALYSIS CONFIGURATION\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Model: {cls.FINBERT_MODEL}\")\n",
    "        print(f\"Max Sequence Length: {cls.MAX_SEQUENCE_LENGTH}\")\n",
    "        print(f\"Batch Size: {cls.BATCH_SIZE}\")\n",
    "        print(f\"Min Text Length: {cls.MIN_TEXT_LENGTH} chars\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "config = SentimentConfig()\n",
    "config.print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MOUNT GOOGLE DRIVE\n",
    "# =============================================================================\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Verify directories exist\n",
    "os.makedirs(config.PROCESSED_DATA_PATH, exist_ok=True)\n",
    "print(f\"Data directories verified.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD RAW POST DATA\n",
    "# =============================================================================\n",
    "\n",
    "def load_raw_data(data_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load raw post data from Notebook 1.\n",
    "    \n",
    "    Args:\n",
    "        data_path: Path to data directory\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with raw post data\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(data_path, 'wsb_posts_raw.parquet')\n",
    "    \n",
    "    print(f\"Loading data from: {filepath}\")\n",
    "    df = pd.read_parquet(filepath)\n",
    "    \n",
    "    print(f\"\\nLoaded {len(df):,} posts\")\n",
    "    print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "    print(f\"\\nColumns: {list(df.columns)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data\n",
    "raw_posts = load_raw_data(config.PROCESSED_DATA_PATH)\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample posts:\")\n",
    "raw_posts[['date', 'ticker', 'title', 'score']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Cleaning Pipeline\n",
    "\n",
    "### 3.1 WSB-Specific Text Cleaner\n",
    "\n",
    "We preserve financial terminology and WSB-specific language that carries sentiment information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TEXT CLEANING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "class WSBTextCleaner:\n",
    "    \"\"\"Text cleaner optimized for WSB/financial social media.\n",
    "    \n",
    "    Design principles:\n",
    "    1. Preserve sentiment-carrying elements (emojis, slang, capitalization)\n",
    "    2. Remove noise (URLs, HTML, bots)\n",
    "    3. Standardize format while maintaining readability\n",
    "    \"\"\"\n",
    "    \n",
    "    # WSB slang with sentiment implications\n",
    "    WSB_POSITIVE_TERMS = {\n",
    "        'moon', 'mooning', 'rocket', 'tendies', 'gains', 'bull', 'bullish',\n",
    "        'diamond hands', 'diamondhands', 'hodl', 'hold', 'squeeze', 'gamma',\n",
    "        'squeeze', 'calls', 'yolo', 'to the moon', 'stonks', 'apes', 'strong',\n",
    "        'lambo', 'printing', 'brrrr', 'free money', 'cant go tits up'\n",
    "    }\n",
    "    \n",
    "    WSB_NEGATIVE_TERMS = {\n",
    "        'bear', 'bearish', 'puts', 'short', 'drill', 'drilling', 'tanking',\n",
    "        'bagholding', 'bagholder', 'bag holder', 'loss', 'losses', 'red',\n",
    "        'rope', 'guh', 'rekt', 'wrecked', 'worthless', 'expire worthless',\n",
    "        'paper hands', 'paperhands', 'dump', 'dumping', 'crashed', 'margin call'\n",
    "    }\n",
    "    \n",
    "    # Emoji to text mapping for sentiment preservation\n",
    "    EMOJI_SENTIMENT = {\n",
    "        'ğŸš€': ' rocket bullish ',\n",
    "        'ğŸŒ™': ' moon bullish ',\n",
    "        'ğŸ’': ' diamond hands bullish ',\n",
    "        'ğŸ™Œ': ' hands holding ',\n",
    "        'ğŸ¦': ' ape strong ',\n",
    "        'ğŸ“ˆ': ' chart up bullish ',\n",
    "        'ğŸ“‰': ' chart down bearish ',\n",
    "        'ğŸ’°': ' money gains ',\n",
    "        'ğŸ’µ': ' money gains ',\n",
    "        'ğŸ‚': ' bull bullish ',\n",
    "        'ğŸ»': ' bear bearish ',\n",
    "        'ğŸ¤¡': ' clown negative ',\n",
    "        'ğŸ˜‚': ' laughing ',\n",
    "        'ğŸ˜­': ' crying sad ',\n",
    "        'ğŸ”¥': ' fire hot ',\n",
    "        'ğŸ’€': ' dead negative ',\n",
    "        'â¬†ï¸': ' up bullish ',\n",
    "        'â¬‡ï¸': ' down bearish ',\n",
    "        'âœ…': ' yes positive ',\n",
    "        'âŒ': ' no negative ',\n",
    "        'ğŸ°': ' gambling risky ',\n",
    "        'ğŸ²': ' gambling risky '\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Compile regex patterns\n",
    "        self.url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        self.html_pattern = re.compile(r'<[^>]+>')\n",
    "        self.reddit_pattern = re.compile(r'\\[removed\\]|\\[deleted\\]|\\[deleted by user\\]')\n",
    "        self.mention_pattern = re.compile(r'u/\\w+|r/\\w+')\n",
    "        self.ticker_clean = re.compile(r'\\$([A-Z]{1,5})\\b')\n",
    "        self.whitespace_pattern = re.compile(r'\\s+')\n",
    "        self.special_chars = re.compile(r'[^\\w\\s.,!?\\'-]')\n",
    "        \n",
    "    def clean_text(self, text: str, preserve_emojis: bool = True) -> str:\n",
    "        \"\"\"Clean text while preserving sentiment information.\n",
    "        \n",
    "        Args:\n",
    "            text: Raw text input\n",
    "            preserve_emojis: Whether to convert emojis to text\n",
    "            \n",
    "        Returns:\n",
    "            Cleaned text\n",
    "        \"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Remove deleted/removed markers\n",
    "        text = self.reddit_pattern.sub('', text)\n",
    "        \n",
    "        # Check if text is effectively empty after removal\n",
    "        if len(text.strip()) < 5:\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert emojis to sentiment text\n",
    "        if preserve_emojis:\n",
    "            for emoji_char, sentiment_text in self.EMOJI_SENTIMENT.items():\n",
    "                text = text.replace(emoji_char, sentiment_text)\n",
    "            # Remove remaining emojis\n",
    "            text = emoji.replace_emoji(text, replace=' ')\n",
    "        else:\n",
    "            text = emoji.replace_emoji(text, replace='')\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = self.url_pattern.sub(' ', text)\n",
    "        \n",
    "        # Remove HTML tags\n",
    "        text = self.html_pattern.sub(' ', text)\n",
    "        \n",
    "        # Standardize ticker format ($TICKER -> TICKER)\n",
    "        text = self.ticker_clean.sub(r'\\1', text)\n",
    "        \n",
    "        # Remove Reddit-specific mentions\n",
    "        text = self.mention_pattern.sub(' ', text)\n",
    "        \n",
    "        # Normalize whitespace\n",
    "        text = self.whitespace_pattern.sub(' ', text)\n",
    "        \n",
    "        # Strip and return\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def prepare_for_sentiment(self, text: str, max_length: int = 512) -> str:\n",
    "        \"\"\"Prepare text for sentiment model input.\n",
    "        \n",
    "        Args:\n",
    "            text: Cleaned text\n",
    "            max_length: Maximum character length\n",
    "            \n",
    "        Returns:\n",
    "            Model-ready text\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        # Truncate if needed (keep beginning, most relevant for posts)\n",
    "        if len(text) > max_length:\n",
    "            # Try to truncate at sentence boundary\n",
    "            truncated = text[:max_length]\n",
    "            last_period = truncated.rfind('.')\n",
    "            if last_period > max_length * 0.7:\n",
    "                text = truncated[:last_period + 1]\n",
    "            else:\n",
    "                text = truncated\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def combine_title_body(self, title: str, body: str, \n",
    "                          title_weight: int = 2) -> str:\n",
    "        \"\"\"Combine title and body for sentiment analysis.\n",
    "        \n",
    "        Title is typically more informative for sentiment, so we can\n",
    "        optionally weight it more heavily.\n",
    "        \n",
    "        Args:\n",
    "            title: Post title\n",
    "            body: Post body\n",
    "            title_weight: How many times to include title (1-3)\n",
    "            \n",
    "        Returns:\n",
    "            Combined text\n",
    "        \"\"\"\n",
    "        title = self.clean_text(title or '')\n",
    "        body = self.clean_text(body or '')\n",
    "        \n",
    "        if title and body:\n",
    "            # Repeat title for weighting\n",
    "            title_repeated = ' '.join([title] * min(title_weight, 3))\n",
    "            return f\"{title_repeated} {body}\"\n",
    "        elif title:\n",
    "            return title\n",
    "        else:\n",
    "            return body\n",
    "\n",
    "# Initialize cleaner\n",
    "text_cleaner = WSBTextCleaner()\n",
    "\n",
    "# Test cleaning\n",
    "test_texts = [\n",
    "    \"$GME to the moon! ğŸš€ğŸš€ğŸš€ Diamond hands baby!\",\n",
    "    \"AAPL puts are printing. Bear gang ğŸ»ğŸ“‰\",\n",
    "    \"Check out my DD: https://reddit.com/wsb/xxx [removed]\",\n",
    "    \"This is gonna squeeze so hard. Not financial advice.\"\n",
    "]\n",
    "\n",
    "print(\"Text Cleaning Examples:\")\n",
    "print(\"=\"*60)\n",
    "for text in test_texts:\n",
    "    cleaned = text_cleaner.clean_text(text)\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"Cleaned:  {cleaned}\")\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# APPLY TEXT CLEANING TO DATASET\n",
    "# =============================================================================\n",
    "\n",
    "def clean_dataset(df: pd.DataFrame, cleaner: WSBTextCleaner) -> pd.DataFrame:\n",
    "    \"\"\"Apply text cleaning to entire dataset.\n",
    "    \n",
    "    Args:\n",
    "        df: Raw posts DataFrame\n",
    "        cleaner: Text cleaner instance\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with cleaned text columns\n",
    "    \"\"\"\n",
    "    print(\"Cleaning text data...\")\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Clean individual columns\n",
    "    tqdm.pandas(desc=\"Cleaning titles\")\n",
    "    df['title_clean'] = df['title'].progress_apply(\n",
    "        lambda x: cleaner.clean_text(x) if pd.notna(x) else ''\n",
    "    )\n",
    "    \n",
    "    tqdm.pandas(desc=\"Cleaning text\")\n",
    "    df['text_clean'] = df['text'].progress_apply(\n",
    "        lambda x: cleaner.clean_text(x) if pd.notna(x) else ''\n",
    "    )\n",
    "    \n",
    "    # Create combined text for sentiment analysis\n",
    "    print(\"Creating combined text...\")\n",
    "    df['text_combined'] = df.apply(\n",
    "        lambda row: cleaner.combine_title_body(\n",
    "            row.get('title', ''), \n",
    "            row.get('text', '')\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Calculate text length\n",
    "    df['text_length'] = df['text_combined'].str.len()\n",
    "    \n",
    "    # Filter out empty/very short texts\n",
    "    original_count = len(df)\n",
    "    df = df[df['text_length'] >= config.MIN_TEXT_LENGTH].copy()\n",
    "    removed = original_count - len(df)\n",
    "    \n",
    "    print(f\"\\nCleaning complete:\")\n",
    "    print(f\"  Original posts: {original_count:,}\")\n",
    "    print(f\"  After cleaning: {len(df):,}\")\n",
    "    print(f\"  Removed (too short): {removed:,}\")\n",
    "    print(f\"  Average text length: {df['text_length'].mean():.0f} chars\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_posts = clean_dataset(raw_posts, text_cleaner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. FinBERT Sentiment Model\n",
    "\n",
    "### 4.1 Load and Initialize FinBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FINBERT SENTIMENT ANALYZER\n",
    "# =============================================================================\n",
    "\n",
    "class FinBERTSentimentAnalyzer:\n",
    "    \"\"\"Sentiment analyzer using FinBERT model.\n",
    "    \n",
    "    FinBERT is a BERT model pre-trained on financial text and fine-tuned\n",
    "    for financial sentiment analysis.\n",
    "    \n",
    "    Output classes:\n",
    "    - positive: Bullish/optimistic sentiment\n",
    "    - negative: Bearish/pessimistic sentiment  \n",
    "    - neutral: Factual/neutral content\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"ProsusAI/finbert\", device: str = None):\n",
    "        \"\"\"Initialize FinBERT model.\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model identifier\n",
    "            device: 'cuda' or 'cpu'\n",
    "        \"\"\"\n",
    "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Loading FinBERT model: {model_name}\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        \n",
    "        # Load tokenizer and model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Create pipeline for easier inference\n",
    "        self.pipeline = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            device=0 if self.device == 'cuda' else -1,\n",
    "            max_length=512,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        # Label mapping\n",
    "        self.label_map = {\n",
    "            'positive': 1,\n",
    "            'negative': -1,\n",
    "            'neutral': 0\n",
    "        }\n",
    "        \n",
    "        print(\"Model loaded successfully.\")\n",
    "        \n",
    "    def analyze_single(self, text: str) -> Dict:\n",
    "        \"\"\"Analyze sentiment of a single text.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with sentiment scores\n",
    "        \"\"\"\n",
    "        if not text or len(text.strip()) < 5:\n",
    "            return {\n",
    "                'label': 'neutral',\n",
    "                'score': 0.0,\n",
    "                'positive': 0.33,\n",
    "                'negative': 0.33,\n",
    "                'neutral': 0.34,\n",
    "                'polarity': 0.0\n",
    "            }\n",
    "        \n",
    "        # Get prediction\n",
    "        result = self.pipeline(text[:512])[0]\n",
    "        \n",
    "        # Get all class probabilities\n",
    "        inputs = self.tokenizer(\n",
    "            text[:512], \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            probs = torch.softmax(outputs.logits, dim=1)[0].cpu().numpy()\n",
    "        \n",
    "        # Map probabilities to labels\n",
    "        # ProsusAI/finbert: [positive, negative, neutral]\n",
    "        prob_dict = {\n",
    "            'positive': float(probs[0]),\n",
    "            'negative': float(probs[1]),\n",
    "            'neutral': float(probs[2])\n",
    "        }\n",
    "        \n",
    "        # Calculate polarity score: P(positive) - P(negative)\n",
    "        polarity = prob_dict['positive'] - prob_dict['negative']\n",
    "        \n",
    "        return {\n",
    "            'label': result['label'].lower(),\n",
    "            'score': result['score'],\n",
    "            'positive': prob_dict['positive'],\n",
    "            'negative': prob_dict['negative'],\n",
    "            'neutral': prob_dict['neutral'],\n",
    "            'polarity': polarity\n",
    "        }\n",
    "    \n",
    "    def analyze_batch(self, texts: List[str], batch_size: int = 32) -> List[Dict]:\n",
    "        \"\"\"Analyze sentiment of multiple texts in batches.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of input texts\n",
    "            batch_size: Batch size for processing\n",
    "            \n",
    "        Returns:\n",
    "            List of sentiment dictionaries\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Analyzing sentiment\"):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            \n",
    "            # Handle empty/short texts\n",
    "            valid_indices = []\n",
    "            valid_texts = []\n",
    "            batch_results = [None] * len(batch)\n",
    "            \n",
    "            for j, text in enumerate(batch):\n",
    "                if text and len(text.strip()) >= 5:\n",
    "                    valid_indices.append(j)\n",
    "                    valid_texts.append(text[:512])\n",
    "                else:\n",
    "                    batch_results[j] = {\n",
    "                        'label': 'neutral',\n",
    "                        'score': 0.0,\n",
    "                        'positive': 0.33,\n",
    "                        'negative': 0.33,\n",
    "                        'neutral': 0.34,\n",
    "                        'polarity': 0.0\n",
    "                    }\n",
    "            \n",
    "            if valid_texts:\n",
    "                # Tokenize batch\n",
    "                inputs = self.tokenizer(\n",
    "                    valid_texts,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=512\n",
    "                ).to(self.device)\n",
    "                \n",
    "                # Get predictions\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(**inputs)\n",
    "                    probs = torch.softmax(outputs.logits, dim=1).cpu().numpy()\n",
    "                \n",
    "                # Process results\n",
    "                for k, idx in enumerate(valid_indices):\n",
    "                    prob = probs[k]\n",
    "                    label_idx = np.argmax(prob)\n",
    "                    labels = ['positive', 'negative', 'neutral']\n",
    "                    \n",
    "                    batch_results[idx] = {\n",
    "                        'label': labels[label_idx],\n",
    "                        'score': float(prob[label_idx]),\n",
    "                        'positive': float(prob[0]),\n",
    "                        'negative': float(prob[1]),\n",
    "                        'neutral': float(prob[2]),\n",
    "                        'polarity': float(prob[0] - prob[1])\n",
    "                    }\n",
    "            \n",
    "            results.extend(batch_results)\n",
    "            \n",
    "            # Clear CUDA cache periodically\n",
    "            if i % (batch_size * 10) == 0 and self.device == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize analyzer\n",
    "print(\"Initializing FinBERT analyzer...\")\n",
    "sentiment_analyzer = FinBERTSentimentAnalyzer(\n",
    "    model_name=config.FINBERT_MODEL,\n",
    "    device=str(device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TEST SENTIMENT ANALYZER\n",
    "# =============================================================================\n",
    "\n",
    "# Test on sample texts\n",
    "test_sentences = [\n",
    "    \"GME is going to the moon! Diamond hands!\",\n",
    "    \"This stock is overvalued and will crash soon.\",\n",
    "    \"Earnings report comes out tomorrow.\",\n",
    "    \"I'm buying calls, extremely bullish on this company.\",\n",
    "    \"Selling everything, this is going to zero.\",\n",
    "    \"The company announced a new product launch.\",\n",
    "    \"YOLO'd my entire savings, can't go tits up.\",\n",
    "    \"Lost 90% of my portfolio on this garbage stock.\"\n",
    "]\n",
    "\n",
    "print(\"\\nSentiment Analysis Test Results:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Text':<50} {'Label':<10} {'Polarity':>10}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for text in test_sentences:\n",
    "    result = sentiment_analyzer.analyze_single(text)\n",
    "    print(f\"{text[:48]:<50} {result['label']:<10} {result['polarity']:>10.3f}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Apply Sentiment Analysis to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# APPLY SENTIMENT TO FULL DATASET\n",
    "# =============================================================================\n",
    "\n",
    "def apply_sentiment_analysis(\n",
    "    df: pd.DataFrame,\n",
    "    analyzer: FinBERTSentimentAnalyzer,\n",
    "    text_column: str = 'text_combined',\n",
    "    batch_size: int = 32\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Apply sentiment analysis to entire dataset.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with text data\n",
    "        analyzer: Sentiment analyzer instance\n",
    "        text_column: Column containing text to analyze\n",
    "        batch_size: Batch size for processing\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with sentiment columns added\n",
    "    \"\"\"\n",
    "    print(f\"\\nApplying sentiment analysis to {len(df):,} posts...\")\n",
    "    print(f\"Using column: {text_column}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Get texts\n",
    "    texts = df[text_column].fillna('').tolist()\n",
    "    \n",
    "    # Analyze in batches\n",
    "    results = analyzer.analyze_batch(texts, batch_size=batch_size)\n",
    "    \n",
    "    # Add results to DataFrame\n",
    "    df = df.copy()\n",
    "    df['sentiment_label'] = [r['label'] for r in results]\n",
    "    df['sentiment_score'] = [r['score'] for r in results]\n",
    "    df['prob_positive'] = [r['positive'] for r in results]\n",
    "    df['prob_negative'] = [r['negative'] for r in results]\n",
    "    df['prob_neutral'] = [r['neutral'] for r in results]\n",
    "    df['sentiment_polarity'] = [r['polarity'] for r in results]\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    duration = (end_time - start_time).total_seconds()\n",
    "    \n",
    "    print(f\"\\nSentiment analysis complete!\")\n",
    "    print(f\"Duration: {duration:.1f} seconds ({len(df)/duration:.1f} posts/sec)\")\n",
    "    \n",
    "    # Print distribution\n",
    "    print(f\"\\nSentiment Distribution:\")\n",
    "    dist = df['sentiment_label'].value_counts(normalize=True)\n",
    "    for label, pct in dist.items():\n",
    "        print(f\"  {label}: {pct*100:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nPolarity Statistics:\")\n",
    "    print(f\"  Mean: {df['sentiment_polarity'].mean():.3f}\")\n",
    "    print(f\"  Std:  {df['sentiment_polarity'].std():.3f}\")\n",
    "    print(f\"  Min:  {df['sentiment_polarity'].min():.3f}\")\n",
    "    print(f\"  Max:  {df['sentiment_polarity'].max():.3f}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply sentiment analysis\n",
    "posts_with_sentiment = apply_sentiment_analysis(\n",
    "    cleaned_posts,\n",
    "    sentiment_analyzer,\n",
    "    text_column='text_combined',\n",
    "    batch_size=config.BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE CHECKPOINT - POST-LEVEL SENTIMENT\n",
    "# =============================================================================\n",
    "\n",
    "# Save intermediate results\n",
    "checkpoint_path = os.path.join(config.PROCESSED_DATA_PATH, 'wsb_posts_sentiment.parquet')\n",
    "posts_with_sentiment.to_parquet(checkpoint_path, index=False)\n",
    "print(f\"Saved checkpoint: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Daily Firm-Level Aggregation\n",
    "\n",
    "### 5.1 Aggregate to Firm-Day Panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FIRM-DAY AGGREGATION\n",
    "# =============================================================================\n",
    "\n",
    "class FirmDayAggregator:\n",
    "    \"\"\"Aggregates post-level data to firm-day panel.\n",
    "    \n",
    "    Creates the following variables for each firm-day:\n",
    "    - PostCount: Number of posts mentioning the firm\n",
    "    - UniqueUsers: Distinct authors mentioning the firm\n",
    "    - SentimentMean: Average polarity score\n",
    "    - SentimentPosShare: Share of positive posts\n",
    "    - Karma: Sum of Reddit scores\n",
    "    - SentimentStd: Standard deviation of polarity (disagreement)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def aggregate(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Aggregate post-level data to firm-day level.\n",
    "        \n",
    "        Args:\n",
    "            df: Post-level DataFrame with sentiment scores\n",
    "            \n",
    "        Returns:\n",
    "            Firm-day panel DataFrame\n",
    "        \"\"\"\n",
    "        # Filter to posts with identified tickers\n",
    "        df_with_ticker = df[df['ticker'].notna()].copy()\n",
    "        print(f\"Posts with tickers: {len(df_with_ticker):,}\")\n",
    "        \n",
    "        # Ensure date is datetime\n",
    "        df_with_ticker['date'] = pd.to_datetime(df_with_ticker['date'])\n",
    "        \n",
    "        # Create binary sentiment indicators\n",
    "        df_with_ticker['is_positive'] = (df_with_ticker['sentiment_label'] == 'positive').astype(int)\n",
    "        df_with_ticker['is_negative'] = (df_with_ticker['sentiment_label'] == 'negative').astype(int)\n",
    "        df_with_ticker['is_neutral'] = (df_with_ticker['sentiment_label'] == 'neutral').astype(int)\n",
    "        \n",
    "        # Aggregation functions\n",
    "        agg_funcs = {\n",
    "            'platform_post_id': 'count',  # PostCount\n",
    "            'author': 'nunique',  # UniqueUsers\n",
    "            'sentiment_polarity': ['mean', 'std', 'median'],  # Sentiment stats\n",
    "            'is_positive': 'mean',  # Share positive\n",
    "            'is_negative': 'mean',  # Share negative\n",
    "            'score': ['sum', 'mean'],  # Karma\n",
    "            'prob_positive': 'mean',  # Average positive probability\n",
    "            'prob_negative': 'mean',  # Average negative probability\n",
    "            'num_comments': 'sum'  # Total comments (engagement)\n",
    "        }\n",
    "        \n",
    "        # Group by ticker and date\n",
    "        print(\"Aggregating to firm-day level...\")\n",
    "        firm_day = df_with_ticker.groupby(['ticker', 'date']).agg(agg_funcs)\n",
    "        \n",
    "        # Flatten column names\n",
    "        firm_day.columns = [\n",
    "            'PostCount',\n",
    "            'UniqueUsers',\n",
    "            'SentimentMean',\n",
    "            'SentimentStd',\n",
    "            'SentimentMedian',\n",
    "            'SentimentPosShare',\n",
    "            'SentimentNegShare',\n",
    "            'KarmaSum',\n",
    "            'KarmaMean',\n",
    "            'ProbPositiveMean',\n",
    "            'ProbNegativeMean',\n",
    "            'TotalComments'\n",
    "        ]\n",
    "        \n",
    "        # Reset index\n",
    "        firm_day = firm_day.reset_index()\n",
    "        \n",
    "        # Fill NaN sentiment std with 0 (single post days)\n",
    "        firm_day['SentimentStd'] = firm_day['SentimentStd'].fillna(0)\n",
    "        \n",
    "        # Add derived variables\n",
    "        firm_day['Attention'] = np.log1p(firm_day['PostCount'])  # Log attention\n",
    "        firm_day['Disagreement'] = firm_day['SentimentStd']  # Alias for clarity\n",
    "        firm_day['NetSentiment'] = firm_day['SentimentPosShare'] - firm_day['SentimentNegShare']\n",
    "        \n",
    "        print(f\"\\nFirm-day panel created:\")\n",
    "        print(f\"  Observations: {len(firm_day):,}\")\n",
    "        print(f\"  Unique firms: {firm_day['ticker'].nunique():,}\")\n",
    "        print(f\"  Date range: {firm_day['date'].min()} to {firm_day['date'].max()}\")\n",
    "        print(f\"  Avg posts per firm-day: {firm_day['PostCount'].mean():.1f}\")\n",
    "        \n",
    "        return firm_day\n",
    "    \n",
    "    def add_trading_day_indicators(self, firm_day: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Add trading day indicators for market analysis.\n",
    "        \n",
    "        Args:\n",
    "            firm_day: Firm-day panel\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with trading day columns\n",
    "        \"\"\"\n",
    "        firm_day = firm_day.copy()\n",
    "        \n",
    "        # Day of week\n",
    "        firm_day['DayOfWeek'] = firm_day['date'].dt.dayofweek\n",
    "        firm_day['IsWeekend'] = firm_day['DayOfWeek'].isin([5, 6]).astype(int)\n",
    "        \n",
    "        # Month and year\n",
    "        firm_day['Month'] = firm_day['date'].dt.month\n",
    "        firm_day['Year'] = firm_day['date'].dt.year\n",
    "        firm_day['YearMonth'] = firm_day['date'].dt.to_period('M')\n",
    "        \n",
    "        # Quarter\n",
    "        firm_day['Quarter'] = firm_day['date'].dt.quarter\n",
    "        \n",
    "        return firm_day\n",
    "\n",
    "# Initialize aggregator\n",
    "aggregator = FirmDayAggregator()\n",
    "\n",
    "# Create firm-day panel\n",
    "firm_day_panel = aggregator.aggregate(posts_with_sentiment)\n",
    "firm_day_panel = aggregator.add_trading_day_indicators(firm_day_panel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FIRM-DAY PANEL STATISTICS\n",
    "# =============================================================================\n",
    "\n",
    "def print_panel_statistics(df: pd.DataFrame):\n",
    "    \"\"\"Print comprehensive statistics for firm-day panel.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FIRM-DAY PANEL STATISTICS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Basic dimensions\n",
    "    print(f\"\\n--- Panel Dimensions ---\")\n",
    "    print(f\"Total observations: {len(df):,}\")\n",
    "    print(f\"Unique tickers: {df['ticker'].nunique():,}\")\n",
    "    print(f\"Unique dates: {df['date'].nunique():,}\")\n",
    "    print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "    \n",
    "    # Attention variables\n",
    "    print(f\"\\n--- Attention Variables ---\")\n",
    "    print(f\"Posts per firm-day:\")\n",
    "    print(f\"  Mean: {df['PostCount'].mean():.2f}\")\n",
    "    print(f\"  Median: {df['PostCount'].median():.1f}\")\n",
    "    print(f\"  Max: {df['PostCount'].max():,}\")\n",
    "    print(f\"  P95: {df['PostCount'].quantile(0.95):.0f}\")\n",
    "    \n",
    "    print(f\"\\nUnique users per firm-day:\")\n",
    "    print(f\"  Mean: {df['UniqueUsers'].mean():.2f}\")\n",
    "    print(f\"  Median: {df['UniqueUsers'].median():.1f}\")\n",
    "    \n",
    "    # Sentiment variables\n",
    "    print(f\"\\n--- Sentiment Variables ---\")\n",
    "    print(f\"Sentiment Polarity (mean):\")\n",
    "    print(f\"  Mean: {df['SentimentMean'].mean():.4f}\")\n",
    "    print(f\"  Std: {df['SentimentMean'].std():.4f}\")\n",
    "    print(f\"  Min: {df['SentimentMean'].min():.4f}\")\n",
    "    print(f\"  Max: {df['SentimentMean'].max():.4f}\")\n",
    "    \n",
    "    print(f\"\\nPositive share: {df['SentimentPosShare'].mean()*100:.1f}%\")\n",
    "    print(f\"Negative share: {df['SentimentNegShare'].mean()*100:.1f}%\")\n",
    "    print(f\"Neutral share: {(1 - df['SentimentPosShare'].mean() - df['SentimentNegShare'].mean())*100:.1f}%\")\n",
    "    \n",
    "    # Top tickers\n",
    "    print(f\"\\n--- Most Discussed Tickers ---\")\n",
    "    top_tickers = df.groupby('ticker')['PostCount'].sum().sort_values(ascending=False).head(15)\n",
    "    for ticker, count in top_tickers.items():\n",
    "        print(f\"  {ticker}: {count:,} posts\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Print statistics\n",
    "print_panel_statistics(firm_day_panel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Create Lagged and Rolling Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CREATE LAGGED AND ROLLING VARIABLES\n",
    "# =============================================================================\n",
    "\n",
    "def create_time_series_features(df: pd.DataFrame, \n",
    "                                windows: List[int] = [3, 5, 10, 20]) -> pd.DataFrame:\n",
    "    \"\"\"Create lagged and rolling window features.\n",
    "    \n",
    "    Args:\n",
    "        df: Firm-day panel\n",
    "        windows: List of rolling window sizes\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with additional time series features\n",
    "    \"\"\"\n",
    "    print(\"Creating time series features...\")\n",
    "    df = df.sort_values(['ticker', 'date']).copy()\n",
    "    \n",
    "    # Variables to create lags/rolling for\n",
    "    sentiment_vars = ['SentimentMean', 'SentimentPosShare', 'PostCount', 'UniqueUsers']\n",
    "    \n",
    "    for var in tqdm(sentiment_vars, desc=\"Processing variables\"):\n",
    "        # Lagged values\n",
    "        df[f'{var}_L1'] = df.groupby('ticker')[var].shift(1)\n",
    "        df[f'{var}_L5'] = df.groupby('ticker')[var].shift(5)\n",
    "        \n",
    "        # Rolling means\n",
    "        for window in windows:\n",
    "            df[f'{var}_MA{window}'] = df.groupby('ticker')[var].transform(\n",
    "                lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "            )\n",
    "        \n",
    "        # Rolling standard deviation (for sentiment)\n",
    "        if 'Sentiment' in var:\n",
    "            for window in [5, 10]:\n",
    "                df[f'{var}_Std{window}'] = df.groupby('ticker')[var].transform(\n",
    "                    lambda x: x.rolling(window=window, min_periods=2).std()\n",
    "                )\n",
    "    \n",
    "    # Sentiment momentum: current - MA\n",
    "    df['SentimentMomentum_5'] = df['SentimentMean'] - df['SentimentMean_MA5']\n",
    "    df['SentimentMomentum_10'] = df['SentimentMean'] - df['SentimentMean_MA10']\n",
    "    \n",
    "    # Attention surge: current vs average\n",
    "    df['AttentionSurge_5'] = df['PostCount'] / df['PostCount_MA5'].replace(0, np.nan)\n",
    "    df['AttentionSurge_10'] = df['PostCount'] / df['PostCount_MA10'].replace(0, np.nan)\n",
    "    \n",
    "    print(f\"Created {len([c for c in df.columns if '_L' in c or '_MA' in c or '_Std' in c])} new features\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create time series features\n",
    "firm_day_panel = create_time_series_features(firm_day_panel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Output: Firm-Day Social Media Panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE FINAL OUTPUT\n",
    "# =============================================================================\n",
    "\n",
    "def save_firm_day_panel(df: pd.DataFrame, output_dir: str):\n",
    "    \"\"\"Save firm-day panel with documentation.\"\"\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Main dataset\n",
    "    filepath = os.path.join(output_dir, 'wsb_firm_day_panel.parquet')\n",
    "    df.to_parquet(filepath, index=False)\n",
    "    print(f\"Saved: {filepath}\")\n",
    "    \n",
    "    # CSV sample\n",
    "    csv_path = os.path.join(output_dir, 'wsb_firm_day_sample.csv')\n",
    "    df.head(5000).to_csv(csv_path, index=False)\n",
    "    print(f\"Saved: {csv_path}\")\n",
    "    \n",
    "    # Data dictionary\n",
    "    data_dict = {\n",
    "        'ticker': 'Stock ticker symbol',\n",
    "        'date': 'Calendar date',\n",
    "        'PostCount': 'Number of posts mentioning ticker on date',\n",
    "        'UniqueUsers': 'Number of distinct authors posting about ticker',\n",
    "        'SentimentMean': 'Average FinBERT polarity score (P(pos) - P(neg))',\n",
    "        'SentimentStd': 'Standard deviation of polarity within day (disagreement)',\n",
    "        'SentimentMedian': 'Median polarity score',\n",
    "        'SentimentPosShare': 'Share of posts classified as positive',\n",
    "        'SentimentNegShare': 'Share of posts classified as negative',\n",
    "        'KarmaSum': 'Sum of Reddit scores (upvotes - downvotes)',\n",
    "        'KarmaMean': 'Average Reddit score per post',\n",
    "        'ProbPositiveMean': 'Average P(positive) from FinBERT',\n",
    "        'ProbNegativeMean': 'Average P(negative) from FinBERT',\n",
    "        'TotalComments': 'Sum of comments on posts',\n",
    "        'Attention': 'Log(1 + PostCount)',\n",
    "        'Disagreement': 'Alias for SentimentStd',\n",
    "        'NetSentiment': 'SentimentPosShare - SentimentNegShare',\n",
    "        'DayOfWeek': 'Day of week (0=Monday, 6=Sunday)',\n",
    "        'IsWeekend': 'Weekend indicator',\n",
    "        'Month': 'Month (1-12)',\n",
    "        'Year': 'Year',\n",
    "        'Quarter': 'Fiscal quarter (1-4)',\n",
    "        '_L1/_L5': 'Lagged values (1 day, 5 days)',\n",
    "        '_MA{n}': 'Rolling mean over n days',\n",
    "        '_Std{n}': 'Rolling standard deviation over n days',\n",
    "        'SentimentMomentum': 'Current sentiment minus rolling average',\n",
    "        'AttentionSurge': 'Current posts / rolling average posts'\n",
    "    }\n",
    "    \n",
    "    dict_path = os.path.join(output_dir, 'firm_day_dictionary.json')\n",
    "    with open(dict_path, 'w') as f:\n",
    "        json.dump(data_dict, f, indent=2)\n",
    "    print(f\"Saved: {dict_path}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    summary = {\n",
    "        'total_observations': len(df),\n",
    "        'unique_tickers': int(df['ticker'].nunique()),\n",
    "        'unique_dates': int(df['date'].nunique()),\n",
    "        'date_range': [str(df['date'].min()), str(df['date'].max())],\n",
    "        'avg_posts_per_firmday': float(df['PostCount'].mean()),\n",
    "        'avg_sentiment': float(df['SentimentMean'].mean()),\n",
    "        'sentiment_std': float(df['SentimentMean'].std()),\n",
    "        'model_used': config.FINBERT_MODEL,\n",
    "        'created_at': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    summary_path = os.path.join(output_dir, 'firm_day_summary.json')\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(f\"Saved: {summary_path}\")\n",
    "\n",
    "# Save outputs\n",
    "save_firm_day_panel(firm_day_panel, config.PROCESSED_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# NOTEBOOK SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘       NOTEBOOK 2: SENTIMENT ANALYSIS COMPLETE                    â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "OUTPUT FILES:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "â€¢ wsb_posts_sentiment.parquet   - Post-level data with sentiment\n",
    "â€¢ wsb_firm_day_panel.parquet    - Firm-day aggregated panel\n",
    "â€¢ firm_day_dictionary.json      - Variable definitions\n",
    "â€¢ firm_day_summary.json         - Summary statistics\n",
    "\n",
    "KEY VARIABLES CREATED:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Attention:\n",
    "  â€¢ PostCount, UniqueUsers, Attention (log), TotalComments\n",
    "\n",
    "Sentiment:\n",
    "  â€¢ SentimentMean (FinBERT polarity)\n",
    "  â€¢ SentimentPosShare, SentimentNegShare\n",
    "  â€¢ SentimentStd (disagreement)\n",
    "  â€¢ NetSentiment\n",
    "\n",
    "Time Series:\n",
    "  â€¢ Lagged values (_L1, _L5)\n",
    "  â€¢ Rolling means (_MA3, _MA5, _MA10, _MA20)\n",
    "  â€¢ Momentum and surge indicators\n",
    "\n",
    "METHODOLOGY NOTES:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "â€¢ FinBERT model: ProsusAI/finbert\n",
    "â€¢ Text cleaning preserves WSB slang and emojis\n",
    "â€¢ Polarity = P(positive) - P(negative)\n",
    "â€¢ Aggregation: Simple mean across posts\n",
    "\n",
    "NEXT STEPS:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "â†’ Notebook 3: Financial Data Collection\n",
    "  - Stock prices and returns\n",
    "  - Earnings announcement dates\n",
    "  - Calculate earnings surprises\n",
    "\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

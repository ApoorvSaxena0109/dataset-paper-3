{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5: Data Merging & Final Dataset Construction\n",
    "## Creating the Event-Level Analysis Dataset\n",
    "\n",
    "---\n",
    "\n",
    "**Research Project:** Retail Sentiment, Earnings Quality, and Stock Returns\n",
    "\n",
    "**Purpose:** Merge all data layers to create the final analysis dataset.\n",
    "\n",
    "**Input Files:**\n",
    "- `wsb_firm_day_panel.parquet` - Social media sentiment\n",
    "- `stock_returns_panel.parquet` - Stock returns & characteristics\n",
    "- `earnings_quality_panel.parquet` - Earnings quality measures\n",
    "- Earnings announcement dates\n",
    "\n",
    "**Output:** Event-level panel linking:\n",
    "- Pre-EA social media sentiment and attention\n",
    "- EA abnormal returns (CAR)\n",
    "- Post-EA drift returns\n",
    "- Firm-level earnings quality\n",
    "- Control variables\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# INSTALL REQUIRED PACKAGES\n",
    "# =============================================================================\n",
    "\n",
    "!pip install pandas==2.0.3\n",
    "!pip install numpy==1.24.3\n",
    "!pip install scipy==1.11.3\n",
    "!pip install pyarrow==14.0.1\n",
    "!pip install tqdm==4.66.1\n",
    "!pip install yfinance==0.2.31\n",
    "\n",
    "print(\"All packages installed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORT LIBRARIES\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from tqdm.notebook import tqdm\n",
    "import yfinance as yf\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "print(f\"Environment setup complete. Timestamp: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "class MergeConfig:\n",
    "    \"\"\"Configuration for data merging.\"\"\"\n",
    "    \n",
    "    # Data paths\n",
    "    BASE_PATH = \"/content/drive/MyDrive/Research/RetailSentiment/\"\n",
    "    PROCESSED_DATA_PATH = BASE_PATH + \"data/processed/\"\n",
    "    FINAL_DATA_PATH = BASE_PATH + \"data/final/\"\n",
    "    \n",
    "    # Event windows (in trading days)\n",
    "    PRE_EA_WINDOW_START = -10  # Start of pre-EA sentiment window\n",
    "    PRE_EA_WINDOW_END = -2     # End of pre-EA sentiment window\n",
    "    \n",
    "    EA_WINDOW_START = -1       # EA return window start\n",
    "    EA_WINDOW_END = 1          # EA return window end\n",
    "    \n",
    "    POST_EA_WINDOW_START = 2   # Post-EA drift start\n",
    "    POST_EA_WINDOW_END = 20    # Post-EA drift end\n",
    "    \n",
    "    # Sample filters\n",
    "    MIN_POSTS_PRE_EA = 1       # Minimum posts in pre-EA window\n",
    "    MIN_MARKET_CAP = 1e8      # Minimum market cap ($100M)\n",
    "    MIN_PRICE = 5.0           # Minimum stock price\n",
    "    \n",
    "    @classmethod\n",
    "    def print_config(cls):\n",
    "        print(\"=\"*60)\n",
    "        print(\"DATA MERGE CONFIGURATION\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Pre-EA window: [{cls.PRE_EA_WINDOW_START}, {cls.PRE_EA_WINDOW_END}]\")\n",
    "        print(f\"EA window: [{cls.EA_WINDOW_START}, {cls.EA_WINDOW_END}]\")\n",
    "        print(f\"Post-EA window: [{cls.POST_EA_WINDOW_START}, {cls.POST_EA_WINDOW_END}]\")\n",
    "        print(f\"Min posts pre-EA: {cls.MIN_POSTS_PRE_EA}\")\n",
    "        print(f\"Min market cap: ${cls.MIN_MARKET_CAP/1e6:.0f}M\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "config = MergeConfig()\n",
    "config.print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MOUNT GOOGLE DRIVE\n",
    "# =============================================================================\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "os.makedirs(config.FINAL_DATA_PATH, exist_ok=True)\n",
    "print(\"Data directories ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load All Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA LOADER\n",
    "# =============================================================================\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"Loads and validates all input data sources.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path: str):\n",
    "        self.data_path = data_path\n",
    "        self.data = {}\n",
    "        \n",
    "    def load_social_media(self) -> pd.DataFrame:\n",
    "        \"\"\"Load social media sentiment panel.\"\"\"\n",
    "        filepath = os.path.join(self.data_path, 'wsb_firm_day_panel.parquet')\n",
    "        \n",
    "        if os.path.exists(filepath):\n",
    "            df = pd.read_parquet(filepath)\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            print(f\"Loaded social media panel: {len(df):,} obs, {df['ticker'].nunique()} firms\")\n",
    "            return df\n",
    "        else:\n",
    "            print(f\"Warning: {filepath} not found. Creating synthetic data.\")\n",
    "            return self._create_synthetic_sentiment()\n",
    "    \n",
    "    def load_stock_returns(self) -> pd.DataFrame:\n",
    "        \"\"\"Load stock returns panel.\"\"\"\n",
    "        filepath = os.path.join(self.data_path, 'stock_returns_panel.parquet')\n",
    "        \n",
    "        if os.path.exists(filepath):\n",
    "            df = pd.read_parquet(filepath)\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            print(f\"Loaded stock returns: {len(df):,} obs, {df['ticker'].nunique()} firms\")\n",
    "            return df\n",
    "        else:\n",
    "            print(f\"Warning: {filepath} not found. Creating synthetic data.\")\n",
    "            return self._create_synthetic_returns()\n",
    "    \n",
    "    def load_earnings_quality(self) -> pd.DataFrame:\n",
    "        \"\"\"Load earnings quality panel.\"\"\"\n",
    "        filepath = os.path.join(self.data_path, 'earnings_quality_panel.parquet')\n",
    "        \n",
    "        if os.path.exists(filepath):\n",
    "            df = pd.read_parquet(filepath)\n",
    "            df['period_end'] = pd.to_datetime(df['period_end'])\n",
    "            print(f\"Loaded earnings quality: {len(df):,} obs, {df['ticker'].nunique()} firms\")\n",
    "            return df\n",
    "        else:\n",
    "            print(f\"Warning: {filepath} not found. Creating synthetic data.\")\n",
    "            return self._create_synthetic_eq()\n",
    "    \n",
    "    def load_firm_characteristics(self) -> pd.DataFrame:\n",
    "        \"\"\"Load firm characteristics.\"\"\"\n",
    "        filepath = os.path.join(self.data_path, 'firm_characteristics.parquet')\n",
    "        \n",
    "        if os.path.exists(filepath):\n",
    "            df = pd.read_parquet(filepath)\n",
    "            print(f\"Loaded firm characteristics: {len(df)} firms\")\n",
    "            return df\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _create_synthetic_sentiment(self) -> pd.DataFrame:\n",
    "        \"\"\"Create synthetic sentiment data for demonstration.\"\"\"\n",
    "        np.random.seed(42)\n",
    "        tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META', 'TSLA', 'NVDA', \n",
    "                   'GME', 'AMC', 'BB', 'PLTR', 'NIO', 'SPY', 'QQQ', 'AMD']\n",
    "        dates = pd.date_range('2020-01-01', '2023-12-31', freq='D')\n",
    "        \n",
    "        data = []\n",
    "        for ticker in tickers:\n",
    "            for date in dates:\n",
    "                if np.random.random() > 0.3:  # 70% coverage\n",
    "                    data.append({\n",
    "                        'ticker': ticker,\n",
    "                        'date': date,\n",
    "                        'PostCount': np.random.poisson(10),\n",
    "                        'UniqueUsers': np.random.poisson(8),\n",
    "                        'SentimentMean': np.random.normal(0.1, 0.3),\n",
    "                        'SentimentPosShare': np.random.uniform(0.2, 0.6),\n",
    "                        'SentimentNegShare': np.random.uniform(0.1, 0.4),\n",
    "                        'SentimentStd': np.random.uniform(0.1, 0.5),\n",
    "                        'KarmaSum': np.random.poisson(100),\n",
    "                        'Attention': np.random.uniform(1, 4)\n",
    "                    })\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def _create_synthetic_returns(self) -> pd.DataFrame:\n",
    "        \"\"\"Create synthetic returns data for demonstration.\"\"\"\n",
    "        np.random.seed(42)\n",
    "        tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META', 'TSLA', 'NVDA', \n",
    "                   'GME', 'AMC', 'BB', 'PLTR', 'NIO', 'SPY', 'QQQ', 'AMD']\n",
    "        dates = pd.bdate_range('2020-01-01', '2023-12-31')\n",
    "        \n",
    "        data = []\n",
    "        for ticker in tickers:\n",
    "            price = 100\n",
    "            for date in dates:\n",
    "                ret = np.random.normal(0.0005, 0.02)\n",
    "                price = price * (1 + ret)\n",
    "                data.append({\n",
    "                    'ticker': ticker,\n",
    "                    'date': date,\n",
    "                    'close': price,\n",
    "                    'volume': np.random.poisson(1e7),\n",
    "                    'ret': ret,\n",
    "                    'market_return': np.random.normal(0.0004, 0.01),\n",
    "                    'ret_mktadj': ret - np.random.normal(0.0004, 0.01)\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def _create_synthetic_eq(self) -> pd.DataFrame:\n",
    "        \"\"\"Create synthetic earnings quality data.\"\"\"\n",
    "        np.random.seed(42)\n",
    "        tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META', 'TSLA', 'NVDA', \n",
    "                   'GME', 'AMC', 'BB', 'PLTR', 'NIO', 'SPY', 'QQQ', 'AMD']\n",
    "        quarters = pd.date_range('2020-01-01', '2023-12-31', freq='Q')\n",
    "        \n",
    "        data = []\n",
    "        for ticker in tickers:\n",
    "            base_eq = np.random.normal(0, 1)\n",
    "            for q in quarters:\n",
    "                data.append({\n",
    "                    'ticker': ticker,\n",
    "                    'period_end': q,\n",
    "                    'earnings_quality_dd_std': base_eq + np.random.normal(0, 0.2),\n",
    "                    'earnings_quality_mcn_std': base_eq + np.random.normal(0, 0.25),\n",
    "                    'earnings_quality_composite': base_eq + np.random.normal(0, 0.15)\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "# Initialize loader\n",
    "loader = DataLoader(config.PROCESSED_DATA_PATH)\n",
    "\n",
    "# Load all data\n",
    "print(\"Loading all data sources...\\n\")\n",
    "social_media = loader.load_social_media()\n",
    "stock_returns = loader.load_stock_returns()\n",
    "earnings_quality = loader.load_earnings_quality()\n",
    "firm_chars = loader.load_firm_characteristics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Earnings Announcement Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EARNINGS ANNOUNCEMENT EVENT CREATOR\n",
    "# =============================================================================\n",
    "\n",
    "class EarningsEventCreator:\n",
    "    \"\"\"Creates earnings announcement event observations.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: MergeConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    def get_earnings_dates(self, tickers: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"Fetch earnings announcement dates from Yahoo Finance.\n",
    "        \n",
    "        Args:\n",
    "            tickers: List of ticker symbols\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with earnings events\n",
    "        \"\"\"\n",
    "        print(f\"Fetching earnings dates for {len(tickers)} tickers...\")\n",
    "        \n",
    "        events = []\n",
    "        \n",
    "        for ticker in tqdm(tickers, desc=\"Fetching EA dates\"):\n",
    "            try:\n",
    "                stock = yf.Ticker(ticker)\n",
    "                earnings = stock.earnings_dates\n",
    "                \n",
    "                if earnings is not None and len(earnings) > 0:\n",
    "                    earnings_df = earnings.reset_index()\n",
    "                    earnings_df.columns = ['ea_datetime'] + list(earnings_df.columns[1:])\n",
    "                    \n",
    "                    for _, row in earnings_df.iterrows():\n",
    "                        events.append({\n",
    "                            'ticker': ticker,\n",
    "                            'ea_date': row['ea_datetime'].date() if hasattr(row['ea_datetime'], 'date') else row['ea_datetime'],\n",
    "                            'eps_estimate': row.get('EPS Estimate', np.nan),\n",
    "                            'reported_eps': row.get('Reported EPS', np.nan),\n",
    "                            'surprise_pct': row.get('Surprise(%)', np.nan)\n",
    "                        })\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        df = pd.DataFrame(events)\n",
    "        if len(df) > 0:\n",
    "            df['ea_date'] = pd.to_datetime(df['ea_date'])\n",
    "            print(f\"Collected {len(df)} earnings events\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_synthetic_events(self, tickers: List[str],\n",
    "                                start_date: str = '2020-01-01',\n",
    "                                end_date: str = '2023-12-31') -> pd.DataFrame:\n",
    "        \"\"\"Create synthetic earnings events for demonstration.\n",
    "        \n",
    "        Assumes quarterly earnings with some randomization.\n",
    "        \"\"\"\n",
    "        print(\"Creating synthetic earnings events...\")\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        events = []\n",
    "        quarters = pd.date_range(start_date, end_date, freq='Q')\n",
    "        \n",
    "        for ticker in tickers:\n",
    "            for q_end in quarters:\n",
    "                # EA typically 2-6 weeks after quarter end\n",
    "                ea_date = q_end + pd.Timedelta(days=np.random.randint(14, 42))\n",
    "                \n",
    "                # Random surprise\n",
    "                surprise = np.random.normal(0.02, 0.15)  # 2% mean, 15% std\n",
    "                \n",
    "                events.append({\n",
    "                    'ticker': ticker,\n",
    "                    'ea_date': ea_date,\n",
    "                    'quarter_end': q_end,\n",
    "                    'eps_estimate': np.random.uniform(0.5, 3.0),\n",
    "                    'reported_eps': None,  # Will be calculated\n",
    "                    'surprise_pct': surprise * 100\n",
    "                })\n",
    "        \n",
    "        df = pd.DataFrame(events)\n",
    "        df['reported_eps'] = df['eps_estimate'] * (1 + df['surprise_pct']/100)\n",
    "        df['ea_date'] = pd.to_datetime(df['ea_date'])\n",
    "        df['quarter_end'] = pd.to_datetime(df['quarter_end'])\n",
    "        \n",
    "        print(f\"Created {len(df)} synthetic earnings events\")\n",
    "        return df\n",
    "\n",
    "# Initialize event creator\n",
    "event_creator = EarningsEventCreator(config)\n",
    "\n",
    "# Get unique tickers from data\n",
    "all_tickers = list(set(social_media['ticker'].unique()) & \n",
    "                   set(stock_returns['ticker'].unique()))\n",
    "print(f\"Common tickers: {len(all_tickers)}\")\n",
    "\n",
    "# Get earnings events\n",
    "# Try real data first, fall back to synthetic\n",
    "try:\n",
    "    ea_events = event_creator.get_earnings_dates(all_tickers[:20])  # Subset for demo\n",
    "    if len(ea_events) < 50:\n",
    "        raise ValueError(\"Insufficient real data\")\n",
    "except:\n",
    "    ea_events = event_creator.create_synthetic_events(all_tickers)\n",
    "\n",
    "print(f\"\\nEarnings events: {len(ea_events)}\")\n",
    "print(ea_events.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate Event-Window Variables\n",
    "\n",
    "### 4.1 Pre-EA Social Media Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PRE-EA SENTIMENT AGGREGATOR\n",
    "# =============================================================================\n",
    "\n",
    "class PreEASentimentAggregator:\n",
    "    \"\"\"Aggregates social media variables over pre-EA window.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: MergeConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    def aggregate_sentiment(\n",
    "        self,\n",
    "        events: pd.DataFrame,\n",
    "        sentiment_panel: pd.DataFrame,\n",
    "        returns_panel: pd.DataFrame\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Aggregate sentiment over pre-EA window for each event.\n",
    "        \n",
    "        Args:\n",
    "            events: Earnings announcement events\n",
    "            sentiment_panel: Daily firm sentiment data\n",
    "            returns_panel: Stock returns (for trading day calculation)\n",
    "            \n",
    "        Returns:\n",
    "            Events with pre-EA sentiment variables\n",
    "        \"\"\"\n",
    "        print(\"Aggregating pre-EA sentiment...\")\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # Create trading day calendar\n",
    "        trading_days = returns_panel['date'].sort_values().unique()\n",
    "        trading_day_idx = {d: i for i, d in enumerate(trading_days)}\n",
    "        \n",
    "        for _, event in tqdm(events.iterrows(), total=len(events), desc=\"Processing events\"):\n",
    "            ticker = event['ticker']\n",
    "            ea_date = event['ea_date']\n",
    "            \n",
    "            # Find EA date in trading calendar\n",
    "            ea_date_np = np.datetime64(ea_date)\n",
    "            idx = np.searchsorted(trading_days, ea_date_np)\n",
    "            \n",
    "            if idx == 0 or idx >= len(trading_days):\n",
    "                continue\n",
    "            \n",
    "            # Get pre-EA window dates\n",
    "            start_idx = max(0, idx + self.config.PRE_EA_WINDOW_START)\n",
    "            end_idx = idx + self.config.PRE_EA_WINDOW_END\n",
    "            \n",
    "            if end_idx < 0 or start_idx >= len(trading_days):\n",
    "                continue\n",
    "            \n",
    "            window_start = pd.Timestamp(trading_days[start_idx])\n",
    "            window_end = pd.Timestamp(trading_days[end_idx])\n",
    "            \n",
    "            # Filter sentiment data\n",
    "            mask = (\n",
    "                (sentiment_panel['ticker'] == ticker) &\n",
    "                (sentiment_panel['date'] >= window_start) &\n",
    "                (sentiment_panel['date'] <= window_end)\n",
    "            )\n",
    "            window_data = sentiment_panel[mask]\n",
    "            \n",
    "            # Aggregate\n",
    "            result = {\n",
    "                'ticker': ticker,\n",
    "                'ea_date': ea_date,\n",
    "                'window_start': window_start,\n",
    "                'window_end': window_end,\n",
    "                'n_days_with_posts': len(window_data),\n",
    "            }\n",
    "            \n",
    "            if len(window_data) > 0:\n",
    "                # Attention measures\n",
    "                result['pre_ea_posts'] = window_data['PostCount'].sum()\n",
    "                result['pre_ea_users'] = window_data['UniqueUsers'].sum()\n",
    "                result['pre_ea_attention'] = np.log1p(result['pre_ea_posts'])\n",
    "                result['pre_ea_attention_avg'] = window_data['Attention'].mean() if 'Attention' in window_data.columns else np.nan\n",
    "                \n",
    "                # Sentiment measures\n",
    "                result['pre_ea_sentiment_mean'] = window_data['SentimentMean'].mean()\n",
    "                result['pre_ea_sentiment_median'] = window_data['SentimentMean'].median()\n",
    "                result['pre_ea_pos_share'] = window_data['SentimentPosShare'].mean()\n",
    "                result['pre_ea_neg_share'] = window_data['SentimentNegShare'].mean()\n",
    "                result['pre_ea_sentiment_std'] = window_data['SentimentMean'].std()\n",
    "                result['pre_ea_disagreement'] = window_data['SentimentStd'].mean() if 'SentimentStd' in window_data.columns else np.nan\n",
    "                \n",
    "                # Karma-weighted sentiment\n",
    "                if 'KarmaSum' in window_data.columns:\n",
    "                    total_karma = window_data['KarmaSum'].sum()\n",
    "                    if total_karma > 0:\n",
    "                        result['pre_ea_sentiment_karma_wtd'] = (\n",
    "                            (window_data['SentimentMean'] * window_data['KarmaSum']).sum() / total_karma\n",
    "                        )\n",
    "                    else:\n",
    "                        result['pre_ea_sentiment_karma_wtd'] = result['pre_ea_sentiment_mean']\n",
    "            else:\n",
    "                # No posts in window\n",
    "                result['pre_ea_posts'] = 0\n",
    "                result['pre_ea_users'] = 0\n",
    "                result['pre_ea_attention'] = 0\n",
    "                result['pre_ea_sentiment_mean'] = np.nan\n",
    "            \n",
    "            # Add original event data\n",
    "            for col in ['quarter_end', 'eps_estimate', 'reported_eps', 'surprise_pct']:\n",
    "                if col in event.index:\n",
    "                    result[col] = event[col]\n",
    "            \n",
    "            results.append(result)\n",
    "        \n",
    "        df = pd.DataFrame(results)\n",
    "        print(f\"\\nPre-EA sentiment aggregated for {len(df)} events\")\n",
    "        print(f\"Events with posts: {(df['pre_ea_posts'] > 0).sum()}\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Aggregate pre-EA sentiment\n",
    "sentiment_agg = PreEASentimentAggregator(config)\n",
    "events_with_sentiment = sentiment_agg.aggregate_sentiment(\n",
    "    ea_events,\n",
    "    social_media,\n",
    "    stock_returns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Calculate Cumulative Abnormal Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CAR CALCULATOR FOR EVENTS\n",
    "# =============================================================================\n",
    "\n",
    "class EventCARCalculator:\n",
    "    \"\"\"Calculates CARs for earnings announcement events.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: MergeConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    def calculate_cars(\n",
    "        self,\n",
    "        events: pd.DataFrame,\n",
    "        returns_panel: pd.DataFrame\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Calculate CARs for various windows around EA.\n",
    "        \n",
    "        Windows:\n",
    "        - CAR[-1,+1]: EA announcement return\n",
    "        - CAR[0,+2]: Immediate post-EA return\n",
    "        - CAR[+2,+20]: Post-EA drift\n",
    "        - CAR[-10,-2]: Pre-EA return (control)\n",
    "        \n",
    "        Args:\n",
    "            events: Earnings events with sentiment\n",
    "            returns_panel: Stock returns panel\n",
    "            \n",
    "        Returns:\n",
    "            Events with CAR variables\n",
    "        \"\"\"\n",
    "        print(\"Calculating CARs...\")\n",
    "        \n",
    "        # Define windows\n",
    "        windows = [\n",
    "            ('CAR_m1_p1', -1, 1),     # EA window\n",
    "            ('CAR_0_p2', 0, 2),        # Immediate post\n",
    "            ('CAR_p2_p20', 2, 20),     # Drift\n",
    "            ('CAR_m10_m2', -10, -2),   # Pre-EA\n",
    "            ('CAR_m5_p5', -5, 5),      # Extended EA\n",
    "        ]\n",
    "        \n",
    "        # Get trading days\n",
    "        trading_days = returns_panel['date'].sort_values().unique()\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for _, event in tqdm(events.iterrows(), total=len(events), desc=\"Calculating CARs\"):\n",
    "            ticker = event['ticker']\n",
    "            ea_date = pd.Timestamp(event['ea_date'])\n",
    "            \n",
    "            # Get ticker returns\n",
    "            ticker_returns = returns_panel[\n",
    "                returns_panel['ticker'] == ticker\n",
    "            ].set_index('date').sort_index()\n",
    "            \n",
    "            if len(ticker_returns) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Find EA date index\n",
    "            ea_date_np = np.datetime64(ea_date)\n",
    "            idx = np.searchsorted(trading_days, ea_date_np)\n",
    "            \n",
    "            result = event.to_dict()\n",
    "            \n",
    "            # Calculate each window\n",
    "            for window_name, start_offset, end_offset in windows:\n",
    "                start_idx = idx + start_offset\n",
    "                end_idx = idx + end_offset + 1\n",
    "                \n",
    "                if start_idx < 0 or end_idx > len(trading_days):\n",
    "                    result[window_name] = np.nan\n",
    "                    result[f'{window_name}_raw'] = np.nan\n",
    "                    continue\n",
    "                \n",
    "                window_dates = trading_days[start_idx:end_idx]\n",
    "                \n",
    "                # Get returns in window\n",
    "                window_returns = ticker_returns[\n",
    "                    ticker_returns.index.isin(window_dates)\n",
    "                ]\n",
    "                \n",
    "                if len(window_returns) > 0:\n",
    "                    # Market-adjusted CAR\n",
    "                    if 'ret_mktadj' in window_returns.columns:\n",
    "                        result[window_name] = window_returns['ret_mktadj'].sum()\n",
    "                    else:\n",
    "                        result[window_name] = window_returns['ret'].sum() - window_returns.get('market_return', 0).sum()\n",
    "                    \n",
    "                    # Raw return\n",
    "                    result[f'{window_name}_raw'] = window_returns['ret'].sum()\n",
    "                else:\n",
    "                    result[window_name] = np.nan\n",
    "                    result[f'{window_name}_raw'] = np.nan\n",
    "            \n",
    "            # Add volatility around EA\n",
    "            vol_window_dates = trading_days[max(0, idx-20):min(len(trading_days), idx+20)]\n",
    "            vol_returns = ticker_returns[ticker_returns.index.isin(vol_window_dates)]\n",
    "            result['ea_volatility'] = vol_returns['ret'].std() * np.sqrt(252) if len(vol_returns) > 5 else np.nan\n",
    "            \n",
    "            results.append(result)\n",
    "        \n",
    "        df = pd.DataFrame(results)\n",
    "        print(f\"\\nCARs calculated for {len(df)} events\")\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\nCAR Summary Statistics:\")\n",
    "        for window_name, _, _ in windows:\n",
    "            if window_name in df.columns:\n",
    "                print(f\"  {window_name}: mean={df[window_name].mean():.4f}, std={df[window_name].std():.4f}\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Calculate CARs\n",
    "car_calculator = EventCARCalculator(config)\n",
    "events_with_cars = car_calculator.calculate_cars(\n",
    "    events_with_sentiment,\n",
    "    stock_returns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Merge with Earnings Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MERGE EARNINGS QUALITY\n",
    "# =============================================================================\n",
    "\n",
    "def merge_earnings_quality(\n",
    "    events: pd.DataFrame,\n",
    "    eq_panel: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Merge earnings quality measures with events.\n",
    "    \n",
    "    Match EQ from most recent quarter before EA date.\n",
    "    \n",
    "    Args:\n",
    "        events: Event-level data\n",
    "        eq_panel: Earnings quality panel\n",
    "        \n",
    "    Returns:\n",
    "        Events with EQ variables\n",
    "    \"\"\"\n",
    "    print(\"Merging earnings quality...\")\n",
    "    \n",
    "    # Ensure date columns are datetime\n",
    "    events = events.copy()\n",
    "    events['ea_date'] = pd.to_datetime(events['ea_date'])\n",
    "    eq_panel['period_end'] = pd.to_datetime(eq_panel['period_end'])\n",
    "    \n",
    "    # For each event, find most recent EQ\n",
    "    merged_events = []\n",
    "    \n",
    "    for _, event in events.iterrows():\n",
    "        ticker = event['ticker']\n",
    "        ea_date = event['ea_date']\n",
    "        \n",
    "        # Get EQ data for ticker before EA date\n",
    "        ticker_eq = eq_panel[\n",
    "            (eq_panel['ticker'] == ticker) &\n",
    "            (eq_panel['period_end'] < ea_date)\n",
    "        ].sort_values('period_end', ascending=False)\n",
    "        \n",
    "        result = event.to_dict()\n",
    "        \n",
    "        if len(ticker_eq) > 0:\n",
    "            # Take most recent\n",
    "            latest_eq = ticker_eq.iloc[0]\n",
    "            \n",
    "            # Add EQ variables\n",
    "            eq_cols = [c for c in latest_eq.index if 'earnings_quality' in c]\n",
    "            for col in eq_cols:\n",
    "                result[col] = latest_eq[col]\n",
    "            \n",
    "            result['eq_period_end'] = latest_eq['period_end']\n",
    "            result['eq_lag_days'] = (ea_date - latest_eq['period_end']).days\n",
    "        \n",
    "        merged_events.append(result)\n",
    "    \n",
    "    df = pd.DataFrame(merged_events)\n",
    "    \n",
    "    # Create EQ quintiles for analysis\n",
    "    if 'earnings_quality_composite' in df.columns:\n",
    "        df['eq_quintile'] = pd.qcut(\n",
    "            df['earnings_quality_composite'].rank(method='first'),\n",
    "            5, labels=['Q1_Low', 'Q2', 'Q3', 'Q4', 'Q5_High']\n",
    "        )\n",
    "        df['eq_high'] = (df['eq_quintile'].isin(['Q4', 'Q5_High'])).astype(int)\n",
    "        df['eq_low'] = (df['eq_quintile'].isin(['Q1_Low', 'Q2'])).astype(int)\n",
    "    \n",
    "    print(f\"Merged EQ for {df['earnings_quality_composite'].notna().sum()} events\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Merge earnings quality\n",
    "events_with_eq = merge_earnings_quality(events_with_cars, earnings_quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Add Control Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ADD CONTROL VARIABLES\n",
    "# =============================================================================\n",
    "\n",
    "def add_control_variables(\n",
    "    events: pd.DataFrame,\n",
    "    returns_panel: pd.DataFrame,\n",
    "    firm_chars: pd.DataFrame = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Add control variables to event data.\n",
    "    \n",
    "    Controls:\n",
    "    - Size (log market cap)\n",
    "    - Book-to-market\n",
    "    - Prior returns (momentum)\n",
    "    - Volatility\n",
    "    - Liquidity\n",
    "    - Sector\n",
    "    - Announcement timing\n",
    "    \n",
    "    Args:\n",
    "        events: Event data\n",
    "        returns_panel: Stock returns\n",
    "        firm_chars: Firm characteristics (optional)\n",
    "        \n",
    "    Returns:\n",
    "        Events with control variables\n",
    "    \"\"\"\n",
    "    print(\"Adding control variables...\")\n",
    "    \n",
    "    df = events.copy()\n",
    "    \n",
    "    # Time-varying controls from returns panel\n",
    "    controls = []\n",
    "    \n",
    "    for _, event in df.iterrows():\n",
    "        ticker = event['ticker']\n",
    "        ea_date = pd.Timestamp(event['ea_date'])\n",
    "        \n",
    "        # Get returns before EA\n",
    "        ticker_returns = returns_panel[\n",
    "            (returns_panel['ticker'] == ticker) &\n",
    "            (returns_panel['date'] < ea_date)\n",
    "        ].sort_values('date', ascending=False)\n",
    "        \n",
    "        result = {'ticker': ticker, 'ea_date': ea_date}\n",
    "        \n",
    "        if len(ticker_returns) > 0:\n",
    "            latest = ticker_returns.iloc[0]\n",
    "            \n",
    "            # Price at EA-2\n",
    "            result['price_pre_ea'] = latest['close']\n",
    "            result['log_price'] = np.log(latest['close']) if latest['close'] > 0 else np.nan\n",
    "            \n",
    "            # Volume\n",
    "            result['volume_pre_ea'] = latest['volume']\n",
    "            result['log_volume'] = np.log1p(latest['volume'])\n",
    "            \n",
    "            # Prior returns (if available)\n",
    "            if 'ret_1m' in latest.index:\n",
    "                result['ret_1m'] = latest['ret_1m']\n",
    "            if 'ret_3m' in latest.index:\n",
    "                result['ret_3m'] = latest['ret_3m']\n",
    "            if 'ret_6m' in latest.index:\n",
    "                result['ret_6m'] = latest['ret_6m']\n",
    "                \n",
    "            # Volatility\n",
    "            if len(ticker_returns) >= 20:\n",
    "                result['volatility_pre_ea'] = ticker_returns.head(20)['ret'].std() * np.sqrt(252)\n",
    "            else:\n",
    "                result['volatility_pre_ea'] = ticker_returns['ret'].std() * np.sqrt(252)\n",
    "            \n",
    "            # Illiquidity\n",
    "            if 'illiquidity_avg' in latest.index:\n",
    "                result['illiquidity'] = latest['illiquidity_avg']\n",
    "        \n",
    "        controls.append(result)\n",
    "    \n",
    "    controls_df = pd.DataFrame(controls)\n",
    "    \n",
    "    # Merge controls\n",
    "    df = df.merge(\n",
    "        controls_df,\n",
    "        on=['ticker', 'ea_date'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Merge firm characteristics if available\n",
    "    if firm_chars is not None and len(firm_chars) > 0:\n",
    "        firm_cols = ['ticker', 'sector', 'industry', 'market_cap', 'beta']\n",
    "        firm_cols = [c for c in firm_cols if c in firm_chars.columns]\n",
    "        \n",
    "        df = df.merge(\n",
    "            firm_chars[firm_cols],\n",
    "            on='ticker',\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Log market cap\n",
    "        if 'market_cap' in df.columns:\n",
    "            df['log_mcap'] = np.log(df['market_cap'])\n",
    "    \n",
    "    # Time controls\n",
    "    df['ea_year'] = df['ea_date'].dt.year\n",
    "    df['ea_quarter'] = df['ea_date'].dt.quarter\n",
    "    df['ea_month'] = df['ea_date'].dt.month\n",
    "    df['ea_dayofweek'] = df['ea_date'].dt.dayofweek\n",
    "    \n",
    "    print(f\"Added controls for {len(df)} events\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Add control variables\n",
    "final_events = add_control_variables(\n",
    "    events_with_eq,\n",
    "    stock_returns,\n",
    "    firm_chars\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Sample Filters and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAMPLE FILTERS AND VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "def apply_sample_filters(df: pd.DataFrame, config: MergeConfig) -> pd.DataFrame:\n",
    "    \"\"\"Apply sample filters for clean analysis dataset.\n",
    "    \n",
    "    Filters:\n",
    "    1. Minimum WSB coverage (posts in pre-EA window)\n",
    "    2. Minimum price\n",
    "    3. Minimum market cap (if available)\n",
    "    4. Non-missing key variables\n",
    "    \n",
    "    Args:\n",
    "        df: Raw event dataset\n",
    "        config: Configuration with filter parameters\n",
    "        \n",
    "    Returns:\n",
    "        Filtered dataset\n",
    "    \"\"\"\n",
    "    print(\"\\nApplying sample filters...\")\n",
    "    print(f\"Initial observations: {len(df):,}\")\n",
    "    \n",
    "    filters_applied = []\n",
    "    \n",
    "    # Filter 1: Minimum WSB posts\n",
    "    if 'pre_ea_posts' in df.columns:\n",
    "        n_before = len(df)\n",
    "        df = df[df['pre_ea_posts'] >= config.MIN_POSTS_PRE_EA]\n",
    "        filters_applied.append(f\"Min posts ({config.MIN_POSTS_PRE_EA}): -{n_before - len(df):,}\")\n",
    "    \n",
    "    # Filter 2: Minimum price\n",
    "    if 'price_pre_ea' in df.columns:\n",
    "        n_before = len(df)\n",
    "        df = df[df['price_pre_ea'] >= config.MIN_PRICE]\n",
    "        filters_applied.append(f\"Min price (${config.MIN_PRICE}): -{n_before - len(df):,}\")\n",
    "    \n",
    "    # Filter 3: Non-missing CAR\n",
    "    if 'CAR_m1_p1' in df.columns:\n",
    "        n_before = len(df)\n",
    "        df = df[df['CAR_m1_p1'].notna()]\n",
    "        filters_applied.append(f\"Missing CAR: -{n_before - len(df):,}\")\n",
    "    \n",
    "    # Filter 4: Non-missing earnings quality\n",
    "    if 'earnings_quality_composite' in df.columns:\n",
    "        n_before = len(df)\n",
    "        df = df[df['earnings_quality_composite'].notna()]\n",
    "        filters_applied.append(f\"Missing EQ: -{n_before - len(df):,}\")\n",
    "    \n",
    "    print(\"\\nFilters Applied:\")\n",
    "    for f in filters_applied:\n",
    "        print(f\"  {f}\")\n",
    "    \n",
    "    print(f\"\\nFinal sample: {len(df):,} observations\")\n",
    "    print(f\"Unique firms: {df['ticker'].nunique()}\")\n",
    "    \n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "# Apply filters\n",
    "analysis_sample = apply_sample_filters(final_events, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA VALIDATION AND SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "def validate_and_summarize(df: pd.DataFrame):\n",
    "    \"\"\"Validate data quality and print summary statistics.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL DATASET VALIDATION AND SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Dimensions\n",
    "    print(f\"\\n--- Sample Dimensions ---\")\n",
    "    print(f\"Total observations: {len(df):,}\")\n",
    "    print(f\"Unique firms: {df['ticker'].nunique()}\")\n",
    "    print(f\"Date range: {df['ea_date'].min()} to {df['ea_date'].max()}\")\n",
    "    \n",
    "    # Key variables\n",
    "    print(f\"\\n--- Key Variables ---\")\n",
    "    \n",
    "    # Social media variables\n",
    "    sentiment_cols = [c for c in df.columns if 'pre_ea' in c and 'sentiment' in c]\n",
    "    print(\"\\nSocial Media Variables:\")\n",
    "    for col in sentiment_cols[:5]:\n",
    "        print(f\"  {col}: mean={df[col].mean():.4f}, std={df[col].std():.4f}, N={df[col].notna().sum()}\")\n",
    "    \n",
    "    # CAR variables\n",
    "    car_cols = [c for c in df.columns if c.startswith('CAR_') and '_raw' not in c]\n",
    "    print(\"\\nCAR Variables:\")\n",
    "    for col in car_cols:\n",
    "        print(f\"  {col}: mean={df[col].mean():.4f}, std={df[col].std():.4f}\")\n",
    "    \n",
    "    # Earnings quality\n",
    "    eq_cols = [c for c in df.columns if 'earnings_quality' in c]\n",
    "    print(\"\\nEarnings Quality Variables:\")\n",
    "    for col in eq_cols[:3]:\n",
    "        print(f\"  {col}: mean={df[col].mean():.4f}, std={df[col].std():.4f}\")\n",
    "    \n",
    "    # Correlations\n",
    "    print(\"\\n--- Key Correlations ---\")\n",
    "    key_vars = ['pre_ea_sentiment_mean', 'CAR_m1_p1', 'earnings_quality_composite', 'pre_ea_posts']\n",
    "    key_vars = [v for v in key_vars if v in df.columns]\n",
    "    if len(key_vars) > 1:\n",
    "        print(df[key_vars].corr().to_string())\n",
    "    \n",
    "    # Missing values\n",
    "    print(\"\\n--- Missing Values (>5%) ---\")\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = 100 * missing / len(df)\n",
    "    significant_missing = missing_pct[missing_pct > 5].sort_values(ascending=False)\n",
    "    if len(significant_missing) > 0:\n",
    "        for col, pct in significant_missing.items():\n",
    "            print(f\"  {col}: {pct:.1f}%\")\n",
    "    else:\n",
    "        print(\"  No variables with >5% missing\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Validate\n",
    "validate_and_summarize(analysis_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Interaction and Derived Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CREATE INTERACTION AND DERIVED VARIABLES\n",
    "# =============================================================================\n",
    "\n",
    "def create_analysis_variables(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create interaction terms and derived variables for analysis.\n",
    "    \n",
    "    Key interactions:\n",
    "    - Sentiment × EQ\n",
    "    - Attention × EQ\n",
    "    - Sentiment × Surprise\n",
    "    \n",
    "    Args:\n",
    "        df: Event-level dataset\n",
    "        \n",
    "    Returns:\n",
    "        Dataset with analysis variables\n",
    "    \"\"\"\n",
    "    print(\"Creating analysis variables...\")\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Standardize key variables\n",
    "    def standardize(series):\n",
    "        return (series - series.mean()) / series.std()\n",
    "    \n",
    "    # Standardized sentiment\n",
    "    if 'pre_ea_sentiment_mean' in df.columns:\n",
    "        df['sentiment_std'] = standardize(df['pre_ea_sentiment_mean'])\n",
    "        df['sentiment_high'] = (df['sentiment_std'] > 0.5).astype(int)\n",
    "        df['sentiment_low'] = (df['sentiment_std'] < -0.5).astype(int)\n",
    "    \n",
    "    # Standardized attention\n",
    "    if 'pre_ea_attention' in df.columns:\n",
    "        df['attention_std'] = standardize(df['pre_ea_attention'])\n",
    "        df['attention_high'] = (df['attention_std'] > 0.5).astype(int)\n",
    "    \n",
    "    # Standardized EQ\n",
    "    if 'earnings_quality_composite' in df.columns:\n",
    "        df['eq_std'] = standardize(df['earnings_quality_composite'])\n",
    "    \n",
    "    # Key interaction: Sentiment × EQ\n",
    "    if 'sentiment_std' in df.columns and 'eq_std' in df.columns:\n",
    "        df['sentiment_x_eq'] = df['sentiment_std'] * df['eq_std']\n",
    "        df['sentiment_x_eq_high'] = df['sentiment_std'] * df.get('eq_high', 0)\n",
    "        df['sentiment_x_eq_low'] = df['sentiment_std'] * df.get('eq_low', 0)\n",
    "    \n",
    "    # Attention × EQ\n",
    "    if 'attention_std' in df.columns and 'eq_std' in df.columns:\n",
    "        df['attention_x_eq'] = df['attention_std'] * df['eq_std']\n",
    "    \n",
    "    # Surprise interactions\n",
    "    if 'surprise_pct' in df.columns:\n",
    "        df['surprise_std'] = standardize(df['surprise_pct'])\n",
    "        df['surprise_positive'] = (df['surprise_pct'] > 0).astype(int)\n",
    "        df['surprise_negative'] = (df['surprise_pct'] < 0).astype(int)\n",
    "        \n",
    "        if 'sentiment_std' in df.columns:\n",
    "            df['sentiment_x_surprise'] = df['sentiment_std'] * df['surprise_std']\n",
    "    \n",
    "    # Sentiment disagreement\n",
    "    if 'pre_ea_sentiment_std' in df.columns:\n",
    "        df['disagreement_std'] = standardize(df['pre_ea_sentiment_std'])\n",
    "        df['high_disagreement'] = (df['disagreement_std'] > 0.5).astype(int)\n",
    "    \n",
    "    # Triple interaction: Sentiment × EQ × Surprise\n",
    "    if all(c in df.columns for c in ['sentiment_std', 'eq_std', 'surprise_std']):\n",
    "        df['sentiment_x_eq_x_surprise'] = df['sentiment_std'] * df['eq_std'] * df['surprise_std']\n",
    "    \n",
    "    print(f\"Created {len([c for c in df.columns if '_x_' in c or '_std' in c])} analysis variables\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create analysis variables\n",
    "analysis_sample = create_analysis_variables(analysis_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE FINAL DATASET\n",
    "# =============================================================================\n",
    "\n",
    "def save_final_dataset(df: pd.DataFrame, output_dir: str):\n",
    "    \"\"\"Save final analysis dataset with documentation.\"\"\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Main dataset (Parquet)\n",
    "    filepath = os.path.join(output_dir, 'analysis_sample.parquet')\n",
    "    df.to_parquet(filepath, index=False)\n",
    "    print(f\"Saved: {filepath}\")\n",
    "    \n",
    "    # CSV for Stata/other software\n",
    "    csv_path = os.path.join(output_dir, 'analysis_sample.csv')\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved: {csv_path}\")\n",
    "    \n",
    "    # Data dictionary\n",
    "    data_dict = {\n",
    "        'Identifiers': {\n",
    "            'ticker': 'Stock ticker symbol',\n",
    "            'ea_date': 'Earnings announcement date'\n",
    "        },\n",
    "        'Pre-EA Social Media': {\n",
    "            'pre_ea_posts': 'Total posts in pre-EA window [-10,-2]',\n",
    "            'pre_ea_users': 'Unique users in pre-EA window',\n",
    "            'pre_ea_attention': 'Log(1 + pre_ea_posts)',\n",
    "            'pre_ea_sentiment_mean': 'Average FinBERT polarity',\n",
    "            'pre_ea_pos_share': 'Share of positive posts',\n",
    "            'pre_ea_neg_share': 'Share of negative posts',\n",
    "            'pre_ea_disagreement': 'Average sentiment std (disagreement)',\n",
    "            'sentiment_std': 'Standardized pre-EA sentiment'\n",
    "        },\n",
    "        'Returns (CARs)': {\n",
    "            'CAR_m1_p1': 'Market-adjusted CAR[-1,+1] (EA window)',\n",
    "            'CAR_0_p2': 'Market-adjusted CAR[0,+2]',\n",
    "            'CAR_p2_p20': 'Market-adjusted CAR[+2,+20] (drift)',\n",
    "            'CAR_m10_m2': 'Market-adjusted CAR[-10,-2] (pre-EA)',\n",
    "            'ea_volatility': 'Return volatility around EA (annualized)'\n",
    "        },\n",
    "        'Earnings Quality': {\n",
    "            'earnings_quality_dd_std': 'DD model quality (standardized)',\n",
    "            'earnings_quality_mcn_std': 'McNichols model quality (standardized)',\n",
    "            'earnings_quality_composite': 'Composite EQ measure',\n",
    "            'eq_quintile': 'EQ quintile (Q1_Low to Q5_High)',\n",
    "            'eq_high': 'High EQ indicator (Q4-Q5)',\n",
    "            'eq_low': 'Low EQ indicator (Q1-Q2)',\n",
    "            'eq_std': 'Standardized composite EQ'\n",
    "        },\n",
    "        'Earnings Surprise': {\n",
    "            'eps_estimate': 'Consensus EPS estimate',\n",
    "            'reported_eps': 'Reported EPS',\n",
    "            'surprise_pct': 'Earnings surprise (%)',\n",
    "            'surprise_std': 'Standardized surprise',\n",
    "            'surprise_positive': 'Positive surprise indicator',\n",
    "            'surprise_negative': 'Negative surprise indicator'\n",
    "        },\n",
    "        'Controls': {\n",
    "            'price_pre_ea': 'Stock price before EA',\n",
    "            'log_price': 'Log stock price',\n",
    "            'volume_pre_ea': 'Trading volume before EA',\n",
    "            'volatility_pre_ea': 'Pre-EA volatility (annualized)',\n",
    "            'ret_1m/3m/6m': 'Prior cumulative returns',\n",
    "            'sector': 'GICS sector',\n",
    "            'ea_year/quarter/month': 'Time indicators'\n",
    "        },\n",
    "        'Interactions': {\n",
    "            'sentiment_x_eq': 'Sentiment × EQ interaction',\n",
    "            'attention_x_eq': 'Attention × EQ interaction',\n",
    "            'sentiment_x_surprise': 'Sentiment × Surprise interaction',\n",
    "            'sentiment_x_eq_x_surprise': 'Triple interaction'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    dict_path = os.path.join(output_dir, 'analysis_data_dictionary.json')\n",
    "    with open(dict_path, 'w') as f:\n",
    "        json.dump(data_dict, f, indent=2)\n",
    "    print(f\"Saved: {dict_path}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    summary = {\n",
    "        'sample_size': len(df),\n",
    "        'unique_firms': int(df['ticker'].nunique()),\n",
    "        'date_range': [str(df['ea_date'].min()), str(df['ea_date'].max())],\n",
    "        'variables': list(df.columns),\n",
    "        'created_at': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    summary_path = os.path.join(output_dir, 'analysis_summary.json')\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=2, default=str)\n",
    "    print(f\"Saved: {summary_path}\")\n",
    "\n",
    "# Save final dataset\n",
    "save_final_dataset(analysis_sample, config.FINAL_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# NOTEBOOK SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════╗\n",
    "║       NOTEBOOK 5: DATA MERGING COMPLETE                          ║\n",
    "╚══════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "FINAL DATASET STRUCTURE:\n",
    "────────────────────────\n",
    "Unit of Observation: Firm-Earnings Announcement Event\n",
    "\n",
    "Variable Blocks:\n",
    "  1. Pre-EA WSB Sentiment and Attention\n",
    "     • Posts, users, sentiment scores\n",
    "     • Aggregated over [-10, -2] trading days\n",
    "\n",
    "  2. Earnings Announcement CARs\n",
    "     • CAR[-1,+1]: EA announcement return\n",
    "     • CAR[+2,+20]: Post-EA drift\n",
    "\n",
    "  3. Earnings Quality\n",
    "     • Dechow-Dichev measure\n",
    "     • McNichols modification\n",
    "     • Composite score\n",
    "\n",
    "  4. Earnings Surprise\n",
    "     • Actual vs. expected EPS\n",
    "     • Surprise direction\n",
    "\n",
    "  5. Control Variables\n",
    "     • Size, momentum, volatility\n",
    "     • Sector, timing\n",
    "\n",
    "  6. Interaction Terms\n",
    "     • Sentiment × EQ\n",
    "     • Attention × EQ\n",
    "     • Triple interactions\n",
    "\n",
    "OUTPUT FILES:\n",
    "─────────────\n",
    "• analysis_sample.parquet        - Main analysis dataset\n",
    "• analysis_sample.csv            - CSV version\n",
    "• analysis_data_dictionary.json  - Variable definitions\n",
    "• analysis_summary.json          - Summary statistics\n",
    "\n",
    "NEXT STEPS:\n",
    "───────────\n",
    "→ Notebook 6: Empirical Analysis & Regression Tests\n",
    "  - Baseline pricing regressions\n",
    "  - EQ interaction analysis\n",
    "  - Reversal/mispricing tests\n",
    "  - Robustness checks\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Final sample: {len(analysis_sample):,} observations\")\n",
    "print(f\"Unique firms: {analysis_sample['ticker'].nunique()}\")\n",
    "print(f\"Columns: {len(analysis_sample.columns)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 3: Yahoo Finance Message Board Scraping\n",
        "## Social Media-Driven Stock Manipulation and Tail Risk Research\n",
        "\n",
        "---\n",
        "\n",
        "**Research Project:** Social Media-Driven Stock Manipulation and Tail Risk\n",
        "\n",
        "**Purpose:** Scrape social media discussion data from Yahoo Finance message boards. Compute message volume baselines and identify social media bursts.\n",
        "\n",
        "**Data Source:** Yahoo Finance Community/Conversations (`finance.yahoo.com/quote/{TICKER}/community`)\n",
        "\n",
        "**Output:** \n",
        "- Message-level data (timestamp, username, text)\n",
        "- Daily message volume aggregates\n",
        "- Social media burst flags\n",
        "\n",
        "**Ethics:**\n",
        "- Respect robots.txt and rate limits\n",
        "- Only scrape publicly visible content\n",
        "- Anonymize usernames in final analysis\n",
        "\n",
        "---\n",
        "\n",
        "**Last Updated:** 2025"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# INSTALL REQUIRED PACKAGES\n",
        "# =============================================================================\n",
        "\n",
        "!pip install pandas==2.0.3\n",
        "!pip install numpy==1.24.3\n",
        "!pip install requests==2.31.0\n",
        "!pip install beautifulsoup4==4.12.2\n",
        "!pip install lxml==4.9.3\n",
        "!pip install selenium==4.15.2\n",
        "!pip install webdriver-manager==4.0.1\n",
        "!pip install tqdm==4.66.1\n",
        "!pip install pyarrow==14.0.1\n",
        "!pip install fake-useragent==1.4.0\n",
        "\n",
        "# For Colab: Install Chrome and ChromeDriver\n",
        "!apt-get update\n",
        "!apt-get install -y chromium-chromedriver\n",
        "\n",
        "print(\"All packages installed successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# IMPORT LIBRARIES\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import hashlib\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "from collections import defaultdict\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Selenium for dynamic content\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
        "\n",
        "try:\n",
        "    from fake_useragent import UserAgent\n",
        "    ua = UserAgent()\n",
        "except:\n",
        "    ua = None\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "\n",
        "print(f\"Environment setup complete. Timestamp: {datetime.now()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "class ResearchConfig:\n",
        "    \"\"\"Configuration for social media scraping.\"\"\"\n",
        "    \n",
        "    # Sample Period\n",
        "    START_DATE = \"2019-01-01\"\n",
        "    END_DATE = \"2025-12-31\"\n",
        "    \n",
        "    # Social Media Parameters\n",
        "    ROLLING_WINDOW = 60  # days for baseline\n",
        "    MIN_PERIODS = 20\n",
        "    SOCIAL_ZSCORE_THRESHOLD = 3.0\n",
        "    \n",
        "    # Data Storage Paths\n",
        "    BASE_PATH = \"/content/drive/MyDrive/Research/PumpDump/\"\n",
        "    RAW_DATA_PATH = BASE_PATH + \"data/raw/\"\n",
        "    PROCESSED_DATA_PATH = BASE_PATH + \"data/processed/\"\n",
        "    \n",
        "    # Scraping Rate Limits (BE POLITE!)\n",
        "    MIN_DELAY = 3.0  # seconds between requests\n",
        "    MAX_DELAY = 7.0  # randomized delay\n",
        "    MAX_PAGES_PER_TICKER = 50  # limit pages to scrape\n",
        "    MAX_RETRIES = 3\n",
        "    \n",
        "    # User Agents (rotate to avoid detection)\n",
        "    USER_AGENTS = [\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
        "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',\n",
        "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',\n",
        "    ]\n",
        "\n",
        "config = ResearchConfig()\n",
        "\n",
        "# Handle Colab vs local\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    print(\"Not running in Colab - using local paths\")\n",
        "    IN_COLAB = False\n",
        "    config.BASE_PATH = \"./research_data/\"\n",
        "    config.RAW_DATA_PATH = config.BASE_PATH + \"data/raw/\"\n",
        "    config.PROCESSED_DATA_PATH = config.BASE_PATH + \"data/processed/\"\n",
        "\n",
        "os.makedirs(config.RAW_DATA_PATH, exist_ok=True)\n",
        "os.makedirs(config.PROCESSED_DATA_PATH, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# LOAD STOCK UNIVERSE\n",
        "# =============================================================================\n",
        "\n",
        "def load_universe(data_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Load stock universe from Notebook 1 output.\"\"\"\n",
        "    universe_path = os.path.join(data_path, 'stock_universe.parquet')\n",
        "    \n",
        "    if os.path.exists(universe_path):\n",
        "        universe = pd.read_parquet(universe_path)\n",
        "        print(f\"Loaded universe: {len(universe)} tickers\")\n",
        "    else:\n",
        "        print(\"Universe file not found - creating sample universe\")\n",
        "        sample_tickers = [\n",
        "            'GME', 'AMC', 'BB', 'NOK', 'BBBY', 'KOSS', 'CLOV', 'WISH',\n",
        "            'PLTR', 'SPCE', 'TLRY', 'SNDL', 'MULN', 'FFIE'\n",
        "        ]\n",
        "        universe = pd.DataFrame({\n",
        "            'ticker': sample_tickers,\n",
        "            'is_confirmed_manipulation': [False] * len(sample_tickers),\n",
        "            'source': ['sample'] * len(sample_tickers)\n",
        "        })\n",
        "    \n",
        "    return universe\n",
        "\n",
        "universe_df = load_universe(config.PROCESSED_DATA_PATH)\n",
        "print(f\"\\nTickers to scrape: {len(universe_df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Yahoo Finance Message Board Scraper\n",
        "\n",
        "### 3.1 Scraper Implementation\n",
        "\n",
        "**IMPORTANT NOTES:**\n",
        "- Yahoo Finance message boards use dynamic JavaScript loading\n",
        "- We use Selenium for JavaScript rendering\n",
        "- Always respect rate limits and robots.txt\n",
        "- Only scrape publicly visible content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# YAHOO MESSAGE BOARD SCRAPER\n",
        "# =============================================================================\n",
        "\n",
        "class YahooMessageBoardScraper:\n",
        "    \"\"\"Scrapes Yahoo Finance message boards/conversations.\n",
        "    \n",
        "    Yahoo Finance community pages require JavaScript rendering.\n",
        "    Uses Selenium with headless Chrome for scraping.\n",
        "    \n",
        "    IMPORTANT: This scraper respects rate limits and only collects\n",
        "    publicly available information. Usernames are hashed for privacy.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config: ResearchConfig):\n",
        "        self.config = config\n",
        "        self.driver = None\n",
        "        self.messages_collected = []\n",
        "        self.failed_tickers = []\n",
        "        \n",
        "    def _get_random_user_agent(self) -> str:\n",
        "        \"\"\"Get a random user agent.\"\"\"\n",
        "        if ua:\n",
        "            return ua.random\n",
        "        return random.choice(self.config.USER_AGENTS)\n",
        "    \n",
        "    def _rate_limit(self):\n",
        "        \"\"\"Implement polite rate limiting.\"\"\"\n",
        "        delay = random.uniform(self.config.MIN_DELAY, self.config.MAX_DELAY)\n",
        "        time.sleep(delay)\n",
        "    \n",
        "    def _hash_username(self, username: str) -> str:\n",
        "        \"\"\"Hash username for privacy.\"\"\"\n",
        "        if not username:\n",
        "            return 'anonymous'\n",
        "        return hashlib.md5(username.encode()).hexdigest()[:12]\n",
        "    \n",
        "    def setup_driver(self):\n",
        "        \"\"\"Initialize Selenium WebDriver.\"\"\"\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument('--headless')\n",
        "        chrome_options.add_argument('--no-sandbox')\n",
        "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "        chrome_options.add_argument('--disable-gpu')\n",
        "        chrome_options.add_argument(f'user-agent={self._get_random_user_agent()}')\n",
        "        chrome_options.add_argument('--window-size=1920,1080')\n",
        "        \n",
        "        # Disable images for faster loading\n",
        "        prefs = {'profile.managed_default_content_settings.images': 2}\n",
        "        chrome_options.add_experimental_option('prefs', prefs)\n",
        "        \n",
        "        try:\n",
        "            # For Colab\n",
        "            self.driver = webdriver.Chrome(options=chrome_options)\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing driver: {e}\")\n",
        "            print(\"Trying with webdriver-manager...\")\n",
        "            from webdriver_manager.chrome import ChromeDriverManager\n",
        "            service = Service(ChromeDriverManager().install())\n",
        "            self.driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "        \n",
        "        self.driver.set_page_load_timeout(30)\n",
        "        print(\"WebDriver initialized\")\n",
        "    \n",
        "    def close_driver(self):\n",
        "        \"\"\"Close the WebDriver.\"\"\"\n",
        "        if self.driver:\n",
        "            self.driver.quit()\n",
        "            self.driver = None\n",
        "    \n",
        "    def scrape_ticker_conversations(self, ticker: str, \n",
        "                                     max_scrolls: int = 20) -> List[Dict]:\n",
        "        \"\"\"Scrape conversations for a single ticker.\n",
        "        \n",
        "        Args:\n",
        "            ticker: Stock ticker symbol\n",
        "            max_scrolls: Maximum page scrolls for lazy loading\n",
        "            \n",
        "        Returns:\n",
        "            List of message dictionaries\n",
        "        \"\"\"\n",
        "        messages = []\n",
        "        url = f\"https://finance.yahoo.com/quote/{ticker}/community\"\n",
        "        \n",
        "        try:\n",
        "            self.driver.get(url)\n",
        "            time.sleep(3)  # Wait for initial load\n",
        "            \n",
        "            # Scroll to load more content (lazy loading)\n",
        "            for scroll in range(max_scrolls):\n",
        "                # Scroll down\n",
        "                self.driver.execute_script(\n",
        "                    \"window.scrollTo(0, document.body.scrollHeight);\"\n",
        "                )\n",
        "                time.sleep(1.5)  # Wait for content to load\n",
        "                \n",
        "                # Check if we've reached the bottom\n",
        "                try:\n",
        "                    # Look for \"Show more\" or end of content\n",
        "                    end_marker = self.driver.find_elements(\n",
        "                        By.XPATH, \n",
        "                        \"//div[contains(text(), 'No more comments')]|//div[contains(text(), 'End of')]\"\n",
        "                    )\n",
        "                    if end_marker:\n",
        "                        break\n",
        "                except:\n",
        "                    pass\n",
        "            \n",
        "            # Parse page content\n",
        "            soup = BeautifulSoup(self.driver.page_source, 'lxml')\n",
        "            \n",
        "            # Yahoo Finance conversation structure (may change - inspect page)\n",
        "            # Common selectors to try:\n",
        "            comment_selectors = [\n",
        "                'div[data-test=\"comment-content\"]',\n",
        "                'li[class*=\"comment\"]',\n",
        "                'div[class*=\"Comment\"]',\n",
        "                'article[class*=\"comment\"]',\n",
        "                'div[class*=\"conversation\"]'\n",
        "            ]\n",
        "            \n",
        "            comments = []\n",
        "            for selector in comment_selectors:\n",
        "                comments = soup.select(selector)\n",
        "                if comments:\n",
        "                    break\n",
        "            \n",
        "            # Extract message details\n",
        "            for comment in comments:\n",
        "                try:\n",
        "                    # Extract text content\n",
        "                    text_elem = comment.find(['p', 'div', 'span'], \n",
        "                                             class_=lambda x: x and 'content' in x.lower() if x else False)\n",
        "                    if not text_elem:\n",
        "                        text_elem = comment\n",
        "                    text = text_elem.get_text(strip=True)\n",
        "                    \n",
        "                    # Extract timestamp\n",
        "                    time_elem = comment.find(['time', 'span'], \n",
        "                                             attrs={'datetime': True})\n",
        "                    if time_elem:\n",
        "                        timestamp = time_elem.get('datetime')\n",
        "                    else:\n",
        "                        # Try to find relative time\n",
        "                        time_text = comment.find(string=re.compile(r'\\d+\\s*(hour|day|week|month|min)'))\n",
        "                        timestamp = str(time_text) if time_text else None\n",
        "                    \n",
        "                    # Extract username\n",
        "                    user_elem = comment.find(['a', 'span'], \n",
        "                                             class_=lambda x: x and ('author' in x.lower() or 'user' in x.lower()) if x else False)\n",
        "                    username = user_elem.get_text(strip=True) if user_elem else 'anonymous'\n",
        "                    \n",
        "                    # Extract reaction counts if available\n",
        "                    likes_elem = comment.find(string=re.compile(r'^\\d+$'))\n",
        "                    likes = int(likes_elem) if likes_elem else 0\n",
        "                    \n",
        "                    if text and len(text) > 5:  # Filter out very short/empty posts\n",
        "                        messages.append({\n",
        "                            'ticker': ticker,\n",
        "                            'timestamp_raw': timestamp,\n",
        "                            'username_hash': self._hash_username(username),\n",
        "                            'text': text[:5000],  # Truncate very long posts\n",
        "                            'likes': likes,\n",
        "                            'scrape_time': datetime.now().isoformat()\n",
        "                        })\n",
        "                        \n",
        "                except Exception as e:\n",
        "                    continue\n",
        "            \n",
        "        except TimeoutException:\n",
        "            print(f\"  Timeout loading {ticker}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Error scraping {ticker}: {e}\")\n",
        "        \n",
        "        return messages\n",
        "    \n",
        "    def scrape_all_tickers(self, tickers: List[str], \n",
        "                           checkpoint_every: int = 10) -> pd.DataFrame:\n",
        "        \"\"\"Scrape message boards for all tickers.\n",
        "        \n",
        "        Args:\n",
        "            tickers: List of ticker symbols\n",
        "            checkpoint_every: Save checkpoint every N tickers\n",
        "            \n",
        "        Returns:\n",
        "            DataFrame with all messages\n",
        "        \"\"\"\n",
        "        print(f\"Scraping {len(tickers)} tickers\")\n",
        "        print(\"IMPORTANT: This process respects rate limits and will take time.\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        if not self.driver:\n",
        "            self.setup_driver()\n",
        "        \n",
        "        all_messages = []\n",
        "        \n",
        "        for i, ticker in enumerate(tqdm(tickers, desc=\"Scraping tickers\")):\n",
        "            print(f\"\\n  [{i+1}/{len(tickers)}] Scraping {ticker}...\")\n",
        "            \n",
        "            # Scrape messages\n",
        "            messages = self.scrape_ticker_conversations(ticker)\n",
        "            all_messages.extend(messages)\n",
        "            \n",
        "            print(f\"    Found {len(messages)} messages\")\n",
        "            \n",
        "            # Rate limiting\n",
        "            self._rate_limit()\n",
        "            \n",
        "            # Checkpoint save\n",
        "            if (i + 1) % checkpoint_every == 0:\n",
        "                checkpoint_df = pd.DataFrame(all_messages)\n",
        "                checkpoint_path = os.path.join(\n",
        "                    self.config.RAW_DATA_PATH, \n",
        "                    f'messages_checkpoint_{i+1}.parquet'\n",
        "                )\n",
        "                checkpoint_df.to_parquet(checkpoint_path, index=False)\n",
        "                print(f\"  Checkpoint saved: {checkpoint_path}\")\n",
        "        \n",
        "        self.close_driver()\n",
        "        \n",
        "        df = pd.DataFrame(all_messages)\n",
        "        \n",
        "        print(f\"\\n\" + \"=\"*60)\n",
        "        print(\"SCRAPING COMPLETE\")\n",
        "        print(f\"Total messages: {len(df):,}\")\n",
        "        print(f\"Unique tickers with messages: {df['ticker'].nunique()}\")\n",
        "        \n",
        "        return df\n",
        "\n",
        "\n",
        "# Initialize scraper\n",
        "message_scraper = YahooMessageBoardScraper(config)\n",
        "print(\"Yahoo Message Board Scraper initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Alternative: Simpler Request-Based Scraper\n",
        "\n",
        "If Selenium is too slow or unreliable, this simpler approach uses requests + BeautifulSoup.\n",
        "Note: This may not capture all content due to JavaScript loading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SIMPLE REQUEST-BASED SCRAPER (FALLBACK)\n",
        "# =============================================================================\n",
        "\n",
        "class SimpleYahooScraper:\n",
        "    \"\"\"Simplified scraper using requests only.\n",
        "    \n",
        "    This is faster but may miss dynamically loaded content.\n",
        "    Use as fallback when Selenium is not available.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config: ResearchConfig):\n",
        "        self.config = config\n",
        "        self.session = requests.Session()\n",
        "        \n",
        "    def _get_headers(self) -> Dict:\n",
        "        return {\n",
        "            'User-Agent': random.choice(self.config.USER_AGENTS),\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "            'Accept-Encoding': 'gzip, deflate',\n",
        "            'Connection': 'keep-alive',\n",
        "        }\n",
        "    \n",
        "    def _rate_limit(self):\n",
        "        time.sleep(random.uniform(self.config.MIN_DELAY, self.config.MAX_DELAY))\n",
        "    \n",
        "    def _hash_username(self, username: str) -> str:\n",
        "        if not username:\n",
        "            return 'anonymous'\n",
        "        return hashlib.md5(username.encode()).hexdigest()[:12]\n",
        "    \n",
        "    def scrape_ticker(self, ticker: str) -> List[Dict]:\n",
        "        \"\"\"Scrape basic page content for a ticker.\"\"\"\n",
        "        messages = []\n",
        "        url = f\"https://finance.yahoo.com/quote/{ticker}/community\"\n",
        "        \n",
        "        try:\n",
        "            response = self.session.get(url, headers=self._get_headers(), timeout=30)\n",
        "            response.raise_for_status()\n",
        "            \n",
        "            soup = BeautifulSoup(response.content, 'lxml')\n",
        "            \n",
        "            # Try to extract any visible messages\n",
        "            # Note: Most content requires JavaScript\n",
        "            text_blocks = soup.find_all(['p', 'div'], \n",
        "                                        class_=lambda x: x and 'comment' in x.lower() if x else False)\n",
        "            \n",
        "            for block in text_blocks:\n",
        "                text = block.get_text(strip=True)\n",
        "                if text and len(text) > 10:\n",
        "                    messages.append({\n",
        "                        'ticker': ticker,\n",
        "                        'timestamp_raw': None,\n",
        "                        'username_hash': 'unknown',\n",
        "                        'text': text[:5000],\n",
        "                        'likes': 0,\n",
        "                        'scrape_time': datetime.now().isoformat()\n",
        "                    })\n",
        "                    \n",
        "        except Exception as e:\n",
        "            print(f\"Error scraping {ticker}: {e}\")\n",
        "        \n",
        "        self._rate_limit()\n",
        "        return messages\n",
        "\n",
        "\n",
        "# Alternative scraper instance\n",
        "simple_scraper = SimpleYahooScraper(config)\n",
        "print(\"Simple Yahoo Scraper initialized (fallback)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Execute Scraping\n",
        "\n",
        "**WARNING:** This section will take significant time due to rate limiting.\n",
        "Consider running overnight or in batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# EXECUTE SCRAPING (CAUTION: TIME-CONSUMING)\n",
        "# =============================================================================\n",
        "\n",
        "# Get tickers to scrape (limit for demo)\n",
        "tickers_to_scrape = universe_df['ticker'].unique().tolist()\n",
        "\n",
        "# For demo purposes, limit to top volatile tickers\n",
        "# Remove this limit for full research\n",
        "MAX_TICKERS_DEMO = 20\n",
        "if len(tickers_to_scrape) > MAX_TICKERS_DEMO:\n",
        "    print(f\"Limiting to {MAX_TICKERS_DEMO} tickers for demonstration\")\n",
        "    print(\"Remove MAX_TICKERS_DEMO limit for full research\")\n",
        "    tickers_to_scrape = tickers_to_scrape[:MAX_TICKERS_DEMO]\n",
        "\n",
        "print(f\"\\nTickers to scrape: {len(tickers_to_scrape)}\")\n",
        "print(tickers_to_scrape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# RUN SCRAPING (UNCOMMENT TO EXECUTE)\n",
        "# =============================================================================\n",
        "\n",
        "# WARNING: This will take a long time!\n",
        "# Estimated time: 3-7 seconds per ticker\n",
        "# For 20 tickers: ~2-3 minutes\n",
        "# For 200 tickers: ~20-30 minutes\n",
        "\n",
        "print(\"Starting message board scraping...\")\n",
        "print(f\"Estimated time: {len(tickers_to_scrape) * 5 / 60:.1f} minutes\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "try:\n",
        "    # Try Selenium scraper first\n",
        "    messages_df = message_scraper.scrape_all_tickers(tickers_to_scrape)\n",
        "except Exception as e:\n",
        "    print(f\"Selenium scraper failed: {e}\")\n",
        "    print(\"Falling back to simple scraper...\")\n",
        "    \n",
        "    # Fallback to simple scraper\n",
        "    all_messages = []\n",
        "    for ticker in tqdm(tickers_to_scrape, desc=\"Scraping (simple)\"):\n",
        "        messages = simple_scraper.scrape_ticker(ticker)\n",
        "        all_messages.extend(messages)\n",
        "    messages_df = pd.DataFrame(all_messages)\n",
        "\n",
        "print(f\"\\nScraping complete. Total messages: {len(messages_df):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CREATE SYNTHETIC DATA FOR DEMONSTRATION\n",
        "# =============================================================================\n",
        "\n",
        "# If scraping failed or for demonstration, create synthetic data\n",
        "# This simulates the structure of scraped message data\n",
        "\n",
        "def create_synthetic_messages(tickers: List[str], \n",
        "                               start_date: str, \n",
        "                               end_date: str,\n",
        "                               base_messages_per_day: int = 5) -> pd.DataFrame:\n",
        "    \"\"\"Create synthetic message data for demonstration.\n",
        "    \n",
        "    This generates realistic-looking message patterns including:\n",
        "    - Normal baseline activity\n",
        "    - Burst periods with high message volume\n",
        "    - User concentration patterns\n",
        "    \"\"\"\n",
        "    np.random.seed(42)\n",
        "    messages = []\n",
        "    \n",
        "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
        "    \n",
        "    # Promotional message templates\n",
        "    promo_templates = [\n",
        "        \"Buy now before it's too late! {} is about to moon!\",\n",
        "        \"{} to the moon! Get in while you can!\",\n",
        "        \"This is the next GME! {} squeeze incoming!\",\n",
        "        \"Easy money on {}. Don't miss this!\",\n",
        "        \"{} rocket launching soon!\",\n",
        "        \"Huge gains coming on {}. Last chance!\"\n",
        "    ]\n",
        "    \n",
        "    # Normal message templates\n",
        "    normal_templates = [\n",
        "        \"What do you think about {} earnings?\",\n",
        "        \"Holding {} long term.\",\n",
        "        \"Anyone else watching {}?\",\n",
        "        \"Good entry point for {}?\",\n",
        "        \"{} looks oversold.\",\n",
        "        \"DD on {}: fundamentals look solid.\"\n",
        "    ]\n",
        "    \n",
        "    for ticker in tickers:\n",
        "        # Generate burst periods (random 3-5 day windows)\n",
        "        num_bursts = np.random.randint(2, 5)\n",
        "        burst_starts = np.random.choice(len(date_range) - 10, num_bursts, replace=False)\n",
        "        burst_periods = set()\n",
        "        for start_idx in burst_starts:\n",
        "            for i in range(np.random.randint(2, 5)):\n",
        "                if start_idx + i < len(date_range):\n",
        "                    burst_periods.add(start_idx + i)\n",
        "        \n",
        "        for day_idx, date in enumerate(date_range):\n",
        "            # Determine message count\n",
        "            if day_idx in burst_periods:\n",
        "                # Burst period: 5-20x normal volume\n",
        "                msg_count = np.random.poisson(base_messages_per_day * np.random.randint(5, 20))\n",
        "                promo_ratio = 0.6  # Higher promotional content during bursts\n",
        "            else:\n",
        "                # Normal period\n",
        "                msg_count = np.random.poisson(base_messages_per_day)\n",
        "                promo_ratio = 0.1\n",
        "            \n",
        "            # Generate messages\n",
        "            for _ in range(msg_count):\n",
        "                # Choose template\n",
        "                is_promo = np.random.random() < promo_ratio\n",
        "                if is_promo:\n",
        "                    template = np.random.choice(promo_templates)\n",
        "                else:\n",
        "                    template = np.random.choice(normal_templates)\n",
        "                \n",
        "                text = template.format(ticker)\n",
        "                \n",
        "                # User ID (concentrated during bursts)\n",
        "                if day_idx in burst_periods:\n",
        "                    # Few users dominate during pumps\n",
        "                    user_id = np.random.choice(10)  # Only 10 active users\n",
        "                else:\n",
        "                    user_id = np.random.randint(0, 100)\n",
        "                \n",
        "                messages.append({\n",
        "                    'ticker': ticker,\n",
        "                    'date': date.date(),\n",
        "                    'timestamp_raw': date + timedelta(hours=np.random.randint(9, 16)),\n",
        "                    'username_hash': hashlib.md5(f\"user_{user_id}\".encode()).hexdigest()[:12],\n",
        "                    'text': text,\n",
        "                    'is_promotional': is_promo,\n",
        "                    'likes': np.random.poisson(3) if is_promo else np.random.poisson(1)\n",
        "                })\n",
        "    \n",
        "    return pd.DataFrame(messages)\n",
        "\n",
        "\n",
        "# Check if we have scraped data, otherwise create synthetic\n",
        "if 'messages_df' not in dir() or len(messages_df) == 0:\n",
        "    print(\"Creating synthetic message data for demonstration...\")\n",
        "    messages_df = create_synthetic_messages(\n",
        "        tickers=tickers_to_scrape,\n",
        "        start_date='2020-01-01',\n",
        "        end_date='2023-12-31',\n",
        "        base_messages_per_day=3\n",
        "    )\n",
        "    print(f\"Created {len(messages_df):,} synthetic messages\")\n",
        "\n",
        "print(f\"\\nMessages DataFrame:\")\n",
        "print(messages_df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Compute Social Media Metrics\n",
        "\n",
        "### 5.1 Aggregate to Daily Level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SOCIAL MEDIA METRICS CALCULATOR\n",
        "# =============================================================================\n",
        "\n",
        "class SocialMetricsCalculator:\n",
        "    \"\"\"Computes social media metrics for episode detection.\n",
        "    \n",
        "    Metrics computed:\n",
        "    - Daily message count\n",
        "    - Unique users per day\n",
        "    - User concentration (Gini coefficient)\n",
        "    - Promotional message share\n",
        "    - Rolling baselines and z-scores\n",
        "    \"\"\"\n",
        "    \n",
        "    # Keywords indicating promotional content\n",
        "    PROMO_KEYWORDS = [\n",
        "        'buy now', 'get in', 'moon', 'rocket', 'to the moon',\n",
        "        'guaranteed', 'easy money', 'next gme', 'squeeze', 'huge gains',\n",
        "        'dont miss', \"don't miss\", 'last chance', 'about to explode',\n",
        "        'yolo', 'all in', 'going viral', 'insider', 'manipulation',\n",
        "        'short squeeze', 'gamma squeeze', 'diamond hands', 'hold the line'\n",
        "    ]\n",
        "    \n",
        "    def __init__(self, window: int = 60, min_periods: int = 20):\n",
        "        self.window = window\n",
        "        self.min_periods = min_periods\n",
        "    \n",
        "    def classify_promotional(self, text: str) -> bool:\n",
        "        \"\"\"Classify if a message is promotional.\"\"\"\n",
        "        if not text or not isinstance(text, str):\n",
        "            return False\n",
        "        text_lower = text.lower()\n",
        "        return any(kw in text_lower for kw in self.PROMO_KEYWORDS)\n",
        "    \n",
        "    def compute_gini(self, values: List[int]) -> float:\n",
        "        \"\"\"Compute Gini coefficient for user concentration.\n",
        "        \n",
        "        Gini = 0: Perfect equality (all users post equally)\n",
        "        Gini = 1: Perfect inequality (one user dominates)\n",
        "        \"\"\"\n",
        "        if not values or len(values) == 0:\n",
        "            return np.nan\n",
        "        \n",
        "        values = np.array(values)\n",
        "        values = values[values > 0]  # Remove zeros\n",
        "        \n",
        "        if len(values) == 0:\n",
        "            return np.nan\n",
        "        \n",
        "        sorted_values = np.sort(values)\n",
        "        n = len(sorted_values)\n",
        "        cumsum = np.cumsum(sorted_values)\n",
        "        gini = (2 * np.sum((np.arange(1, n+1) * sorted_values))) / (n * cumsum[-1]) - (n + 1) / n\n",
        "        \n",
        "        return max(0, min(1, gini))  # Clamp to [0, 1]\n",
        "    \n",
        "    def aggregate_daily(self, messages_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Aggregate message-level data to daily ticker-level.\"\"\"\n",
        "        df = messages_df.copy()\n",
        "        \n",
        "        # Ensure date column exists\n",
        "        if 'date' not in df.columns:\n",
        "            if 'timestamp_raw' in df.columns:\n",
        "                df['date'] = pd.to_datetime(df['timestamp_raw']).dt.date\n",
        "            else:\n",
        "                raise ValueError(\"No date or timestamp column found\")\n",
        "        \n",
        "        # Classify promotional messages\n",
        "        if 'is_promotional' not in df.columns:\n",
        "            df['is_promotional'] = df['text'].apply(self.classify_promotional)\n",
        "        \n",
        "        # Aggregate\n",
        "        daily_agg = df.groupby(['ticker', 'date']).agg(\n",
        "            msg_count=('text', 'count'),\n",
        "            unique_users=('username_hash', 'nunique'),\n",
        "            promo_count=('is_promotional', 'sum'),\n",
        "            total_likes=('likes', 'sum')\n",
        "        ).reset_index()\n",
        "        \n",
        "        # Compute promotional share\n",
        "        daily_agg['promo_share'] = daily_agg['promo_count'] / daily_agg['msg_count']\n",
        "        daily_agg['promo_share'] = daily_agg['promo_share'].fillna(0)\n",
        "        \n",
        "        # Compute user concentration per day\n",
        "        user_counts = df.groupby(['ticker', 'date', 'username_hash']).size().reset_index(name='user_msg_count')\n",
        "        \n",
        "        gini_scores = []\n",
        "        for (ticker, date), group in user_counts.groupby(['ticker', 'date']):\n",
        "            gini = self.compute_gini(group['user_msg_count'].tolist())\n",
        "            gini_scores.append({'ticker': ticker, 'date': date, 'user_concentration': gini})\n",
        "        \n",
        "        gini_df = pd.DataFrame(gini_scores)\n",
        "        daily_agg = daily_agg.merge(gini_df, on=['ticker', 'date'], how='left')\n",
        "        \n",
        "        return daily_agg\n",
        "    \n",
        "    def compute_rolling_baselines(self, daily_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Compute rolling baseline statistics.\"\"\"\n",
        "        df = daily_df.copy()\n",
        "        df = df.sort_values(['ticker', 'date'])\n",
        "        \n",
        "        # Rolling mean and std for message count\n",
        "        df['msg_mean'] = df.groupby('ticker')['msg_count'].transform(\n",
        "            lambda x: x.rolling(window=self.window, min_periods=self.min_periods).mean()\n",
        "        )\n",
        "        df['msg_std'] = df.groupby('ticker')['msg_count'].transform(\n",
        "            lambda x: x.rolling(window=self.window, min_periods=self.min_periods).std()\n",
        "        )\n",
        "        df['msg_median'] = df.groupby('ticker')['msg_count'].transform(\n",
        "            lambda x: x.rolling(window=self.window, min_periods=self.min_periods).median()\n",
        "        )\n",
        "        \n",
        "        # Z-score for message volume\n",
        "        df['msg_zscore'] = (df['msg_count'] - df['msg_mean']) / df['msg_std']\n",
        "        \n",
        "        # Message ratio to median\n",
        "        df['msg_ratio'] = df['msg_count'] / df['msg_median']\n",
        "        \n",
        "        # Rolling promotional share baseline\n",
        "        df['promo_share_mean'] = df.groupby('ticker')['promo_share'].transform(\n",
        "            lambda x: x.rolling(window=self.window, min_periods=self.min_periods).mean()\n",
        "        )\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def flag_social_bursts(self, daily_df: pd.DataFrame, \n",
        "                           threshold: float = 3.0) -> pd.DataFrame:\n",
        "        \"\"\"Flag days with social media burst activity.\"\"\"\n",
        "        df = daily_df.copy()\n",
        "        \n",
        "        # Primary burst condition: message z-score > threshold\n",
        "        df['is_social_burst'] = df['msg_zscore'] > threshold\n",
        "        \n",
        "        # Alternative conditions for robustness\n",
        "        df['is_volume_burst'] = df['msg_ratio'] > 3.0  # 3x median\n",
        "        df['is_concentrated'] = df['user_concentration'] > 0.5  # High Gini\n",
        "        df['is_promo_heavy'] = df['promo_share'] > 0.3  # >30% promotional\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def process_all(self, messages_df: pd.DataFrame, \n",
        "                    zscore_threshold: float = 3.0) -> pd.DataFrame:\n",
        "        \"\"\"Run complete social metrics pipeline.\"\"\"\n",
        "        print(\"=\"*60)\n",
        "        print(\"COMPUTING SOCIAL MEDIA METRICS\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        # Step 1: Aggregate to daily\n",
        "        print(\"Aggregating to daily level...\")\n",
        "        daily = self.aggregate_daily(messages_df)\n",
        "        print(f\"  Daily observations: {len(daily):,}\")\n",
        "        \n",
        "        # Step 2: Rolling baselines\n",
        "        print(\"Computing rolling baselines...\")\n",
        "        daily = self.compute_rolling_baselines(daily)\n",
        "        \n",
        "        # Step 3: Flag bursts\n",
        "        print(\"Flagging social bursts...\")\n",
        "        daily = self.flag_social_bursts(daily, threshold=zscore_threshold)\n",
        "        \n",
        "        # Summary\n",
        "        burst_count = daily['is_social_burst'].sum()\n",
        "        print(f\"\\nSocial bursts detected: {burst_count}\")\n",
        "        print(f\"Burst rate: {100*burst_count/len(daily):.2f}%\")\n",
        "        \n",
        "        return daily\n",
        "\n",
        "\n",
        "# Initialize calculator\n",
        "social_calc = SocialMetricsCalculator(\n",
        "    window=config.ROLLING_WINDOW,\n",
        "    min_periods=config.MIN_PERIODS\n",
        ")\n",
        "print(\"Social Metrics Calculator initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# COMPUTE SOCIAL METRICS\n",
        "# =============================================================================\n",
        "\n",
        "# Process messages\n",
        "daily_social = social_calc.process_all(\n",
        "    messages_df, \n",
        "    zscore_threshold=config.SOCIAL_ZSCORE_THRESHOLD\n",
        ")\n",
        "\n",
        "print(\"\\nDaily Social Data Sample:\")\n",
        "print(daily_social.head(10))\n",
        "\n",
        "print(\"\\nSocial Metrics Summary:\")\n",
        "print(daily_social[['msg_count', 'msg_zscore', 'unique_users', \n",
        "                    'user_concentration', 'promo_share']].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# VISUALIZATIONS\n",
        "# =============================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "def plot_social_distributions(daily_df: pd.DataFrame):\n",
        "    \"\"\"Plot distributions of social metrics.\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    \n",
        "    # Message count distribution\n",
        "    ax1 = axes[0, 0]\n",
        "    data = daily_df['msg_count'].dropna()\n",
        "    data = data[data.between(0, data.quantile(0.99))]\n",
        "    ax1.hist(data, bins=50, edgecolor='black', alpha=0.7)\n",
        "    ax1.set_xlabel('Daily Message Count')\n",
        "    ax1.set_ylabel('Frequency')\n",
        "    ax1.set_title('Distribution of Daily Message Counts')\n",
        "    \n",
        "    # Message z-score distribution\n",
        "    ax2 = axes[0, 1]\n",
        "    data = daily_df['msg_zscore'].dropna()\n",
        "    data = data[data.between(-5, 10)]\n",
        "    ax2.hist(data, bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
        "    ax2.axvline(x=config.SOCIAL_ZSCORE_THRESHOLD, color='red', linestyle='--', \n",
        "                label=f'Threshold ({config.SOCIAL_ZSCORE_THRESHOLD})')\n",
        "    ax2.set_xlabel('Message Z-Score')\n",
        "    ax2.set_ylabel('Frequency')\n",
        "    ax2.set_title('Distribution of Message Z-Scores')\n",
        "    ax2.legend()\n",
        "    \n",
        "    # User concentration (Gini)\n",
        "    ax3 = axes[1, 0]\n",
        "    data = daily_df['user_concentration'].dropna()\n",
        "    ax3.hist(data, bins=50, edgecolor='black', alpha=0.7, color='green')\n",
        "    ax3.axvline(x=0.5, color='red', linestyle='--', label='Concentration threshold')\n",
        "    ax3.set_xlabel('User Concentration (Gini)')\n",
        "    ax3.set_ylabel('Frequency')\n",
        "    ax3.set_title('Distribution of User Concentration')\n",
        "    ax3.legend()\n",
        "    \n",
        "    # Promotional share\n",
        "    ax4 = axes[1, 1]\n",
        "    data = daily_df['promo_share'].dropna()\n",
        "    ax4.hist(data, bins=50, edgecolor='black', alpha=0.7, color='purple')\n",
        "    ax4.axvline(x=0.3, color='red', linestyle='--', label='High promo threshold')\n",
        "    ax4.set_xlabel('Promotional Share')\n",
        "    ax4.set_ylabel('Frequency')\n",
        "    ax4.set_title('Distribution of Promotional Content Share')\n",
        "    ax4.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(config.PROCESSED_DATA_PATH, 'social_distributions.png'), dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_ticker_social_activity(daily_df: pd.DataFrame, ticker: str = None):\n",
        "    \"\"\"Plot social activity for a specific ticker.\"\"\"\n",
        "    if ticker is None:\n",
        "        # Pick ticker with most bursts\n",
        "        burst_counts = daily_df.groupby('ticker')['is_social_burst'].sum()\n",
        "        if burst_counts.max() > 0:\n",
        "            ticker = burst_counts.idxmax()\n",
        "        else:\n",
        "            ticker = daily_df['ticker'].iloc[0]\n",
        "    \n",
        "    ticker_data = daily_df[daily_df['ticker'] == ticker].copy()\n",
        "    ticker_data['date'] = pd.to_datetime(ticker_data['date'])\n",
        "    ticker_data = ticker_data.sort_values('date')\n",
        "    \n",
        "    fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
        "    \n",
        "    # Message count\n",
        "    ax1 = axes[0]\n",
        "    ax1.bar(ticker_data['date'], ticker_data['msg_count'], alpha=0.5, color='blue', label='Messages')\n",
        "    ax1.plot(ticker_data['date'], ticker_data['msg_mean'], color='red', linewidth=2, label='60d Mean')\n",
        "    burst_days = ticker_data[ticker_data['is_social_burst']]\n",
        "    ax1.scatter(burst_days['date'], burst_days['msg_count'], color='red', s=100, marker='^', \n",
        "                label='Social Burst', zorder=5)\n",
        "    ax1.set_ylabel('Message Count')\n",
        "    ax1.set_title(f'{ticker} - Daily Message Activity')\n",
        "    ax1.legend()\n",
        "    \n",
        "    # Message z-score\n",
        "    ax2 = axes[1]\n",
        "    ax2.plot(ticker_data['date'], ticker_data['msg_zscore'], color='green', linewidth=1)\n",
        "    ax2.axhline(y=config.SOCIAL_ZSCORE_THRESHOLD, color='red', linestyle='--', \n",
        "                label=f'Threshold ({config.SOCIAL_ZSCORE_THRESHOLD})')\n",
        "    ax2.scatter(burst_days['date'], burst_days['msg_zscore'], color='red', s=100, marker='^', zorder=5)\n",
        "    ax2.set_ylabel('Message Z-Score')\n",
        "    ax2.set_title(f'{ticker} - Message Volume Z-Score')\n",
        "    ax2.legend()\n",
        "    \n",
        "    # Promotional share\n",
        "    ax3 = axes[2]\n",
        "    ax3.plot(ticker_data['date'], ticker_data['promo_share'], color='purple', linewidth=1)\n",
        "    ax3.axhline(y=0.3, color='red', linestyle='--', label='High promo threshold')\n",
        "    ax3.scatter(burst_days['date'], burst_days['promo_share'], color='red', s=100, marker='^', zorder=5)\n",
        "    ax3.set_ylabel('Promotional Share')\n",
        "    ax3.set_xlabel('Date')\n",
        "    ax3.set_title(f'{ticker} - Promotional Content Share')\n",
        "    ax3.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(config.PROCESSED_DATA_PATH, f'social_activity_{ticker}.png'), dpi=150)\n",
        "    plt.show()\n",
        "    \n",
        "    return ticker\n",
        "\n",
        "\n",
        "# Generate visualizations\n",
        "print(\"Generating social metrics visualizations...\")\n",
        "plot_social_distributions(daily_social)\n",
        "example_ticker = plot_ticker_social_activity(daily_social)\n",
        "print(f\"\\nExample ticker plotted: {example_ticker}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save Outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SAVE OUTPUTS\n",
        "# =============================================================================\n",
        "\n",
        "def save_social_data(messages_df: pd.DataFrame, \n",
        "                     daily_df: pd.DataFrame,\n",
        "                     output_dir: str):\n",
        "    \"\"\"Save social media data outputs.\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # Save raw messages (with hashed usernames)\n",
        "    messages_path = os.path.join(output_dir, 'yahoo_messages_raw.parquet')\n",
        "    messages_df.to_parquet(messages_path, index=False)\n",
        "    print(f\"Saved raw messages: {messages_path}\")\n",
        "    \n",
        "    # Save daily aggregates\n",
        "    daily_path = os.path.join(output_dir, 'daily_social_metrics.parquet')\n",
        "    daily_df.to_parquet(daily_path, index=False)\n",
        "    print(f\"Saved daily metrics: {daily_path}\")\n",
        "    \n",
        "    # Save burst events only\n",
        "    bursts = daily_df[daily_df['is_social_burst']].copy()\n",
        "    bursts_path = os.path.join(output_dir, 'social_burst_events.parquet')\n",
        "    bursts.to_parquet(bursts_path, index=False)\n",
        "    print(f\"Saved burst events: {bursts_path}\")\n",
        "    \n",
        "    # Save summary\n",
        "    summary = {\n",
        "        'total_messages': len(messages_df),\n",
        "        'daily_observations': len(daily_df),\n",
        "        'unique_tickers': int(messages_df['ticker'].nunique()),\n",
        "        'date_range': [str(daily_df['date'].min()), str(daily_df['date'].max())],\n",
        "        'social_bursts': int(daily_df['is_social_burst'].sum()),\n",
        "        'burst_rate': float(daily_df['is_social_burst'].mean()),\n",
        "        'tickers_with_bursts': int(daily_df[daily_df['is_social_burst']]['ticker'].nunique()),\n",
        "        'avg_messages_per_day': float(daily_df['msg_count'].mean()),\n",
        "        'avg_promo_share': float(daily_df['promo_share'].mean()),\n",
        "        'config': {\n",
        "            'rolling_window': config.ROLLING_WINDOW,\n",
        "            'zscore_threshold': config.SOCIAL_ZSCORE_THRESHOLD\n",
        "        },\n",
        "        'created_at': datetime.now().isoformat()\n",
        "    }\n",
        "    \n",
        "    summary_path = os.path.join(output_dir, 'notebook03_summary.json')\n",
        "    with open(summary_path, 'w') as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "    print(f\"Saved summary: {summary_path}\")\n",
        "    \n",
        "    return summary\n",
        "\n",
        "\n",
        "# Save outputs\n",
        "output_summary = save_social_data(\n",
        "    messages_df=messages_df,\n",
        "    daily_df=daily_social,\n",
        "    output_dir=config.PROCESSED_DATA_PATH\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Output Summary:\")\n",
        "print(json.dumps(output_summary, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary and Next Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# NOTEBOOK 3 SUMMARY\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\"\"\n",
        "\n",
        "          NOTEBOOK 3: SOCIAL MEDIA SCRAPING COMPLETE                          \n",
        "\n",
        "\n",
        "OUTPUT FILES:\n",
        "\n",
        " yahoo_messages_raw.parquet      - Raw message data (usernames hashed)\n",
        " daily_social_metrics.parquet    - Daily aggregated metrics\n",
        " social_burst_events.parquet     - Flagged social burst days\n",
        " social_distributions.png        - Distribution plots\n",
        " social_activity_{ticker}.png    - Example ticker activity\n",
        " notebook03_summary.json         - Summary statistics\n",
        "\n",
        "KEY METRICS COMPUTED:\n",
        "\n",
        " Daily message count per ticker\n",
        " Unique users per day\n",
        " User concentration (Gini coefficient)\n",
        " Promotional content share\n",
        " Rolling baselines (60-day mean, std)\n",
        " Message volume z-scores\n",
        " Social burst flags\n",
        "\n",
        "SOCIAL BURST CRITERIA:\n",
        "\n",
        " Message z-score > 3.0 (primary condition)\n",
        " Alternative flags: volume ratio > 3x, Gini > 0.5, promo share > 30%\n",
        "\n",
        "NEXT STEPS:\n",
        "\n",
        " Notebook 4: Episode Detection\n",
        "  - Merge price-volume and social data\n",
        "  - Identify joint events (price spike + social burst)\n",
        "  - Define episode windows\n",
        "  - Apply news filters\n",
        "\n",
        "IMPORTANT NOTES:\n",
        "\n",
        "1. Usernames are hashed for privacy - do not attempt to de-anonymize\n",
        "2. Scraping respects rate limits - be patient with large universes\n",
        "3. Social data may have survivorship bias (deleted posts not captured)\n",
        "4. Yahoo message boards have lower volume than Twitter/Reddit\n",
        "\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ENVIRONMENT INFO FOR REPRODUCIBILITY\n",
        "# =============================================================================\n",
        "\n",
        "import sys\n",
        "import platform\n",
        "\n",
        "print(\"Environment Information:\")\n",
        "print(f\"  Python: {sys.version}\")\n",
        "print(f\"  Platform: {platform.platform()}\")\n",
        "print(f\"  Pandas: {pd.__version__}\")\n",
        "print(f\"  NumPy: {np.__version__}\")\n",
        "print(f\"  Timestamp: {datetime.now().isoformat()}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

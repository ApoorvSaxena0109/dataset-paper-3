{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 4: Episode Detection\n",
        "## Social Media-Driven Stock Manipulation and Tail Risk Research\n",
        "\n",
        "---\n",
        "\n",
        "**Research Project:** Social Media-Driven Stock Manipulation and Tail Risk\n",
        "\n",
        "**Purpose:** Identify pump-and-dump episodes by combining price-volume anomalies with social media bursts. Apply filters to reduce false positives.\n",
        "\n",
        "**Inputs:**\n",
        "- Market data with baselines (Notebook 2)\n",
        "- Social media metrics (Notebook 3)\n",
        "- SEC enforcement labels (Notebook 1)\n",
        "\n",
        "**Output:**\n",
        "- Episode-level dataset with event windows\n",
        "- Ground truth labels\n",
        "- Placebo test results\n",
        "\n",
        "---\n",
        "\n",
        "**Last Updated:** 2025"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# INSTALL REQUIRED PACKAGES\n",
        "# =============================================================================\n",
        "\n",
        "!pip install pandas==2.0.3\n",
        "!pip install numpy==1.24.3\n",
        "!pip install scipy==1.11.4\n",
        "!pip install tqdm==4.66.1\n",
        "!pip install pyarrow==14.0.1\n",
        "!pip install matplotlib==3.8.2\n",
        "!pip install seaborn==0.13.0\n",
        "\n",
        "print(\"All packages installed successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# IMPORT LIBRARIES\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', 100)\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette('husl')\n",
        "\n",
        "print(f\"Environment setup complete. Timestamp: {datetime.now()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration and Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "class ResearchConfig:\n",
        "    \"\"\"Configuration for episode detection.\"\"\"\n",
        "    \n",
        "    # Episode Detection Thresholds\n",
        "    RETURN_ZSCORE_THRESHOLD = 3.0\n",
        "    SOCIAL_ZSCORE_THRESHOLD = 3.0\n",
        "    SOCIAL_WINDOW = 1  # days before/after for social burst matching\n",
        "    \n",
        "    # Episode Windows (trading days)\n",
        "    PRE_WINDOW = 20    # days before event\n",
        "    POST_SHORT = 5     # immediate post-event\n",
        "    POST_MEDIUM = 20   # medium-term post-event\n",
        "    POST_LONG = 60     # long-term post-event\n",
        "    \n",
        "    # Filtering\n",
        "    MIN_CLUSTER_GAP = 5  # days between distinct episodes\n",
        "    \n",
        "    # Data Paths\n",
        "    BASE_PATH = \"/content/drive/MyDrive/Research/PumpDump/\"\n",
        "    PROCESSED_DATA_PATH = BASE_PATH + \"data/processed/\"\n",
        "    RESULTS_PATH = BASE_PATH + \"results/\"\n",
        "\n",
        "config = ResearchConfig()\n",
        "\n",
        "# Handle Colab vs local\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    config.BASE_PATH = \"./research_data/\"\n",
        "    config.PROCESSED_DATA_PATH = config.BASE_PATH + \"data/processed/\"\n",
        "    config.RESULTS_PATH = config.BASE_PATH + \"results/\"\n",
        "\n",
        "os.makedirs(config.RESULTS_PATH, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# LOAD DATA FROM PREVIOUS NOTEBOOKS\n",
        "# =============================================================================\n",
        "\n",
        "def load_data(data_path: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"Load data from previous notebooks.\"\"\"\n",
        "    \n",
        "    # Market data with baselines (Notebook 2)\n",
        "    market_path = os.path.join(data_path, 'market_data_with_baselines.parquet')\n",
        "    if os.path.exists(market_path):\n",
        "        market_data = pd.read_parquet(market_path)\n",
        "        print(f\"Loaded market data: {len(market_data):,} rows\")\n",
        "    else:\n",
        "        print(\"Market data not found - creating sample\")\n",
        "        market_data = create_sample_market_data()\n",
        "    \n",
        "    # Social data (Notebook 3)\n",
        "    social_path = os.path.join(data_path, 'daily_social_metrics.parquet')\n",
        "    if os.path.exists(social_path):\n",
        "        social_data = pd.read_parquet(social_path)\n",
        "        print(f\"Loaded social data: {len(social_data):,} rows\")\n",
        "    else:\n",
        "        print(\"Social data not found - creating sample\")\n",
        "        social_data = create_sample_social_data()\n",
        "    \n",
        "    # SEC labels (Notebook 1)\n",
        "    labels_path = os.path.join(data_path, 'ticker_manipulation_labels.parquet')\n",
        "    if os.path.exists(labels_path):\n",
        "        labels = pd.read_parquet(labels_path)\n",
        "        print(f\"Loaded labels: {len(labels)} tickers\")\n",
        "    else:\n",
        "        print(\"Labels not found - using empty DataFrame\")\n",
        "        labels = pd.DataFrame(columns=['ticker', 'enforcement_date', 'label'])\n",
        "    \n",
        "    return market_data, social_data, labels\n",
        "\n",
        "\n",
        "def create_sample_market_data() -> pd.DataFrame:\n",
        "    \"\"\"Create sample market data for demonstration.\"\"\"\n",
        "    np.random.seed(42)\n",
        "    tickers = ['GME', 'AMC', 'BB', 'NOK', 'CLOV', 'WISH', 'MULN', 'FFIE']\n",
        "    dates = pd.date_range('2020-01-01', '2023-12-31', freq='B')\n",
        "    \n",
        "    records = []\n",
        "    for ticker in tickers:\n",
        "        for date in dates:\n",
        "            close = np.random.lognormal(1, 0.5)\n",
        "            volume = np.random.lognormal(15, 1)\n",
        "            ret = np.random.normal(0, 0.03)\n",
        "            \n",
        "            # Add some spikes\n",
        "            if np.random.random() < 0.01:\n",
        "                ret = np.random.uniform(0.1, 0.5)\n",
        "                volume *= np.random.uniform(5, 20)\n",
        "            \n",
        "            records.append({\n",
        "                'ticker': ticker,\n",
        "                'date': date.date(),\n",
        "                'close': close,\n",
        "                'volume': volume,\n",
        "                'return': ret,\n",
        "                'return_zscore': ret / 0.03,\n",
        "                'volume_zscore': (volume - 1000000) / 500000,\n",
        "                'is_candidate_event': ret > 0.1 and volume > 5000000,\n",
        "                'prev_close': close * (1 - ret)\n",
        "            })\n",
        "    \n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "\n",
        "def create_sample_social_data() -> pd.DataFrame:\n",
        "    \"\"\"Create sample social data for demonstration.\"\"\"\n",
        "    np.random.seed(42)\n",
        "    tickers = ['GME', 'AMC', 'BB', 'NOK', 'CLOV', 'WISH', 'MULN', 'FFIE']\n",
        "    dates = pd.date_range('2020-01-01', '2023-12-31', freq='B')\n",
        "    \n",
        "    records = []\n",
        "    for ticker in tickers:\n",
        "        for date in dates:\n",
        "            msg_count = np.random.poisson(10)\n",
        "            \n",
        "            # Add some bursts\n",
        "            if np.random.random() < 0.02:\n",
        "                msg_count *= np.random.randint(5, 20)\n",
        "            \n",
        "            records.append({\n",
        "                'ticker': ticker,\n",
        "                'date': date.date(),\n",
        "                'msg_count': msg_count,\n",
        "                'msg_zscore': (msg_count - 10) / 5,\n",
        "                'promo_share': np.random.uniform(0, 0.5),\n",
        "                'user_concentration': np.random.uniform(0, 1),\n",
        "                'is_social_burst': msg_count > 50\n",
        "            })\n",
        "    \n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "\n",
        "# Load data\n",
        "market_data, social_data, sec_labels = load_data(config.PROCESSED_DATA_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Merge Price-Volume and Social Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DATA MERGER\n",
        "# =============================================================================\n",
        "\n",
        "class DataMerger:\n",
        "    \"\"\"Merges market and social data for joint event detection.\"\"\"\n",
        "    \n",
        "    def __init__(self, config: ResearchConfig):\n",
        "        self.config = config\n",
        "    \n",
        "    def merge_market_social(self, market_df: pd.DataFrame, \n",
        "                            social_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Merge market and social data on ticker and date.\"\"\"\n",
        "        \n",
        "        # Ensure date columns are the same type\n",
        "        market_df = market_df.copy()\n",
        "        social_df = social_df.copy()\n",
        "        \n",
        "        market_df['date'] = pd.to_datetime(market_df['date']).dt.date\n",
        "        social_df['date'] = pd.to_datetime(social_df['date']).dt.date\n",
        "        \n",
        "        # Select columns to merge\n",
        "        social_cols = ['ticker', 'date', 'msg_count', 'msg_zscore', \n",
        "                       'promo_share', 'user_concentration', 'is_social_burst']\n",
        "        social_cols = [c for c in social_cols if c in social_df.columns]\n",
        "        \n",
        "        # Merge\n",
        "        merged = market_df.merge(\n",
        "            social_df[social_cols],\n",
        "            on=['ticker', 'date'],\n",
        "            how='left'\n",
        "        )\n",
        "        \n",
        "        # Fill missing social data (no messages = 0)\n",
        "        merged['msg_count'] = merged['msg_count'].fillna(0)\n",
        "        merged['msg_zscore'] = merged['msg_zscore'].fillna(0)\n",
        "        merged['is_social_burst'] = merged['is_social_burst'].fillna(False)\n",
        "        \n",
        "        print(f\"Merged data: {len(merged):,} rows\")\n",
        "        print(f\"Rows with social data: {(merged['msg_count'] > 0).sum():,}\")\n",
        "        \n",
        "        return merged\n",
        "    \n",
        "    def add_social_burst_window(self, df: pd.DataFrame, \n",
        "                                 window: int = 1) -> pd.DataFrame:\n",
        "        \"\"\"Add flag for social burst within window of each day.\n",
        "        \n",
        "        For each day, check if there's a social burst within [-window, +window] days.\n",
        "        \"\"\"\n",
        "        df = df.copy()\n",
        "        df = df.sort_values(['ticker', 'date'])\n",
        "        \n",
        "        # For each ticker, check rolling window for social bursts\n",
        "        def check_burst_window(group):\n",
        "            burst = group['is_social_burst'].astype(int)\n",
        "            # Forward and backward rolling sum\n",
        "            forward = burst.rolling(window + 1, min_periods=1).sum().shift(-window)\n",
        "            backward = burst.rolling(window + 1, min_periods=1).sum()\n",
        "            return (forward + backward - burst) > 0  # Subtract self to avoid double counting\n",
        "        \n",
        "        df['social_burst_in_window'] = df.groupby('ticker').apply(\n",
        "            lambda x: check_burst_window(x)\n",
        "        ).reset_index(level=0, drop=True)\n",
        "        \n",
        "        return df\n",
        "\n",
        "\n",
        "# Initialize merger\n",
        "merger = DataMerger(config)\n",
        "\n",
        "# Merge data\n",
        "print(\"Merging market and social data...\")\n",
        "merged_data = merger.merge_market_social(market_data, social_data)\n",
        "merged_data = merger.add_social_burst_window(merged_data, window=config.SOCIAL_WINDOW)\n",
        "\n",
        "print(\"\\nMerged data sample:\")\n",
        "print(merged_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Joint Event Detection\n",
        "\n",
        "### 4.1 Define Episode Criteria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# EPISODE DETECTOR\n",
        "# =============================================================================\n",
        "\n",
        "class EpisodeDetector:\n",
        "    \"\"\"Detects pump-and-dump episodes from merged data.\n",
        "    \n",
        "    An episode is defined as:\n",
        "    1. Price spike (return z-score > threshold) AND\n",
        "    2. Volume spike (volume > 95th percentile) AND\n",
        "    3. Social burst within +-1 day\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config: ResearchConfig):\n",
        "        self.config = config\n",
        "    \n",
        "    def identify_episodes(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Identify joint price-volume-social episodes.\"\"\"\n",
        "        df = df.copy()\n",
        "        \n",
        "        # Condition 1: Price spike (from candidate_event flag or z-score)\n",
        "        if 'is_candidate_event' in df.columns:\n",
        "            price_spike = df['is_candidate_event']\n",
        "        else:\n",
        "            price_spike = df['return_zscore'] > self.config.RETURN_ZSCORE_THRESHOLD\n",
        "        \n",
        "        # Condition 2: Social burst in window\n",
        "        if 'social_burst_in_window' in df.columns:\n",
        "            social_burst = df['social_burst_in_window']\n",
        "        else:\n",
        "            social_burst = df['is_social_burst']\n",
        "        \n",
        "        # Joint event\n",
        "        df['is_episode'] = price_spike & social_burst\n",
        "        \n",
        "        # Also flag without social requirement (for comparison)\n",
        "        df['is_price_only_event'] = price_spike\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def cluster_episodes(self, df: pd.DataFrame, \n",
        "                          min_gap: int = 5) -> pd.DataFrame:\n",
        "        \"\"\"Cluster consecutive episode days into single episodes.\n",
        "        \n",
        "        If multiple qualifying days occur within min_gap days,\n",
        "        treat as single episode (take first day).\n",
        "        \"\"\"\n",
        "        df = df.copy()\n",
        "        df = df.sort_values(['ticker', 'date'])\n",
        "        \n",
        "        def assign_cluster(group):\n",
        "            group = group.copy()\n",
        "            group['episode_cluster'] = np.nan\n",
        "            \n",
        "            episode_days = group[group['is_episode']].index.tolist()\n",
        "            \n",
        "            cluster_id = 0\n",
        "            last_episode_idx = None\n",
        "            \n",
        "            for idx in episode_days:\n",
        "                if last_episode_idx is None:\n",
        "                    cluster_id += 1\n",
        "                    group.loc[idx, 'episode_cluster'] = cluster_id\n",
        "                else:\n",
        "                    # Check gap\n",
        "                    days_since_last = (group.loc[idx, 'date'] - group.loc[last_episode_idx, 'date']).days\n",
        "                    if days_since_last > min_gap:\n",
        "                        cluster_id += 1\n",
        "                    group.loc[idx, 'episode_cluster'] = cluster_id\n",
        "                \n",
        "                last_episode_idx = idx\n",
        "            \n",
        "            return group\n",
        "        \n",
        "        # Apply clustering\n",
        "        df['date'] = pd.to_datetime(df['date'])\n",
        "        df = df.groupby('ticker').apply(assign_cluster).reset_index(drop=True)\n",
        "        \n",
        "        # Mark first day of each cluster as the episode event day\n",
        "        df['is_episode_start'] = False\n",
        "        for ticker in df['ticker'].unique():\n",
        "            ticker_mask = df['ticker'] == ticker\n",
        "            clusters = df[ticker_mask & df['is_episode']].groupby('episode_cluster')\n",
        "            for _, cluster in clusters:\n",
        "                first_idx = cluster.index[0]\n",
        "                df.loc[first_idx, 'is_episode_start'] = True\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def extract_episodes(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Extract episode-level DataFrame.\"\"\"\n",
        "        episodes = df[df['is_episode_start']].copy()\n",
        "        \n",
        "        # Rename date to event_date\n",
        "        episodes = episodes.rename(columns={'date': 'event_date'})\n",
        "        \n",
        "        # Select relevant columns\n",
        "        cols = ['ticker', 'event_date', 'close', 'volume', 'return', 'return_zscore',\n",
        "                'msg_count', 'msg_zscore', 'promo_share', 'user_concentration',\n",
        "                'episode_cluster']\n",
        "        cols = [c for c in cols if c in episodes.columns]\n",
        "        \n",
        "        episodes = episodes[cols].reset_index(drop=True)\n",
        "        \n",
        "        # Add episode ID\n",
        "        episodes['episode_id'] = range(1, len(episodes) + 1)\n",
        "        \n",
        "        return episodes\n",
        "    \n",
        "    def summarize_detection(self, df: pd.DataFrame, episodes: pd.DataFrame) -> Dict:\n",
        "        \"\"\"Generate detection summary statistics.\"\"\"\n",
        "        summary = {\n",
        "            'total_observations': len(df),\n",
        "            'unique_tickers': int(df['ticker'].nunique()),\n",
        "            'price_only_events': int(df['is_price_only_event'].sum()),\n",
        "            'joint_events': int(df['is_episode'].sum()),\n",
        "            'distinct_episodes': len(episodes),\n",
        "            'tickers_with_episodes': int(episodes['ticker'].nunique()),\n",
        "            'episodes_per_ticker': episodes.groupby('ticker').size().describe().to_dict()\n",
        "        }\n",
        "        \n",
        "        return summary\n",
        "\n",
        "\n",
        "# Initialize detector\n",
        "detector = EpisodeDetector(config)\n",
        "print(\"Episode Detector initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DETECT EPISODES\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Identifying episodes...\")\n",
        "\n",
        "# Identify joint events\n",
        "merged_data = detector.identify_episodes(merged_data)\n",
        "\n",
        "# Cluster consecutive episodes\n",
        "merged_data = detector.cluster_episodes(merged_data, min_gap=config.MIN_CLUSTER_GAP)\n",
        "\n",
        "# Extract episode-level data\n",
        "episodes_df = detector.extract_episodes(merged_data)\n",
        "\n",
        "# Summary\n",
        "detection_summary = detector.summarize_detection(merged_data, episodes_df)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EPISODE DETECTION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total observations: {detection_summary['total_observations']:,}\")\n",
        "print(f\"Price-only events: {detection_summary['price_only_events']:,}\")\n",
        "print(f\"Joint events (price + social): {detection_summary['joint_events']:,}\")\n",
        "print(f\"Distinct episodes (after clustering): {detection_summary['distinct_episodes']}\")\n",
        "print(f\"Tickers with episodes: {detection_summary['tickers_with_episodes']}\")\n",
        "\n",
        "print(\"\\nEpisodes DataFrame:\")\n",
        "print(episodes_df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Add Ground Truth Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ADD SEC ENFORCEMENT LABELS\n",
        "# =============================================================================\n",
        "\n",
        "class LabelAssigner:\n",
        "    \"\"\"Assigns ground truth labels to episodes.\n",
        "    \n",
        "    Labels:\n",
        "    - 1 = Confirmed pump (ticker in SEC enforcement)\n",
        "    - 0 = Control (high-volatility but no enforcement)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, labels_df: pd.DataFrame):\n",
        "        self.labels = labels_df\n",
        "        self.confirmed_tickers = set(labels_df['ticker'].unique()) if len(labels_df) > 0 else set()\n",
        "    \n",
        "    def assign_labels(self, episodes_df: pd.DataFrame, \n",
        "                      lookback_days: int = 365) -> pd.DataFrame:\n",
        "        \"\"\"Assign labels based on SEC enforcement.\n",
        "        \n",
        "        An episode is labeled as confirmed pump if:\n",
        "        - Ticker appears in SEC enforcement AND\n",
        "        - Episode date is within lookback_days before enforcement date\n",
        "        \"\"\"\n",
        "        episodes = episodes_df.copy()\n",
        "        episodes['label'] = 0  # Default: control\n",
        "        episodes['enforcement_date'] = None\n",
        "        episodes['enforcement_release'] = None\n",
        "        \n",
        "        if len(self.labels) == 0:\n",
        "            print(\"No enforcement labels available - all episodes labeled as control\")\n",
        "            return episodes\n",
        "        \n",
        "        # Convert dates\n",
        "        episodes['event_date'] = pd.to_datetime(episodes['event_date'])\n",
        "        self.labels['enforcement_date'] = pd.to_datetime(self.labels['enforcement_date'])\n",
        "        \n",
        "        # Match episodes to enforcement\n",
        "        for idx, episode in episodes.iterrows():\n",
        "            ticker = episode['ticker']\n",
        "            event_date = episode['event_date']\n",
        "            \n",
        "            # Check if ticker is in enforcement\n",
        "            ticker_enforcement = self.labels[self.labels['ticker'] == ticker]\n",
        "            \n",
        "            if len(ticker_enforcement) > 0:\n",
        "                # Check if episode is within lookback window before enforcement\n",
        "                for _, enf_row in ticker_enforcement.iterrows():\n",
        "                    enf_date = enf_row['enforcement_date']\n",
        "                    days_before = (enf_date - event_date).days\n",
        "                    \n",
        "                    if 0 <= days_before <= lookback_days:\n",
        "                        episodes.loc[idx, 'label'] = 1\n",
        "                        episodes.loc[idx, 'enforcement_date'] = enf_date\n",
        "                        if 'release_number' in enf_row:\n",
        "                            episodes.loc[idx, 'enforcement_release'] = enf_row['release_number']\n",
        "                        break\n",
        "        \n",
        "        # Summary\n",
        "        confirmed = (episodes['label'] == 1).sum()\n",
        "        control = (episodes['label'] == 0).sum()\n",
        "        \n",
        "        print(f\"Label Assignment:\")\n",
        "        print(f\"  Confirmed pump (label=1): {confirmed}\")\n",
        "        print(f\"  Control (label=0): {control}\")\n",
        "        \n",
        "        return episodes\n",
        "\n",
        "\n",
        "# Assign labels\n",
        "label_assigner = LabelAssigner(sec_labels)\n",
        "episodes_df = label_assigner.assign_labels(episodes_df)\n",
        "\n",
        "print(\"\\nLabeled Episodes Sample:\")\n",
        "print(episodes_df[['episode_id', 'ticker', 'event_date', 'label', 'enforcement_date']].head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Define Episode Windows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# EPISODE WINDOW CONSTRUCTOR\n",
        "# =============================================================================\n",
        "\n",
        "class WindowConstructor:\n",
        "    \"\"\"Constructs event windows for each episode.\n",
        "    \n",
        "    Windows:\n",
        "    - Pre-episode: t0-20 to t0-1 (baseline)\n",
        "    - Event: t0 (spike day)\n",
        "    - Post-short: t0+1 to t0+5 (immediate reversal)\n",
        "    - Post-medium: t0+1 to t0+20 (full reversal)\n",
        "    - Post-long: t0+1 to t0+60 (extended)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config: ResearchConfig):\n",
        "        self.config = config\n",
        "    \n",
        "    def extract_window_data(self, daily_df: pd.DataFrame, \n",
        "                            ticker: str, \n",
        "                            event_date: datetime,\n",
        "                            pre_days: int,\n",
        "                            post_days: int) -> pd.DataFrame:\n",
        "        \"\"\"Extract data for a specific window around an event.\"\"\"\n",
        "        \n",
        "        event_date = pd.to_datetime(event_date)\n",
        "        daily_df['date'] = pd.to_datetime(daily_df['date'])\n",
        "        \n",
        "        # Filter to ticker\n",
        "        ticker_data = daily_df[daily_df['ticker'] == ticker].copy()\n",
        "        ticker_data = ticker_data.sort_values('date')\n",
        "        \n",
        "        # Find event index\n",
        "        event_idx = ticker_data[ticker_data['date'] == event_date].index\n",
        "        if len(event_idx) == 0:\n",
        "            return pd.DataFrame()\n",
        "        \n",
        "        event_idx = event_idx[0]\n",
        "        ticker_idx = ticker_data.index.tolist()\n",
        "        event_pos = ticker_idx.index(event_idx)\n",
        "        \n",
        "        # Get window indices\n",
        "        start_pos = max(0, event_pos - pre_days)\n",
        "        end_pos = min(len(ticker_idx) - 1, event_pos + post_days)\n",
        "        \n",
        "        window_indices = ticker_idx[start_pos:end_pos + 1]\n",
        "        \n",
        "        window_data = ticker_data.loc[window_indices].copy()\n",
        "        window_data['days_from_event'] = range(-event_pos + start_pos, end_pos - event_pos + 1)\n",
        "        \n",
        "        return window_data\n",
        "    \n",
        "    def compute_window_metrics(self, daily_df: pd.DataFrame,\n",
        "                                episodes_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Compute metrics for each episode window.\"\"\"\n",
        "        \n",
        "        episodes = episodes_df.copy()\n",
        "        \n",
        "        # Initialize metric columns\n",
        "        metrics_cols = [\n",
        "            # Event day metrics\n",
        "            'event_return', 'event_volume_ratio',\n",
        "            \n",
        "            # Reversal metrics\n",
        "            'return_5d', 'return_20d', 'return_60d',\n",
        "            \n",
        "            # Drawdown metrics\n",
        "            'max_drawdown_5d', 'max_drawdown_20d', 'max_drawdown_60d',\n",
        "            \n",
        "            # Pre-event baseline\n",
        "            'pre_avg_return', 'pre_avg_volume'\n",
        "        ]\n",
        "        \n",
        "        for col in metrics_cols:\n",
        "            episodes[col] = np.nan\n",
        "        \n",
        "        print(f\"Computing window metrics for {len(episodes)} episodes...\")\n",
        "        \n",
        "        for idx, episode in tqdm(episodes.iterrows(), total=len(episodes)):\n",
        "            ticker = episode['ticker']\n",
        "            event_date = episode['event_date']\n",
        "            \n",
        "            # Get full window data\n",
        "            window = self.extract_window_data(\n",
        "                daily_df, ticker, event_date,\n",
        "                pre_days=self.config.PRE_WINDOW,\n",
        "                post_days=self.config.POST_LONG\n",
        "            )\n",
        "            \n",
        "            if len(window) == 0:\n",
        "                continue\n",
        "            \n",
        "            # Event day metrics\n",
        "            event_row = window[window['days_from_event'] == 0]\n",
        "            if len(event_row) > 0:\n",
        "                episodes.loc[idx, 'event_return'] = event_row['return'].iloc[0]\n",
        "                if 'volume_ratio' in event_row.columns:\n",
        "                    episodes.loc[idx, 'event_volume_ratio'] = event_row['volume_ratio'].iloc[0]\n",
        "            \n",
        "            # Pre-event baseline\n",
        "            pre_window = window[window['days_from_event'] < 0]\n",
        "            if len(pre_window) > 5:\n",
        "                episodes.loc[idx, 'pre_avg_return'] = pre_window['return'].mean()\n",
        "                episodes.loc[idx, 'pre_avg_volume'] = pre_window['volume'].mean()\n",
        "            \n",
        "            # Post-event returns and drawdowns\n",
        "            post_window = window[window['days_from_event'] > 0]\n",
        "            \n",
        "            if len(post_window) > 0:\n",
        "                event_close = event_row['close'].iloc[0] if len(event_row) > 0 else np.nan\n",
        "                \n",
        "                for days, suffix in [(5, '5d'), (20, '20d'), (60, '60d')]:\n",
        "                    period = post_window[post_window['days_from_event'] <= days]\n",
        "                    \n",
        "                    if len(period) > 0 and not np.isnan(event_close):\n",
        "                        # Cumulative return\n",
        "                        final_close = period['close'].iloc[-1]\n",
        "                        cum_return = (final_close / event_close) - 1\n",
        "                        episodes.loc[idx, f'return_{suffix}'] = cum_return\n",
        "                        \n",
        "                        # Max drawdown\n",
        "                        prices = period['close'].values\n",
        "                        running_max = np.maximum.accumulate(np.concatenate([[event_close], prices]))\n",
        "                        drawdowns = (running_max - np.concatenate([[event_close], prices])) / running_max\n",
        "                        episodes.loc[idx, f'max_drawdown_{suffix}'] = drawdowns.max()\n",
        "        \n",
        "        return episodes\n",
        "\n",
        "\n",
        "# Construct windows\n",
        "window_constructor = WindowConstructor(config)\n",
        "episodes_df = window_constructor.compute_window_metrics(merged_data, episodes_df)\n",
        "\n",
        "print(\"\\nEpisodes with Window Metrics:\")\n",
        "print(episodes_df[['episode_id', 'ticker', 'event_return', \n",
        "                   'return_5d', 'return_20d', 'max_drawdown_20d']].head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Placebo Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PLACEBO TESTS\n",
        "# =============================================================================\n",
        "\n",
        "class PlaceboTester:\n",
        "    \"\"\"Runs placebo tests to validate episode detection.\n",
        "    \n",
        "    Tests:\n",
        "    1. Large-cap placebo: Apply filter to S&P 500 stocks\n",
        "    2. Time shuffle: Randomly permute social data dates\n",
        "    3. Random universe: Apply to random stocks\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config: ResearchConfig):\n",
        "        self.config = config\n",
        "    \n",
        "    def time_shuffle_test(self, merged_df: pd.DataFrame, \n",
        "                          n_iterations: int = 100) -> Dict:\n",
        "        \"\"\"Test detection rate with shuffled social data.\n",
        "        \n",
        "        If our detection is meaningful, shuffling social dates\n",
        "        should dramatically reduce joint event detection.\n",
        "        \"\"\"\n",
        "        np.random.seed(42)\n",
        "        \n",
        "        df = merged_df.copy()\n",
        "        original_rate = df['is_episode'].mean()\n",
        "        \n",
        "        shuffled_rates = []\n",
        "        \n",
        "        print(f\"Running time shuffle test ({n_iterations} iterations)...\")\n",
        "        \n",
        "        for _ in tqdm(range(n_iterations)):\n",
        "            # Shuffle social burst flags within each ticker\n",
        "            df_shuffled = df.copy()\n",
        "            \n",
        "            for ticker in df_shuffled['ticker'].unique():\n",
        "                ticker_mask = df_shuffled['ticker'] == ticker\n",
        "                social_flags = df_shuffled.loc[ticker_mask, 'is_social_burst'].values.copy()\n",
        "                np.random.shuffle(social_flags)\n",
        "                df_shuffled.loc[ticker_mask, 'is_social_burst'] = social_flags\n",
        "            \n",
        "            # Recalculate episodes\n",
        "            if 'is_candidate_event' in df_shuffled.columns:\n",
        "                price_spike = df_shuffled['is_candidate_event']\n",
        "            else:\n",
        "                price_spike = df_shuffled['return_zscore'] > self.config.RETURN_ZSCORE_THRESHOLD\n",
        "            \n",
        "            shuffled_episodes = price_spike & df_shuffled['is_social_burst']\n",
        "            shuffled_rates.append(shuffled_episodes.mean())\n",
        "        \n",
        "        results = {\n",
        "            'original_rate': original_rate,\n",
        "            'shuffled_mean': np.mean(shuffled_rates),\n",
        "            'shuffled_std': np.std(shuffled_rates),\n",
        "            'z_score': (original_rate - np.mean(shuffled_rates)) / np.std(shuffled_rates) if np.std(shuffled_rates) > 0 else np.inf,\n",
        "            'ratio': original_rate / np.mean(shuffled_rates) if np.mean(shuffled_rates) > 0 else np.inf\n",
        "        }\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def print_placebo_results(self, results: Dict):\n",
        "        \"\"\"Print formatted placebo test results.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"PLACEBO TEST RESULTS\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Original episode rate: {results['original_rate']:.6f}\")\n",
        "        print(f\"Shuffled mean rate: {results['shuffled_mean']:.6f}\")\n",
        "        print(f\"Shuffled std: {results['shuffled_std']:.6f}\")\n",
        "        print(f\"Z-score: {results['z_score']:.2f}\")\n",
        "        print(f\"Original/Shuffled ratio: {results['ratio']:.2f}x\")\n",
        "        \n",
        "        if results['z_score'] > 3:\n",
        "            print(\"\\n*** PASS: Original rate significantly higher than random ***\")\n",
        "        elif results['z_score'] > 2:\n",
        "            print(\"\\n** MARGINAL: Original rate moderately higher than random **\")\n",
        "        else:\n",
        "            print(\"\\n* WARNING: Original rate not significantly different from random *\")\n",
        "\n",
        "\n",
        "# Run placebo tests\n",
        "placebo_tester = PlaceboTester(config)\n",
        "placebo_results = placebo_tester.time_shuffle_test(merged_data, n_iterations=100)\n",
        "placebo_tester.print_placebo_results(placebo_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# VISUALIZATIONS\n",
        "# =============================================================================\n",
        "\n",
        "def plot_episode_characteristics(episodes_df: pd.DataFrame):\n",
        "    \"\"\"Plot characteristics of detected episodes.\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    \n",
        "    # Event returns distribution\n",
        "    ax1 = axes[0, 0]\n",
        "    data = episodes_df['event_return'].dropna() * 100\n",
        "    ax1.hist(data, bins=30, edgecolor='black', alpha=0.7)\n",
        "    ax1.axvline(x=data.median(), color='red', linestyle='--', label=f'Median: {data.median():.1f}%')\n",
        "    ax1.set_xlabel('Event Day Return (%)')\n",
        "    ax1.set_ylabel('Frequency')\n",
        "    ax1.set_title('Distribution of Event Day Returns')\n",
        "    ax1.legend()\n",
        "    \n",
        "    # Post-event reversals\n",
        "    ax2 = axes[0, 1]\n",
        "    for col, label, color in [('return_5d', '5-Day', 'blue'), \n",
        "                               ('return_20d', '20-Day', 'orange'),\n",
        "                               ('return_60d', '60-Day', 'green')]:\n",
        "        if col in episodes_df.columns:\n",
        "            data = episodes_df[col].dropna() * 100\n",
        "            ax2.hist(data, bins=30, alpha=0.5, label=f'{label} (med: {data.median():.1f}%)', color=color)\n",
        "    ax2.axvline(x=0, color='black', linestyle='-', linewidth=2)\n",
        "    ax2.set_xlabel('Post-Event Return (%)')\n",
        "    ax2.set_ylabel('Frequency')\n",
        "    ax2.set_title('Post-Event Return Distributions')\n",
        "    ax2.legend()\n",
        "    \n",
        "    # Max drawdowns\n",
        "    ax3 = axes[1, 0]\n",
        "    for col, label, color in [('max_drawdown_5d', '5-Day', 'blue'),\n",
        "                               ('max_drawdown_20d', '20-Day', 'orange'),\n",
        "                               ('max_drawdown_60d', '60-Day', 'green')]:\n",
        "        if col in episodes_df.columns:\n",
        "            data = episodes_df[col].dropna() * 100\n",
        "            ax3.hist(data, bins=30, alpha=0.5, label=f'{label} (med: {data.median():.1f}%)', color=color)\n",
        "    ax3.set_xlabel('Maximum Drawdown (%)')\n",
        "    ax3.set_ylabel('Frequency')\n",
        "    ax3.set_title('Post-Event Maximum Drawdown Distributions')\n",
        "    ax3.legend()\n",
        "    \n",
        "    # Episodes over time\n",
        "    ax4 = axes[1, 1]\n",
        "    episodes_df['event_date'] = pd.to_datetime(episodes_df['event_date'])\n",
        "    monthly = episodes_df.groupby(episodes_df['event_date'].dt.to_period('M')).size()\n",
        "    monthly.plot(ax=ax4, kind='bar', color='purple', alpha=0.7)\n",
        "    ax4.set_xlabel('Month')\n",
        "    ax4.set_ylabel('Episode Count')\n",
        "    ax4.set_title('Episodes Over Time')\n",
        "    ax4.tick_params(axis='x', rotation=45)\n",
        "    for i, label in enumerate(ax4.xaxis.get_ticklabels()):\n",
        "        if i % 6 != 0:\n",
        "            label.set_visible(False)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(config.RESULTS_PATH, 'episode_characteristics.png'), dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_confirmed_vs_control(episodes_df: pd.DataFrame):\n",
        "    \"\"\"Compare confirmed pumps vs control episodes.\"\"\"\n",
        "    if 'label' not in episodes_df.columns or episodes_df['label'].sum() == 0:\n",
        "        print(\"No confirmed pump labels available for comparison\")\n",
        "        return\n",
        "    \n",
        "    confirmed = episodes_df[episodes_df['label'] == 1]\n",
        "    control = episodes_df[episodes_df['label'] == 0]\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    \n",
        "    # Event returns\n",
        "    ax1 = axes[0]\n",
        "    ax1.hist(confirmed['event_return'].dropna()*100, bins=20, alpha=0.5, label='Confirmed', color='red')\n",
        "    ax1.hist(control['event_return'].dropna()*100, bins=20, alpha=0.5, label='Control', color='blue')\n",
        "    ax1.set_xlabel('Event Return (%)')\n",
        "    ax1.set_title('Event Day Returns')\n",
        "    ax1.legend()\n",
        "    \n",
        "    # 20-day reversal\n",
        "    ax2 = axes[1]\n",
        "    ax2.hist(confirmed['return_20d'].dropna()*100, bins=20, alpha=0.5, label='Confirmed', color='red')\n",
        "    ax2.hist(control['return_20d'].dropna()*100, bins=20, alpha=0.5, label='Control', color='blue')\n",
        "    ax2.axvline(x=0, color='black', linestyle='--')\n",
        "    ax2.set_xlabel('20-Day Return (%)')\n",
        "    ax2.set_title('20-Day Post-Event Returns')\n",
        "    ax2.legend()\n",
        "    \n",
        "    # Max drawdown\n",
        "    ax3 = axes[2]\n",
        "    ax3.hist(confirmed['max_drawdown_20d'].dropna()*100, bins=20, alpha=0.5, label='Confirmed', color='red')\n",
        "    ax3.hist(control['max_drawdown_20d'].dropna()*100, bins=20, alpha=0.5, label='Control', color='blue')\n",
        "    ax3.set_xlabel('Max Drawdown (%)')\n",
        "    ax3.set_title('20-Day Maximum Drawdown')\n",
        "    ax3.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(config.RESULTS_PATH, 'confirmed_vs_control.png'), dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Generate visualizations\n",
        "if len(episodes_df) > 0:\n",
        "    print(\"Generating episode visualizations...\")\n",
        "    plot_episode_characteristics(episodes_df)\n",
        "    plot_confirmed_vs_control(episodes_df)\n",
        "else:\n",
        "    print(\"No episodes to visualize\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save Outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SAVE OUTPUTS\n",
        "# =============================================================================\n",
        "\n",
        "def save_episode_data(episodes_df: pd.DataFrame,\n",
        "                      merged_df: pd.DataFrame,\n",
        "                      detection_summary: Dict,\n",
        "                      placebo_results: Dict,\n",
        "                      output_dir: str):\n",
        "    \"\"\"Save episode detection outputs.\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # Save episodes\n",
        "    episodes_path = os.path.join(output_dir, 'episodes.parquet')\n",
        "    episodes_df.to_parquet(episodes_path, index=False)\n",
        "    print(f\"Saved episodes: {episodes_path}\")\n",
        "    \n",
        "    # Save episodes CSV\n",
        "    episodes_csv = os.path.join(output_dir, 'episodes.csv')\n",
        "    episodes_df.to_csv(episodes_csv, index=False)\n",
        "    print(f\"Saved episodes CSV: {episodes_csv}\")\n",
        "    \n",
        "    # Save merged daily data with episode flags\n",
        "    merged_path = os.path.join(output_dir, 'merged_daily_data.parquet')\n",
        "    merged_df.to_parquet(merged_path, index=False)\n",
        "    print(f\"Saved merged daily data: {merged_path}\")\n",
        "    \n",
        "    # Save summary\n",
        "    summary = {\n",
        "        'detection': detection_summary,\n",
        "        'placebo': placebo_results,\n",
        "        'episode_stats': {\n",
        "            'total_episodes': len(episodes_df),\n",
        "            'confirmed_pumps': int((episodes_df['label'] == 1).sum()) if 'label' in episodes_df.columns else 0,\n",
        "            'control_episodes': int((episodes_df['label'] == 0).sum()) if 'label' in episodes_df.columns else len(episodes_df),\n",
        "            'unique_tickers': int(episodes_df['ticker'].nunique()),\n",
        "            'avg_event_return': float(episodes_df['event_return'].mean()) if 'event_return' in episodes_df.columns else np.nan,\n",
        "            'avg_20d_reversal': float(episodes_df['return_20d'].mean()) if 'return_20d' in episodes_df.columns else np.nan,\n",
        "            'avg_max_drawdown_20d': float(episodes_df['max_drawdown_20d'].mean()) if 'max_drawdown_20d' in episodes_df.columns else np.nan\n",
        "        },\n",
        "        'config': {\n",
        "            'return_threshold': config.RETURN_ZSCORE_THRESHOLD,\n",
        "            'social_threshold': config.SOCIAL_ZSCORE_THRESHOLD,\n",
        "            'social_window': config.SOCIAL_WINDOW,\n",
        "            'cluster_gap': config.MIN_CLUSTER_GAP\n",
        "        },\n",
        "        'created_at': datetime.now().isoformat()\n",
        "    }\n",
        "    \n",
        "    summary_path = os.path.join(output_dir, 'notebook04_summary.json')\n",
        "    with open(summary_path, 'w') as f:\n",
        "        json.dump(summary, f, indent=2, default=str)\n",
        "    print(f\"Saved summary: {summary_path}\")\n",
        "    \n",
        "    return summary\n",
        "\n",
        "\n",
        "# Save outputs\n",
        "output_summary = save_episode_data(\n",
        "    episodes_df=episodes_df,\n",
        "    merged_df=merged_data,\n",
        "    detection_summary=detection_summary,\n",
        "    placebo_results=placebo_results,\n",
        "    output_dir=config.RESULTS_PATH\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Output Summary:\")\n",
        "print(json.dumps(output_summary, indent=2, default=str))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Summary and Next Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# NOTEBOOK 4 SUMMARY\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\"\"\n",
        "\n",
        "               NOTEBOOK 4: EPISODE DETECTION COMPLETE                         \n",
        "\n",
        "\n",
        "OUTPUT FILES:\n",
        "\n",
        " episodes.parquet                - Episode-level dataset with window metrics\n",
        " episodes.csv                    - CSV for inspection\n",
        " merged_daily_data.parquet       - Full daily data with episode flags\n",
        " episode_characteristics.png     - Visualizations\n",
        " confirmed_vs_control.png        - Comparison plots\n",
        " notebook04_summary.json         - Summary statistics\n",
        "\n",
        "EPISODE DETECTION CRITERIA:\n",
        "\n",
        "1. Return z-score > 3.0 (price spike)\n",
        "2. Volume > 95th percentile (volume spike)\n",
        "3. Social burst within +-1 day (message z-score > 3.0)\n",
        "4. Consecutive events clustered (5-day gap)\n",
        "\n",
        "EPISODE WINDOWS:\n",
        "\n",
        " Pre-event: t-20 to t-1 (baseline)\n",
        " Event: t0 (spike day)\n",
        " Post-short: t+1 to t+5\n",
        " Post-medium: t+1 to t+20\n",
        " Post-long: t+1 to t+60\n",
        "\n",
        "KEY METRICS:\n",
        "\n",
        " Event return: Return on spike day\n",
        " Reversal returns: Cumulative returns in post windows\n",
        " Max drawdown: Worst peak-to-trough in each window\n",
        "\n",
        "NEXT STEPS:\n",
        "\n",
        " Notebook 5: Feature Engineering & Classification\n",
        "  - Engineer features for pump classification\n",
        "  - Train Random Forest classifier\n",
        "  - Generate Pump Likelihood Scores (PLS)\n",
        "\n",
        "IMPORTANT NOTES:\n",
        "\n",
        "1. Episodes are defined by joint conditions - not all price spikes are episodes\n",
        "2. Placebo test validates that joint detection is not random\n",
        "3. Ground truth labels are from SEC enforcement (incomplete)\n",
        "4. PLS score will be computed in Notebook 5 as continuous proxy\n",
        "\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ENVIRONMENT INFO\n",
        "# =============================================================================\n",
        "\n",
        "import sys\n",
        "import platform\n",
        "\n",
        "print(\"Environment Information:\")\n",
        "print(f\"  Python: {sys.version}\")\n",
        "print(f\"  Platform: {platform.platform()}\")\n",
        "print(f\"  Pandas: {pd.__version__}\")\n",
        "print(f\"  NumPy: {np.__version__}\")\n",
        "print(f\"  Timestamp: {datetime.now().isoformat()}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

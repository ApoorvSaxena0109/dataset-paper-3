{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 5: Feature Engineering & Pump Classification\n",
        "## Social Media-Driven Stock Manipulation and Tail Risk Research\n",
        "\n",
        "---\n",
        "\n",
        "**Research Project:** Social Media-Driven Stock Manipulation and Tail Risk\n",
        "\n",
        "**Purpose:** Engineer features for pump detection, train a classification model, and generate Pump Likelihood Scores (PLS) for each episode.\n",
        "\n",
        "**Inputs:**\n",
        "- Episodes with window metrics (Notebook 4)\n",
        "- Ground truth labels (Notebook 1)\n",
        "\n",
        "**Output:**\n",
        "- Feature-engineered episode dataset\n",
        "- Trained classification model\n",
        "- Episodes with PLS scores\n",
        "\n",
        "---\n",
        "\n",
        "**Last Updated:** 2025"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# INSTALL REQUIRED PACKAGES\n",
        "# =============================================================================\n",
        "\n",
        "!pip install pandas==2.0.3\n",
        "!pip install numpy==1.24.3\n",
        "!pip install scikit-learn==1.3.2\n",
        "!pip install scipy==1.11.4\n",
        "!pip install tqdm==4.66.1\n",
        "!pip install pyarrow==14.0.1\n",
        "!pip install matplotlib==3.8.2\n",
        "!pip install seaborn==0.13.0\n",
        "!pip install shap==0.44.0\n",
        "\n",
        "print(\"All packages installed successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# IMPORT LIBRARIES\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split, cross_val_score, cross_val_predict,\n",
        "    StratifiedKFold, TimeSeriesSplit\n",
        ")\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, roc_auc_score,\n",
        "    precision_recall_curve, roc_curve, f1_score, accuracy_score\n",
        ")\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', 100)\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette('husl')\n",
        "\n",
        "print(f\"Environment setup complete. Timestamp: {datetime.now()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration and Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "class ResearchConfig:\n",
        "    \"\"\"Configuration for classification.\"\"\"\n",
        "    \n",
        "    # Model Parameters\n",
        "    N_ESTIMATORS = 100\n",
        "    MAX_DEPTH = 5  # Prevent overfitting with small samples\n",
        "    RANDOM_STATE = 42\n",
        "    \n",
        "    # Cross-validation\n",
        "    N_FOLDS = 5\n",
        "    TEST_SIZE = 0.2\n",
        "    \n",
        "    # Data Paths\n",
        "    BASE_PATH = \"/content/drive/MyDrive/Research/PumpDump/\"\n",
        "    PROCESSED_DATA_PATH = BASE_PATH + \"data/processed/\"\n",
        "    RESULTS_PATH = BASE_PATH + \"results/\"\n",
        "\n",
        "config = ResearchConfig()\n",
        "\n",
        "# Handle Colab vs local\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    config.BASE_PATH = \"./research_data/\"\n",
        "    config.PROCESSED_DATA_PATH = config.BASE_PATH + \"data/processed/\"\n",
        "    config.RESULTS_PATH = config.BASE_PATH + \"results/\"\n",
        "\n",
        "os.makedirs(config.RESULTS_PATH, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# LOAD DATA\n",
        "# =============================================================================\n",
        "\n",
        "def load_episodes(results_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Load episodes from Notebook 4.\"\"\"\n",
        "    episodes_path = os.path.join(results_path, 'episodes.parquet')\n",
        "    \n",
        "    if os.path.exists(episodes_path):\n",
        "        episodes = pd.read_parquet(episodes_path)\n",
        "        print(f\"Loaded episodes: {len(episodes)} rows\")\n",
        "    else:\n",
        "        print(\"Episodes file not found - creating sample\")\n",
        "        episodes = create_sample_episodes()\n",
        "    \n",
        "    return episodes\n",
        "\n",
        "\n",
        "def create_sample_episodes() -> pd.DataFrame:\n",
        "    \"\"\"Create sample episodes for demonstration.\"\"\"\n",
        "    np.random.seed(42)\n",
        "    n = 200\n",
        "    \n",
        "    # Mix of confirmed and control\n",
        "    labels = np.concatenate([np.ones(30), np.zeros(170)])\n",
        "    np.random.shuffle(labels)\n",
        "    \n",
        "    episodes = pd.DataFrame({\n",
        "        'episode_id': range(1, n+1),\n",
        "        'ticker': np.random.choice(['GME', 'AMC', 'BB', 'NOK', 'CLOV', 'WISH', 'MULN', 'FFIE'], n),\n",
        "        'event_date': pd.date_range('2020-01-01', periods=n, freq='W'),\n",
        "        'label': labels.astype(int),\n",
        "        'event_return': np.random.uniform(0.1, 0.5, n),\n",
        "        'event_volume_ratio': np.random.uniform(2, 20, n),\n",
        "        'return_5d': np.where(labels == 1, np.random.uniform(-0.4, -0.1, n), np.random.uniform(-0.2, 0.1, n)),\n",
        "        'return_20d': np.where(labels == 1, np.random.uniform(-0.6, -0.2, n), np.random.uniform(-0.3, 0.2, n)),\n",
        "        'max_drawdown_20d': np.where(labels == 1, np.random.uniform(0.4, 0.7, n), np.random.uniform(0.1, 0.4, n)),\n",
        "        'msg_count': np.random.poisson(50, n) * (1 + labels),\n",
        "        'msg_zscore': np.random.uniform(3, 10, n),\n",
        "        'promo_share': np.where(labels == 1, np.random.uniform(0.3, 0.7, n), np.random.uniform(0.05, 0.3, n)),\n",
        "        'user_concentration': np.where(labels == 1, np.random.uniform(0.5, 0.9, n), np.random.uniform(0.2, 0.5, n)),\n",
        "        'pre_avg_return': np.random.normal(0, 0.02, n),\n",
        "        'pre_avg_volume': np.random.lognormal(14, 1, n)\n",
        "    })\n",
        "    \n",
        "    return episodes\n",
        "\n",
        "\n",
        "# Load episodes\n",
        "episodes_df = load_episodes(config.RESULTS_PATH)\n",
        "print(f\"\\nLabel distribution:\")\n",
        "print(episodes_df['label'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# FEATURE ENGINEER\n",
        "# =============================================================================\n",
        "\n",
        "class FeatureEngineer:\n",
        "    \"\"\"Engineers features for pump classification.\n",
        "    \n",
        "    Feature Categories:\n",
        "    A. Market Features (from price-volume data)\n",
        "    B. Social Features (from message board data)\n",
        "    C. Derived Features (combinations and ratios)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Feature definitions\n",
        "    MARKET_FEATURES = [\n",
        "        'event_return',\n",
        "        'event_volume_ratio',\n",
        "        'return_5d',\n",
        "        'return_20d',\n",
        "        'max_drawdown_5d',\n",
        "        'max_drawdown_20d',\n",
        "        'pre_avg_return',\n",
        "    ]\n",
        "    \n",
        "    SOCIAL_FEATURES = [\n",
        "        'msg_zscore',\n",
        "        'promo_share',\n",
        "        'user_concentration',\n",
        "        'msg_count',\n",
        "    ]\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.feature_names = []\n",
        "        \n",
        "    def add_market_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Add market-based features.\"\"\"\n",
        "        df = df.copy()\n",
        "        \n",
        "        # Reversal magnitude\n",
        "        if 'event_return' in df.columns and 'return_5d' in df.columns:\n",
        "            df['reversal_5d'] = -df['return_5d']  # Positive if price dropped\n",
        "        if 'event_return' in df.columns and 'return_20d' in df.columns:\n",
        "            df['reversal_20d'] = -df['return_20d']\n",
        "            \n",
        "        # Reversal relative to event return\n",
        "        if 'event_return' in df.columns and 'return_20d' in df.columns:\n",
        "            df['reversal_ratio_20d'] = -df['return_20d'] / (df['event_return'] + 0.01)\n",
        "        \n",
        "        # Volume spike severity\n",
        "        if 'event_volume_ratio' in df.columns:\n",
        "            df['log_volume_ratio'] = np.log1p(df['event_volume_ratio'])\n",
        "            \n",
        "        # Price momentum before event\n",
        "        if 'pre_avg_return' in df.columns:\n",
        "            df['pre_momentum'] = df['pre_avg_return'] * 20  # Approximate 20-day momentum\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def add_social_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Add social media-based features.\"\"\"\n",
        "        df = df.copy()\n",
        "        \n",
        "        # Log message count\n",
        "        if 'msg_count' in df.columns:\n",
        "            df['log_msg_count'] = np.log1p(df['msg_count'])\n",
        "        \n",
        "        # Promo intensity (promo share * z-score)\n",
        "        if 'promo_share' in df.columns and 'msg_zscore' in df.columns:\n",
        "            df['promo_intensity'] = df['promo_share'] * df['msg_zscore']\n",
        "        \n",
        "        # Coordination score (high concentration + high promo)\n",
        "        if 'user_concentration' in df.columns and 'promo_share' in df.columns:\n",
        "            df['coordination_score'] = df['user_concentration'] * df['promo_share']\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def add_interaction_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Add interaction terms between market and social.\"\"\"\n",
        "        df = df.copy()\n",
        "        \n",
        "        # Social * Volume interaction\n",
        "        if 'msg_zscore' in df.columns and 'event_volume_ratio' in df.columns:\n",
        "            df['social_volume_interaction'] = df['msg_zscore'] * np.log1p(df['event_volume_ratio'])\n",
        "        \n",
        "        # Promo * Reversal interaction\n",
        "        if 'promo_share' in df.columns and 'reversal_20d' in df.columns:\n",
        "            df['promo_reversal_interaction'] = df['promo_share'] * df['reversal_20d']\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def get_feature_matrix(self, df: pd.DataFrame) -> Tuple[np.ndarray, List[str]]:\n",
        "        \"\"\"Extract feature matrix for modeling.\"\"\"\n",
        "        \n",
        "        # Define all features to use\n",
        "        all_features = [\n",
        "            # Original market features\n",
        "            'event_return', 'event_volume_ratio', 'return_5d', 'return_20d',\n",
        "            'max_drawdown_20d',\n",
        "            \n",
        "            # Original social features\n",
        "            'msg_zscore', 'promo_share', 'user_concentration',\n",
        "            \n",
        "            # Engineered features\n",
        "            'reversal_20d', 'reversal_ratio_20d', 'log_volume_ratio',\n",
        "            'log_msg_count', 'promo_intensity', 'coordination_score',\n",
        "            'social_volume_interaction'\n",
        "        ]\n",
        "        \n",
        "        # Filter to available features\n",
        "        available = [f for f in all_features if f in df.columns]\n",
        "        \n",
        "        self.feature_names = available\n",
        "        \n",
        "        X = df[available].values\n",
        "        \n",
        "        return X, available\n",
        "    \n",
        "    def engineer_all(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Run full feature engineering pipeline.\"\"\"\n",
        "        print(\"Engineering features...\")\n",
        "        \n",
        "        df = self.add_market_features(df)\n",
        "        df = self.add_social_features(df)\n",
        "        df = self.add_interaction_features(df)\n",
        "        \n",
        "        X, features = self.get_feature_matrix(df)\n",
        "        \n",
        "        print(f\"Total features: {len(features)}\")\n",
        "        print(f\"Feature names: {features}\")\n",
        "        \n",
        "        return df\n",
        "\n",
        "\n",
        "# Initialize engineer\n",
        "feature_engineer = FeatureEngineer()\n",
        "\n",
        "# Engineer features\n",
        "episodes_df = feature_engineer.engineer_all(episodes_df)\n",
        "\n",
        "print(\"\\nFeature-engineered data sample:\")\n",
        "print(episodes_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PUMP CLASSIFIER\n",
        "# =============================================================================\n",
        "\n",
        "class PumpClassifier:\n",
        "    \"\"\"Trains and evaluates pump-and-dump classification model.\n",
        "    \n",
        "    Uses Random Forest with careful regularization to prevent\n",
        "    overfitting on small labeled samples.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config: ResearchConfig):\n",
        "        self.config = config\n",
        "        self.model = None\n",
        "        self.scaler = StandardScaler()\n",
        "        self.feature_names = []\n",
        "        self.cv_results = {}\n",
        "        \n",
        "    def prepare_data(self, df: pd.DataFrame, \n",
        "                     feature_engineer: FeatureEngineer) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"Prepare features and labels.\"\"\"\n",
        "        \n",
        "        # Get feature matrix\n",
        "        X, feature_names = feature_engineer.get_feature_matrix(df)\n",
        "        self.feature_names = feature_names\n",
        "        \n",
        "        # Get labels\n",
        "        y = df['label'].values\n",
        "        \n",
        "        # Handle missing values\n",
        "        X = np.nan_to_num(X, nan=0)\n",
        "        \n",
        "        print(f\"Data prepared: {X.shape[0]} samples, {X.shape[1]} features\")\n",
        "        print(f\"Label distribution: {pd.Series(y).value_counts().to_dict()}\")\n",
        "        \n",
        "        return X, y\n",
        "    \n",
        "    def train_model(self, X: np.ndarray, y: np.ndarray) -> Dict:\n",
        "        \"\"\"Train Random Forest classifier with cross-validation.\"\"\"\n",
        "        \n",
        "        # Scale features\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "        \n",
        "        # Initialize model\n",
        "        self.model = RandomForestClassifier(\n",
        "            n_estimators=self.config.N_ESTIMATORS,\n",
        "            max_depth=self.config.MAX_DEPTH,\n",
        "            class_weight='balanced',  # Handle imbalanced classes\n",
        "            random_state=self.config.RANDOM_STATE,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        \n",
        "        # Cross-validation\n",
        "        cv = StratifiedKFold(n_splits=self.config.N_FOLDS, shuffle=True, \n",
        "                             random_state=self.config.RANDOM_STATE)\n",
        "        \n",
        "        # Get cross-validated predictions\n",
        "        cv_probs = cross_val_predict(self.model, X_scaled, y, cv=cv, method='predict_proba')\n",
        "        cv_preds = (cv_probs[:, 1] > 0.5).astype(int)\n",
        "        \n",
        "        # Calculate metrics\n",
        "        self.cv_results = {\n",
        "            'accuracy': accuracy_score(y, cv_preds),\n",
        "            'f1': f1_score(y, cv_preds),\n",
        "            'roc_auc': roc_auc_score(y, cv_probs[:, 1]) if len(np.unique(y)) > 1 else np.nan,\n",
        "            'confusion_matrix': confusion_matrix(y, cv_preds).tolist(),\n",
        "            'classification_report': classification_report(y, cv_preds, output_dict=True)\n",
        "        }\n",
        "        \n",
        "        # Fit final model on all data\n",
        "        self.model.fit(X_scaled, y)\n",
        "        \n",
        "        print(\"\\nCross-Validation Results:\")\n",
        "        print(f\"  Accuracy: {self.cv_results['accuracy']:.3f}\")\n",
        "        print(f\"  F1 Score: {self.cv_results['f1']:.3f}\")\n",
        "        print(f\"  ROC AUC: {self.cv_results['roc_auc']:.3f}\")\n",
        "        print(f\"\\nConfusion Matrix:\")\n",
        "        print(np.array(self.cv_results['confusion_matrix']))\n",
        "        \n",
        "        return self.cv_results\n",
        "    \n",
        "    def predict_pls(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Predict Pump Likelihood Scores.\"\"\"\n",
        "        X_scaled = self.scaler.transform(X)\n",
        "        pls = self.model.predict_proba(X_scaled)[:, 1]\n",
        "        return pls\n",
        "    \n",
        "    def get_feature_importance(self) -> pd.DataFrame:\n",
        "        \"\"\"Get feature importance from trained model.\"\"\"\n",
        "        importance = pd.DataFrame({\n",
        "            'feature': self.feature_names,\n",
        "            'importance': self.model.feature_importances_\n",
        "        }).sort_values('importance', ascending=False)\n",
        "        \n",
        "        return importance\n",
        "    \n",
        "    def temporal_validation(self, df: pd.DataFrame, \n",
        "                            feature_engineer: FeatureEngineer,\n",
        "                            train_end_date: str) -> Dict:\n",
        "        \"\"\"Temporal split validation: train on early data, test on later.\"\"\"\n",
        "        \n",
        "        df = df.copy()\n",
        "        df['event_date'] = pd.to_datetime(df['event_date'])\n",
        "        train_end = pd.to_datetime(train_end_date)\n",
        "        \n",
        "        # Split\n",
        "        train_df = df[df['event_date'] <= train_end]\n",
        "        test_df = df[df['event_date'] > train_end]\n",
        "        \n",
        "        if len(train_df) == 0 or len(test_df) == 0:\n",
        "            print(\"Cannot perform temporal validation - insufficient data\")\n",
        "            return {}\n",
        "        \n",
        "        # Prepare data\n",
        "        X_train, _ = feature_engineer.get_feature_matrix(train_df)\n",
        "        X_test, _ = feature_engineer.get_feature_matrix(test_df)\n",
        "        \n",
        "        X_train = np.nan_to_num(X_train, nan=0)\n",
        "        X_test = np.nan_to_num(X_test, nan=0)\n",
        "        \n",
        "        y_train = train_df['label'].values\n",
        "        y_test = test_df['label'].values\n",
        "        \n",
        "        # Train and evaluate\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "        \n",
        "        model = RandomForestClassifier(\n",
        "            n_estimators=self.config.N_ESTIMATORS,\n",
        "            max_depth=self.config.MAX_DEPTH,\n",
        "            class_weight='balanced',\n",
        "            random_state=self.config.RANDOM_STATE\n",
        "        )\n",
        "        \n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "        y_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
        "        \n",
        "        results = {\n",
        "            'train_size': len(train_df),\n",
        "            'test_size': len(test_df),\n",
        "            'train_period': f\"{train_df['event_date'].min().date()} to {train_df['event_date'].max().date()}\",\n",
        "            'test_period': f\"{test_df['event_date'].min().date()} to {test_df['event_date'].max().date()}\",\n",
        "            'accuracy': accuracy_score(y_test, y_pred),\n",
        "            'f1': f1_score(y_test, y_pred) if y_test.sum() > 0 else np.nan,\n",
        "            'roc_auc': roc_auc_score(y_test, y_prob) if len(np.unique(y_test)) > 1 else np.nan\n",
        "        }\n",
        "        \n",
        "        print(\"\\nTemporal Validation Results:\")\n",
        "        print(f\"  Train: {results['train_period']} ({results['train_size']} samples)\")\n",
        "        print(f\"  Test: {results['test_period']} ({results['test_size']} samples)\")\n",
        "        print(f\"  Accuracy: {results['accuracy']:.3f}\")\n",
        "        print(f\"  F1 Score: {results['f1']:.3f}\")\n",
        "        print(f\"  ROC AUC: {results['roc_auc']:.3f}\")\n",
        "        \n",
        "        return results\n",
        "\n",
        "\n",
        "# Initialize classifier\n",
        "classifier = PumpClassifier(config)\n",
        "print(\"Pump Classifier initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TRAIN MODEL\n",
        "# =============================================================================\n",
        "\n",
        "# Prepare data\n",
        "X, y = classifier.prepare_data(episodes_df, feature_engineer)\n",
        "\n",
        "# Train with cross-validation\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING PUMP CLASSIFIER\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "cv_results = classifier.train_model(X, y)\n",
        "\n",
        "# Feature importance\n",
        "importance_df = classifier.get_feature_importance()\n",
        "print(\"\\nFeature Importance:\")\n",
        "print(importance_df.head(15))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TEMPORAL VALIDATION\n",
        "# =============================================================================\n",
        "\n",
        "# Run temporal validation (train on 2019-2022, test on 2023+)\n",
        "temporal_results = classifier.temporal_validation(\n",
        "    episodes_df, \n",
        "    feature_engineer,\n",
        "    train_end_date='2022-12-31'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Generate Pump Likelihood Scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# GENERATE PLS SCORES\n",
        "# =============================================================================\n",
        "\n",
        "# Predict PLS for all episodes\n",
        "X, _ = feature_engineer.get_feature_matrix(episodes_df)\n",
        "X = np.nan_to_num(X, nan=0)\n",
        "\n",
        "episodes_df['pls'] = classifier.predict_pls(X)\n",
        "\n",
        "print(\"Pump Likelihood Scores (PLS) Generated\")\n",
        "print(f\"\\nPLS Distribution:\")\n",
        "print(episodes_df['pls'].describe())\n",
        "\n",
        "# Decile analysis\n",
        "episodes_df['pls_decile'] = pd.qcut(episodes_df['pls'], q=10, labels=False, duplicates='drop') + 1\n",
        "\n",
        "decile_analysis = episodes_df.groupby('pls_decile').agg({\n",
        "    'pls': 'mean',\n",
        "    'label': 'mean',\n",
        "    'return_20d': 'mean',\n",
        "    'max_drawdown_20d': 'mean',\n",
        "    'promo_share': 'mean'\n",
        "}).round(3)\n",
        "\n",
        "print(\"\\nPLS Decile Analysis:\")\n",
        "print(decile_analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# VISUALIZATIONS\n",
        "# =============================================================================\n",
        "\n",
        "def plot_model_performance(cv_results: Dict, importance_df: pd.DataFrame):\n",
        "    \"\"\"Plot model performance visualizations.\"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    \n",
        "    # Confusion matrix\n",
        "    ax1 = axes[0]\n",
        "    cm = np.array(cv_results['confusion_matrix'])\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
        "                xticklabels=['Control', 'Pump'], yticklabels=['Control', 'Pump'])\n",
        "    ax1.set_xlabel('Predicted')\n",
        "    ax1.set_ylabel('Actual')\n",
        "    ax1.set_title('Confusion Matrix (CV)')\n",
        "    \n",
        "    # Feature importance\n",
        "    ax2 = axes[1]\n",
        "    top_features = importance_df.head(10)\n",
        "    ax2.barh(top_features['feature'], top_features['importance'], color='steelblue')\n",
        "    ax2.set_xlabel('Importance')\n",
        "    ax2.set_title('Top 10 Feature Importance')\n",
        "    ax2.invert_yaxis()\n",
        "    \n",
        "    # Performance metrics\n",
        "    ax3 = axes[2]\n",
        "    metrics = ['accuracy', 'f1', 'roc_auc']\n",
        "    values = [cv_results.get(m, 0) for m in metrics]\n",
        "    colors = ['blue', 'green', 'orange']\n",
        "    ax3.bar(metrics, values, color=colors)\n",
        "    ax3.set_ylim(0, 1)\n",
        "    ax3.set_ylabel('Score')\n",
        "    ax3.set_title('Cross-Validation Metrics')\n",
        "    for i, v in enumerate(values):\n",
        "        ax3.text(i, v + 0.02, f'{v:.3f}', ha='center')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(config.RESULTS_PATH, 'model_performance.png'), dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_pls_analysis(episodes_df: pd.DataFrame):\n",
        "    \"\"\"Plot PLS analysis visualizations.\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    \n",
        "    # PLS distribution\n",
        "    ax1 = axes[0, 0]\n",
        "    confirmed = episodes_df[episodes_df['label'] == 1]['pls']\n",
        "    control = episodes_df[episodes_df['label'] == 0]['pls']\n",
        "    ax1.hist(control, bins=30, alpha=0.5, label='Control', color='blue')\n",
        "    ax1.hist(confirmed, bins=30, alpha=0.5, label='Confirmed Pump', color='red')\n",
        "    ax1.set_xlabel('Pump Likelihood Score (PLS)')\n",
        "    ax1.set_ylabel('Frequency')\n",
        "    ax1.set_title('PLS Distribution by Label')\n",
        "    ax1.legend()\n",
        "    \n",
        "    # PLS vs 20-day return\n",
        "    ax2 = axes[0, 1]\n",
        "    ax2.scatter(episodes_df['pls'], episodes_df['return_20d']*100, \n",
        "                c=episodes_df['label'], cmap='coolwarm', alpha=0.6)\n",
        "    ax2.axhline(y=0, color='black', linestyle='--')\n",
        "    ax2.set_xlabel('PLS')\n",
        "    ax2.set_ylabel('20-Day Return (%)')\n",
        "    ax2.set_title('PLS vs Post-Event Returns')\n",
        "    \n",
        "    # Lift curve\n",
        "    ax3 = axes[1, 0]\n",
        "    decile_rates = episodes_df.groupby('pls_decile')['label'].mean()\n",
        "    baseline = episodes_df['label'].mean()\n",
        "    ax3.bar(decile_rates.index, decile_rates.values, color='steelblue')\n",
        "    ax3.axhline(y=baseline, color='red', linestyle='--', label=f'Baseline ({baseline:.2%})')\n",
        "    ax3.set_xlabel('PLS Decile')\n",
        "    ax3.set_ylabel('Confirmed Pump Rate')\n",
        "    ax3.set_title('Lift Curve: Pump Rate by PLS Decile')\n",
        "    ax3.legend()\n",
        "    \n",
        "    # Drawdown by PLS decile\n",
        "    ax4 = axes[1, 1]\n",
        "    decile_drawdown = episodes_df.groupby('pls_decile')['max_drawdown_20d'].mean() * 100\n",
        "    ax4.bar(decile_drawdown.index, decile_drawdown.values, color='darkred')\n",
        "    ax4.set_xlabel('PLS Decile')\n",
        "    ax4.set_ylabel('Average Max Drawdown (%)')\n",
        "    ax4.set_title('Drawdown by PLS Decile')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(config.RESULTS_PATH, 'pls_analysis.png'), dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Generate visualizations\n",
        "print(\"Generating visualizations...\")\n",
        "plot_model_performance(cv_results, importance_df)\n",
        "plot_pls_analysis(episodes_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Robustness: Threshold Sensitivity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ROBUSTNESS CHECKS\n",
        "# =============================================================================\n",
        "\n",
        "def run_robustness_checks(episodes_df: pd.DataFrame, \n",
        "                          feature_engineer: FeatureEngineer,\n",
        "                          config: ResearchConfig) -> Dict:\n",
        "    \"\"\"Run robustness checks with different model configurations.\"\"\"\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    # Prepare data\n",
        "    X, _ = feature_engineer.get_feature_matrix(episodes_df)\n",
        "    X = np.nan_to_num(X, nan=0)\n",
        "    y = episodes_df['label'].values\n",
        "    \n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    \n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    \n",
        "    # Test 1: Different max_depth\n",
        "    print(\"Testing max_depth sensitivity...\")\n",
        "    depth_results = []\n",
        "    for depth in [3, 5, 7, 10, None]:\n",
        "        model = RandomForestClassifier(\n",
        "            n_estimators=100, max_depth=depth, class_weight='balanced', random_state=42\n",
        "        )\n",
        "        scores = cross_val_score(model, X_scaled, y, cv=cv, scoring='f1')\n",
        "        depth_results.append({'max_depth': depth, 'f1_mean': scores.mean(), 'f1_std': scores.std()})\n",
        "    \n",
        "    results['depth_sensitivity'] = depth_results\n",
        "    \n",
        "    # Test 2: Different models\n",
        "    print(\"Testing different models...\")\n",
        "    model_results = []\n",
        "    \n",
        "    models = {\n",
        "        'RandomForest': RandomForestClassifier(n_estimators=100, max_depth=5, class_weight='balanced', random_state=42),\n",
        "        'GradientBoosting': GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=42),\n",
        "        'LogisticRegression': LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000)\n",
        "    }\n",
        "    \n",
        "    for name, model in models.items():\n",
        "        scores = cross_val_score(model, X_scaled, y, cv=cv, scoring='f1')\n",
        "        model_results.append({'model': name, 'f1_mean': scores.mean(), 'f1_std': scores.std()})\n",
        "    \n",
        "    results['model_comparison'] = model_results\n",
        "    \n",
        "    # Print results\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ROBUSTNESS CHECK RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    print(\"\\nMax Depth Sensitivity:\")\n",
        "    for r in depth_results:\n",
        "        print(f\"  depth={r['max_depth']}: F1 = {r['f1_mean']:.3f} (+/- {r['f1_std']:.3f})\")\n",
        "    \n",
        "    print(\"\\nModel Comparison:\")\n",
        "    for r in model_results:\n",
        "        print(f\"  {r['model']}: F1 = {r['f1_mean']:.3f} (+/- {r['f1_std']:.3f})\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "# Run robustness checks\n",
        "robustness_results = run_robustness_checks(episodes_df, feature_engineer, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SAVE OUTPUTS\n",
        "# =============================================================================\n",
        "\n",
        "def save_classification_results(episodes_df: pd.DataFrame,\n",
        "                                 cv_results: Dict,\n",
        "                                 importance_df: pd.DataFrame,\n",
        "                                 robustness_results: Dict,\n",
        "                                 output_dir: str):\n",
        "    \"\"\"Save classification outputs.\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # Save episodes with PLS\n",
        "    episodes_path = os.path.join(output_dir, 'episodes_with_pls.parquet')\n",
        "    episodes_df.to_parquet(episodes_path, index=False)\n",
        "    print(f\"Saved episodes with PLS: {episodes_path}\")\n",
        "    \n",
        "    # Save CSV\n",
        "    episodes_csv = os.path.join(output_dir, 'episodes_with_pls.csv')\n",
        "    episodes_df.to_csv(episodes_csv, index=False)\n",
        "    print(f\"Saved CSV: {episodes_csv}\")\n",
        "    \n",
        "    # Save feature importance\n",
        "    importance_path = os.path.join(output_dir, 'feature_importance.csv')\n",
        "    importance_df.to_csv(importance_path, index=False)\n",
        "    print(f\"Saved feature importance: {importance_path}\")\n",
        "    \n",
        "    # Save summary\n",
        "    summary = {\n",
        "        'episodes_total': len(episodes_df),\n",
        "        'confirmed_pumps': int((episodes_df['label'] == 1).sum()),\n",
        "        'control_episodes': int((episodes_df['label'] == 0).sum()),\n",
        "        'cv_results': cv_results,\n",
        "        'pls_stats': episodes_df['pls'].describe().to_dict(),\n",
        "        'top_features': importance_df.head(10).to_dict('records'),\n",
        "        'robustness': robustness_results,\n",
        "        'created_at': datetime.now().isoformat()\n",
        "    }\n",
        "    \n",
        "    summary_path = os.path.join(output_dir, 'notebook05_summary.json')\n",
        "    with open(summary_path, 'w') as f:\n",
        "        json.dump(summary, f, indent=2, default=str)\n",
        "    print(f\"Saved summary: {summary_path}\")\n",
        "    \n",
        "    return summary\n",
        "\n",
        "\n",
        "# Save outputs\n",
        "output_summary = save_classification_results(\n",
        "    episodes_df=episodes_df,\n",
        "    cv_results=cv_results,\n",
        "    importance_df=importance_df,\n",
        "    robustness_results=robustness_results,\n",
        "    output_dir=config.RESULTS_PATH\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Output Summary (key metrics):\")\n",
        "print(f\"  Episodes: {output_summary['episodes_total']}\")\n",
        "print(f\"  CV Accuracy: {output_summary['cv_results']['accuracy']:.3f}\")\n",
        "print(f\"  CV F1: {output_summary['cv_results']['f1']:.3f}\")\n",
        "print(f\"  CV AUC: {output_summary['cv_results']['roc_auc']:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Summary and Next Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# NOTEBOOK 5 SUMMARY\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\"\"\n",
        "╔══════════════════════════════════════════════════════════════════════════════╗\n",
        "║        NOTEBOOK 5: FEATURE ENGINEERING & CLASSIFICATION COMPLETE             ║\n",
        "╚══════════════════════════════════════════════════════════════════════════════╝\n",
        "\n",
        "OUTPUT FILES:\n",
        "─────────────\n",
        "• episodes_with_pls.parquet       - Episodes with Pump Likelihood Scores\n",
        "• episodes_with_pls.csv           - CSV for inspection\n",
        "• feature_importance.csv          - Feature importance rankings\n",
        "• model_performance.png           - Performance visualizations\n",
        "• pls_analysis.png                - PLS analysis plots\n",
        "• notebook05_summary.json         - Summary statistics\n",
        "\n",
        "FEATURES ENGINEERED:\n",
        "────────────────────\n",
        "Market Features:\n",
        "• Event return, volume ratio\n",
        "• 5/20-day reversals and drawdowns\n",
        "• Pre-event momentum\n",
        "\n",
        "Social Features:\n",
        "• Message z-score, promotional share\n",
        "• User concentration (Gini)\n",
        "• Promo intensity, coordination score\n",
        "\n",
        "Interaction Features:\n",
        "• Social-volume interaction\n",
        "• Promo-reversal interaction\n",
        "\n",
        "MODEL DETAILS:\n",
        "──────────────\n",
        "• Algorithm: Random Forest\n",
        "• Regularization: max_depth=5, balanced classes\n",
        "• Validation: 5-fold stratified CV\n",
        "\n",
        "PUMP LIKELIHOOD SCORE (PLS):\n",
        "────────────────────────────\n",
        "• Range: 0 to 1\n",
        "• 0 = Low manipulation likelihood\n",
        "• 1 = High manipulation likelihood\n",
        "• Use as continuous proxy for manipulation risk\n",
        "\n",
        "NEXT STEPS:\n",
        "───────────\n",
        "→ Notebook 6: Tail Risk Analysis\n",
        "  - Compute VaR and Expected Shortfall\n",
        "  - Portfolio-level analysis\n",
        "  - Spillover analysis\n",
        "  - Regression models\n",
        "\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ENVIRONMENT INFO\n",
        "# =============================================================================\n",
        "\n",
        "import sys\n",
        "import platform\n",
        "import sklearn\n",
        "\n",
        "print(\"Environment Information:\")\n",
        "print(f\"  Python: {sys.version}\")\n",
        "print(f\"  Platform: {platform.platform()}\")\n",
        "print(f\"  Pandas: {pd.__version__}\")\n",
        "print(f\"  NumPy: {np.__version__}\")\n",
        "print(f\"  Scikit-learn: {sklearn.__version__}\")\n",
        "print(f\"  Timestamp: {datetime.now().isoformat()}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

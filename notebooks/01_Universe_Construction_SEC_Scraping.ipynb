{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Universe Construction & SEC Enforcement Scraping\n",
    "## Social Media-Driven Stock Manipulation and Tail Risk Research\n",
    "\n",
    "---\n",
    "\n",
    "**Research Project:** Social Media-Driven Stock Manipulation and Tail Risk\n",
    "\n",
    "**Purpose:** Build the stock universe for analysis using freely available web sources and extract ground truth labels from SEC enforcement releases.\n",
    "\n",
    "**Data Sources:**\n",
    "- SEC EDGAR Litigation Releases\n",
    "- OTC Markets Stock Screener\n",
    "- Yahoo Finance Screener\n",
    "\n",
    "**Output:** \n",
    "- Ticker universe with metadata\n",
    "- SEC enforcement cases (ground truth labels)\n",
    "\n",
    "---\n",
    "\n",
    "**Last Updated:** 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# INSTALL REQUIRED PACKAGES (Colab-compatible)\n# =============================================================================\n\n# Use Colab's pre-installed pandas, numpy, requests, tqdm to avoid conflicts\n# Only install packages not included in Colab\n\n!pip install -q yfinance\n!pip install -q beautifulsoup4\n!pip install -q lxml\n!pip install -q pyarrow\n\nprint(\"All packages installed successfully.\")\nprint(\"Using Colab's pre-installed: pandas, numpy, requests, tqdm\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORT LIBRARIES\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Set, Optional, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import yfinance as yf\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "print(f\"Environment setup complete. Timestamp: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RESEARCH CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "class ResearchConfig:\n",
    "    \"\"\"Configuration for Social Media Stock Manipulation Research.\n",
    "    \n",
    "    This research focuses on web-scrapeable data only:\n",
    "    - Yahoo Finance (prices, volume, message boards)\n",
    "    - SEC EDGAR (filings, enforcement releases)\n",
    "    - Public news archives\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sample Period\n",
    "    START_DATE = \"2019-01-01\"\n",
    "    END_DATE = \"2025-12-31\"\n",
    "    \n",
    "    # Universe Filters\n",
    "    MAX_MARKET_CAP = 500_000_000  # $500M\n",
    "    MAX_PRICE = 10.0  # $10\n",
    "    MIN_AVG_VOLUME = 10_000  # shares/day\n",
    "    \n",
    "    # Episode Detection Thresholds\n",
    "    RETURN_ZSCORE_THRESHOLD = 3.0\n",
    "    VOLUME_PERCENTILE_THRESHOLD = 95\n",
    "    SOCIAL_ZSCORE_THRESHOLD = 3.0\n",
    "    ROLLING_WINDOW = 60  # days\n",
    "    \n",
    "    # Data Storage Paths (Google Drive mount for Colab)\n",
    "    BASE_PATH = \"/content/drive/MyDrive/Research/PumpDump/\"\n",
    "    RAW_DATA_PATH = BASE_PATH + \"data/raw/\"\n",
    "    PROCESSED_DATA_PATH = BASE_PATH + \"data/processed/\"\n",
    "    RESULTS_PATH = BASE_PATH + \"results/\"\n",
    "    \n",
    "    # Scraping Rate Limits\n",
    "    MIN_DELAY = 2.0  # seconds\n",
    "    MAX_DELAY = 5.0  # seconds\n",
    "    \n",
    "    # User Agent for requests\n",
    "    USER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    \n",
    "    @classmethod\n",
    "    def print_config(cls):\n",
    "        print(\"=\"*60)\n",
    "        print(\"RESEARCH CONFIGURATION\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Sample Period: {cls.START_DATE} to {cls.END_DATE}\")\n",
    "        print(f\"Max Market Cap: ${cls.MAX_MARKET_CAP:,.0f}\")\n",
    "        print(f\"Max Price: ${cls.MAX_PRICE}\")\n",
    "        print(f\"Min Avg Volume: {cls.MIN_AVG_VOLUME:,} shares/day\")\n",
    "        print(f\"Return Z-Score Threshold: {cls.RETURN_ZSCORE_THRESHOLD}\")\n",
    "        print(f\"Volume Percentile Threshold: {cls.VOLUME_PERCENTILE_THRESHOLD}%\")\n",
    "        print(f\"Social Z-Score Threshold: {cls.SOCIAL_ZSCORE_THRESHOLD}\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "config = ResearchConfig()\n",
    "config.print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MOUNT GOOGLE DRIVE (for Colab)\n",
    "# =============================================================================\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    print(\"Not running in Colab - using local paths\")\n",
    "    IN_COLAB = False\n",
    "    # Override paths for local execution\n",
    "    config.BASE_PATH = \"./research_data/\"\n",
    "    config.RAW_DATA_PATH = config.BASE_PATH + \"data/raw/\"\n",
    "    config.PROCESSED_DATA_PATH = config.BASE_PATH + \"data/processed/\"\n",
    "    config.RESULTS_PATH = config.BASE_PATH + \"results/\"\n",
    "\n",
    "# Create directory structure\n",
    "os.makedirs(config.RAW_DATA_PATH, exist_ok=True)\n",
    "os.makedirs(config.PROCESSED_DATA_PATH, exist_ok=True)\n",
    "os.makedirs(config.RESULTS_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"Data directories created at: {config.BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SEC Enforcement Release Scraper\n",
    "\n",
    "### 3.1 Scrape SEC Litigation Releases\n",
    "\n",
    "We scrape SEC litigation releases to identify confirmed pump-and-dump cases. These serve as ground truth labels for our classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SEC ENFORCEMENT SCRAPER\n",
    "# =============================================================================\n",
    "\n",
    "class SECEnforcementScraper:\n",
    "    \"\"\"Scrapes SEC litigation releases for pump-and-dump enforcement actions.\n",
    "    \n",
    "    Sources:\n",
    "    - SEC Litigation Releases: sec.gov/litigation/litreleases.htm\n",
    "    - SEC Press Releases: sec.gov/news/pressreleases\n",
    "    - Administrative Proceedings: sec.gov/litigation/admin.htm\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keywords indicating pump-and-dump or market manipulation\n",
    "    MANIPULATION_KEYWORDS = [\n",
    "        'pump and dump', 'pump-and-dump', 'market manipulation',\n",
    "        'manipulative trading', 'touting', 'promotional campaign',\n",
    "        'artificially inflate', 'artificially inflated',\n",
    "        'scalping', 'front running', 'spoofing',\n",
    "        'wash trading', 'matched orders', 'marking the close',\n",
    "        'penny stock', 'microcap fraud', 'stock promotion scheme',\n",
    "        'social media manipulation', 'coordinated trading'\n",
    "    ]\n",
    "    \n",
    "    def __init__(self, config: ResearchConfig):\n",
    "        self.config = config\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': config.USER_AGENT,\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "        })\n",
    "        self.enforcement_cases = []\n",
    "        \n",
    "    def _rate_limit(self):\n",
    "        \"\"\"Implement polite rate limiting.\"\"\"\n",
    "        time.sleep(random.uniform(self.config.MIN_DELAY, self.config.MAX_DELAY))\n",
    "    \n",
    "    def scrape_litigation_releases_index(self, year: int) -> List[Dict]:\n",
    "        \"\"\"Scrape litigation releases index for a given year.\n",
    "        \n",
    "        Args:\n",
    "            year: Year to scrape\n",
    "            \n",
    "        Returns:\n",
    "            List of release metadata dictionaries\n",
    "        \"\"\"\n",
    "        releases = []\n",
    "        \n",
    "        # SEC changed URL structure over time\n",
    "        if year >= 2020:\n",
    "            url = f\"https://www.sec.gov/litigation/litreleases/litrelarchive/litarchive{year}.htm\"\n",
    "        else:\n",
    "            url = f\"https://www.sec.gov/litigation/litreleases/litrelarchive/litarchive{year}.htm\"\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'lxml')\n",
    "            \n",
    "            # Find all release links\n",
    "            # SEC uses various HTML structures - try multiple selectors\n",
    "            links = soup.find_all('a', href=re.compile(r'/litigation/litreleases/'))\n",
    "            \n",
    "            for link in links:\n",
    "                href = link.get('href', '')\n",
    "                text = link.get_text(strip=True)\n",
    "                \n",
    "                # Extract release number from URL\n",
    "                match = re.search(r'lr(\\d+)', href)\n",
    "                if match:\n",
    "                    releases.append({\n",
    "                        'release_number': match.group(1),\n",
    "                        'url': 'https://www.sec.gov' + href if href.startswith('/') else href,\n",
    "                        'title': text,\n",
    "                        'year': year\n",
    "                    })\n",
    "            \n",
    "            print(f\"Year {year}: Found {len(releases)} litigation releases\")\n",
    "            \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error scraping year {year}: {e}\")\n",
    "        \n",
    "        self._rate_limit()\n",
    "        return releases\n",
    "    \n",
    "    def scrape_release_content(self, url: str) -> Dict:\n",
    "        \"\"\"Scrape the full content of a litigation release.\n",
    "        \n",
    "        Args:\n",
    "            url: URL of the litigation release\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with release content and extracted metadata\n",
    "        \"\"\"\n",
    "        content = {\n",
    "            'url': url,\n",
    "            'full_text': '',\n",
    "            'date': None,\n",
    "            'tickers_mentioned': [],\n",
    "            'companies_mentioned': [],\n",
    "            'is_manipulation_case': False,\n",
    "            'manipulation_type': [],\n",
    "            'defendants': []\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'lxml')\n",
    "            \n",
    "            # Extract main content\n",
    "            # SEC uses different structures - try multiple selectors\n",
    "            main_content = soup.find('div', {'id': 'main-content'})\n",
    "            if not main_content:\n",
    "                main_content = soup.find('div', {'class': 'article-content'})\n",
    "            if not main_content:\n",
    "                main_content = soup.find('body')\n",
    "            \n",
    "            if main_content:\n",
    "                content['full_text'] = main_content.get_text(separator=' ', strip=True)\n",
    "            \n",
    "            # Extract date\n",
    "            date_match = re.search(r'(\\w+ \\d{1,2}, \\d{4})', content['full_text'])\n",
    "            if date_match:\n",
    "                try:\n",
    "                    content['date'] = datetime.strptime(date_match.group(1), '%B %d, %Y').date()\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            \n",
    "            # Extract ticker symbols (pattern: uppercase letters in parentheses or with $)\n",
    "            # Common patterns: (NASDAQ: XXXX), (OTC: XXXX), (NYSE: XXX), stock symbol XXXX\n",
    "            ticker_patterns = [\n",
    "                r'\\((?:NASDAQ|NYSE|OTC|OTCBB|OTC Markets)[:\\s]+([A-Z]{1,5})\\)',\n",
    "                r'stock symbol[:\\s]+([A-Z]{1,5})',\n",
    "                r'ticker symbol[:\\s]+([A-Z]{1,5})',\n",
    "                r'trading under[:\\s]+([A-Z]{1,5})',\n",
    "                r'\\$([A-Z]{1,5})\\b'\n",
    "            ]\n",
    "            \n",
    "            for pattern in ticker_patterns:\n",
    "                matches = re.findall(pattern, content['full_text'])\n",
    "                content['tickers_mentioned'].extend(matches)\n",
    "            \n",
    "            content['tickers_mentioned'] = list(set(content['tickers_mentioned']))\n",
    "            \n",
    "            # Check for manipulation keywords\n",
    "            text_lower = content['full_text'].lower()\n",
    "            for keyword in self.MANIPULATION_KEYWORDS:\n",
    "                if keyword in text_lower:\n",
    "                    content['is_manipulation_case'] = True\n",
    "                    content['manipulation_type'].append(keyword)\n",
    "            \n",
    "            content['manipulation_type'] = list(set(content['manipulation_type']))\n",
    "            \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error scraping {url}: {e}\")\n",
    "        \n",
    "        self._rate_limit()\n",
    "        return content\n",
    "    \n",
    "    def scrape_all_years(self, start_year: int = 2019, end_year: int = 2025) -> pd.DataFrame:\n",
    "        \"\"\"Scrape all litigation releases for the sample period.\n",
    "        \n",
    "        Args:\n",
    "            start_year: First year to scrape\n",
    "            end_year: Last year to scrape\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame of manipulation cases\n",
    "        \"\"\"\n",
    "        all_releases = []\n",
    "        \n",
    "        # First, collect all release URLs\n",
    "        print(\"Phase 1: Collecting litigation release URLs...\")\n",
    "        for year in range(start_year, end_year + 1):\n",
    "            releases = self.scrape_litigation_releases_index(year)\n",
    "            all_releases.extend(releases)\n",
    "        \n",
    "        print(f\"\\nTotal releases found: {len(all_releases)}\")\n",
    "        \n",
    "        # Phase 2: Scrape each release for manipulation cases\n",
    "        print(\"\\nPhase 2: Scraping individual releases (this will take time)...\")\n",
    "        manipulation_cases = []\n",
    "        \n",
    "        for release in tqdm(all_releases, desc=\"Scraping releases\"):\n",
    "            content = self.scrape_release_content(release['url'])\n",
    "            \n",
    "            if content['is_manipulation_case']:\n",
    "                case = {\n",
    "                    'release_number': release['release_number'],\n",
    "                    'release_url': release['url'],\n",
    "                    'release_title': release['title'],\n",
    "                    'release_year': release['year'],\n",
    "                    'release_date': content['date'],\n",
    "                    'tickers': content['tickers_mentioned'],\n",
    "                    'manipulation_types': content['manipulation_type'],\n",
    "                    'full_text': content['full_text'][:5000]  # Truncate for storage\n",
    "                }\n",
    "                manipulation_cases.append(case)\n",
    "                self.enforcement_cases.append(case)\n",
    "        \n",
    "        df = pd.DataFrame(manipulation_cases)\n",
    "        print(f\"\\nFound {len(df)} manipulation-related enforcement cases\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def extract_ticker_date_labels(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Extract ticker-level labels from enforcement cases.\n",
    "        \n",
    "        Creates a lookup table: (ticker, date_range) -> enforcement case\n",
    "        \"\"\"\n",
    "        labels = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            for ticker in row['tickers']:\n",
    "                labels.append({\n",
    "                    'ticker': ticker,\n",
    "                    'enforcement_date': row['release_date'],\n",
    "                    'release_number': row['release_number'],\n",
    "                    'manipulation_types': row['manipulation_types'],\n",
    "                    'label': 1  # Confirmed manipulation\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(labels)\n",
    "\n",
    "\n",
    "# Initialize scraper\n",
    "sec_scraper = SECEnforcementScraper(config)\n",
    "print(\"SEC Enforcement Scraper initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXECUTE SEC SCRAPING\n",
    "# =============================================================================\n",
    "\n",
    "# Scrape SEC enforcement releases\n",
    "# NOTE: This can take 1-2 hours due to rate limiting\n",
    "\n",
    "print(\"Starting SEC enforcement scraping...\")\n",
    "print(\"This will take approximately 1-2 hours due to polite rate limiting.\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Extract start and end years from config\n",
    "start_year = int(config.START_DATE[:4])\n",
    "end_year = int(config.END_DATE[:4])\n",
    "\n",
    "# Scrape all years\n",
    "enforcement_df = sec_scraper.scrape_all_years(start_year, end_year)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SEC ENFORCEMENT SCRAPING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total manipulation cases: {len(enforcement_df)}\")\n",
    "if len(enforcement_df) > 0:\n",
    "    print(f\"Date range: {enforcement_df['release_date'].min()} to {enforcement_df['release_date'].max()}\")\n",
    "    print(f\"\\nManipulation types found:\")\n",
    "    all_types = [t for types in enforcement_df['manipulation_types'] for t in types]\n",
    "    type_counts = pd.Series(all_types).value_counts()\n",
    "    print(type_counts.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXTRACT TICKER-LEVEL LABELS\n",
    "# =============================================================================\n",
    "\n",
    "if len(enforcement_df) > 0:\n",
    "    # Create ticker-level labels\n",
    "    ticker_labels = sec_scraper.extract_ticker_date_labels(enforcement_df)\n",
    "    \n",
    "    print(\"Ticker-Level Labels:\")\n",
    "    print(f\"Total labeled tickers: {len(ticker_labels)}\")\n",
    "    print(f\"Unique tickers: {ticker_labels['ticker'].nunique()}\")\n",
    "    print(f\"\\nSample labels:\")\n",
    "    print(ticker_labels.head(10))\n",
    "else:\n",
    "    print(\"No enforcement cases found - using sample data for demonstration\")\n",
    "    # Create sample data for demonstration\n",
    "    ticker_labels = pd.DataFrame({\n",
    "        'ticker': ['XXXX', 'YYYY', 'ZZZZ'],\n",
    "        'enforcement_date': [datetime(2021, 6, 15).date(), \n",
    "                             datetime(2022, 3, 22).date(),\n",
    "                             datetime(2023, 11, 8).date()],\n",
    "        'release_number': ['25001', '25123', '25456'],\n",
    "        'manipulation_types': [['pump and dump'], ['touting'], ['market manipulation']],\n",
    "        'label': [1, 1, 1]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Universe Construction\n",
    "\n",
    "### 4.1 Build Ticker Universe from Multiple Sources\n",
    "\n",
    "Since we cannot access comprehensive listing databases, we build our universe iteratively:\n",
    "1. Seed from SEC enforcement tickers\n",
    "2. Expand via Yahoo Finance screeners\n",
    "3. Cross-reference OTC Markets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# UNIVERSE BUILDER\n",
    "# =============================================================================\n",
    "\n",
    "class UniverseBuilder:\n",
    "    \"\"\"Builds the stock universe for pump-and-dump research.\n",
    "    \n",
    "    Universe criteria:\n",
    "    - Market cap < $500M (small-cap focus)\n",
    "    - Price < $10 (penny stock territory)\n",
    "    - Average volume > 10,000 shares/day (tradeable)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: ResearchConfig):\n",
    "        self.config = config\n",
    "        self.universe = set()\n",
    "        self.ticker_metadata = {}\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({'User-Agent': config.USER_AGENT})\n",
    "        \n",
    "    def add_sec_enforcement_tickers(self, ticker_labels: pd.DataFrame):\n",
    "        \"\"\"Add tickers from SEC enforcement cases.\"\"\"\n",
    "        tickers = set(ticker_labels['ticker'].unique())\n",
    "        print(f\"Adding {len(tickers)} tickers from SEC enforcement cases\")\n",
    "        self.universe.update(tickers)\n",
    "        \n",
    "        for ticker in tickers:\n",
    "            self.ticker_metadata[ticker] = {\n",
    "                'source': 'sec_enforcement',\n",
    "                'is_confirmed_manipulation': True\n",
    "            }\n",
    "    \n",
    "    def add_known_meme_stocks(self):\n",
    "        \"\"\"Add known meme stocks and pump targets.\"\"\"\n",
    "        meme_stocks = {\n",
    "            # 2021 Meme Stock Saga\n",
    "            'GME': 'GameStop Corp',\n",
    "            'AMC': 'AMC Entertainment',\n",
    "            'BB': 'BlackBerry Limited',\n",
    "            'NOK': 'Nokia Corporation',\n",
    "            'BBBY': 'Bed Bath & Beyond',\n",
    "            'KOSS': 'Koss Corporation',\n",
    "            'EXPR': 'Express Inc',\n",
    "            'NAKD': 'Cenntro Electric',\n",
    "            \n",
    "            # Other Notable Pump Targets\n",
    "            'CLOV': 'Clover Health',\n",
    "            'WISH': 'ContextLogic Inc',\n",
    "            'WKHS': 'Workhorse Group',\n",
    "            'RIDE': 'Lordstown Motors',\n",
    "            'NKLA': 'Nikola Corporation',\n",
    "            'SPCE': 'Virgin Galactic',\n",
    "            'PLTR': 'Palantir Technologies',\n",
    "            'TLRY': 'Tilray Brands',\n",
    "            'SNDL': 'Sundial Growers',\n",
    "            \n",
    "            # 2024-2025 Notable Cases\n",
    "            'DJT': 'Trump Media & Technology',\n",
    "            'SMCI': 'Super Micro Computer',\n",
    "            'FFIE': 'Faraday Future',\n",
    "        }\n",
    "        \n",
    "        print(f\"Adding {len(meme_stocks)} known meme/pump stocks\")\n",
    "        \n",
    "        for ticker, name in meme_stocks.items():\n",
    "            self.universe.add(ticker)\n",
    "            if ticker not in self.ticker_metadata:\n",
    "                self.ticker_metadata[ticker] = {\n",
    "                    'source': 'known_meme_stock',\n",
    "                    'company_name': name,\n",
    "                    'is_confirmed_manipulation': False\n",
    "                }\n",
    "    \n",
    "    def scrape_yahoo_screener_smallcaps(self, max_pages: int = 10) -> List[str]:\n",
    "        \"\"\"Scrape small-cap stocks from Yahoo Finance screener.\n",
    "        \n",
    "        Note: Yahoo Finance screener has rate limits and may require\n",
    "        alternative approaches (e.g., using yfinance Ticker lists).\n",
    "        \"\"\"\n",
    "        tickers = []\n",
    "        \n",
    "        # Yahoo Finance doesn't have a direct screener API\n",
    "        # We'll use a list of known small-cap indexes/ETFs holdings as proxy\n",
    "        \n",
    "        # IWM (Russell 2000) and IWC (Russell Microcap) holdings approximation\n",
    "        small_cap_proxies = [\n",
    "            'IWM',   # iShares Russell 2000 ETF\n",
    "            'IWC',   # iShares Microcap ETF\n",
    "            'SLYV',  # SPDR S&P 600 Small Cap Value\n",
    "            'VBR',   # Vanguard Small-Cap Value\n",
    "        ]\n",
    "        \n",
    "        print(\"Note: Yahoo Finance screener requires workarounds.\")\n",
    "        print(\"Using ETF holdings as proxy for small-cap universe.\")\n",
    "        \n",
    "        return tickers\n",
    "    \n",
    "    def validate_tickers_with_yfinance(self, tickers: List[str], \n",
    "                                       batch_size: int = 50) -> pd.DataFrame:\n",
    "        \"\"\"Validate tickers and get metadata using yfinance.\n",
    "        \n",
    "        Args:\n",
    "            tickers: List of ticker symbols\n",
    "            batch_size: Number of tickers per batch\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with ticker metadata\n",
    "        \"\"\"\n",
    "        validated = []\n",
    "        \n",
    "        ticker_list = list(tickers)\n",
    "        batches = [ticker_list[i:i+batch_size] for i in range(0, len(ticker_list), batch_size)]\n",
    "        \n",
    "        print(f\"Validating {len(ticker_list)} tickers in {len(batches)} batches...\")\n",
    "        \n",
    "        for batch in tqdm(batches, desc=\"Validating tickers\"):\n",
    "            for ticker in batch:\n",
    "                try:\n",
    "                    stock = yf.Ticker(ticker)\n",
    "                    info = stock.info\n",
    "                    \n",
    "                    # Extract key metadata\n",
    "                    validated.append({\n",
    "                        'ticker': ticker,\n",
    "                        'company_name': info.get('longName', info.get('shortName', '')),\n",
    "                        'market_cap': info.get('marketCap', np.nan),\n",
    "                        'current_price': info.get('currentPrice', info.get('regularMarketPrice', np.nan)),\n",
    "                        'avg_volume': info.get('averageVolume', np.nan),\n",
    "                        'exchange': info.get('exchange', ''),\n",
    "                        'sector': info.get('sector', ''),\n",
    "                        'industry': info.get('industry', ''),\n",
    "                        'is_valid': True\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    validated.append({\n",
    "                        'ticker': ticker,\n",
    "                        'company_name': '',\n",
    "                        'market_cap': np.nan,\n",
    "                        'current_price': np.nan,\n",
    "                        'avg_volume': np.nan,\n",
    "                        'exchange': '',\n",
    "                        'sector': '',\n",
    "                        'industry': '',\n",
    "                        'is_valid': False\n",
    "                    })\n",
    "            \n",
    "            # Rate limiting\n",
    "            time.sleep(1)\n",
    "        \n",
    "        return pd.DataFrame(validated)\n",
    "    \n",
    "    def filter_universe(self, metadata_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Filter universe based on research criteria.\n",
    "        \n",
    "        Criteria:\n",
    "        - Market cap < $500M OR unknown (include penny stocks)\n",
    "        - Price < $10 OR unknown\n",
    "        - Average volume > 10,000 shares/day OR unknown\n",
    "        \"\"\"\n",
    "        df = metadata_df.copy()\n",
    "        \n",
    "        # Apply filters (allow NaN values through - might be valid stocks)\n",
    "        mask = (\n",
    "            (df['is_valid']) &\n",
    "            (\n",
    "                (df['market_cap'].isna()) | \n",
    "                (df['market_cap'] <= self.config.MAX_MARKET_CAP) |\n",
    "                (df['market_cap'] == 0)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        filtered = df[mask].copy()\n",
    "        \n",
    "        print(f\"\\nUniverse Filtering Results:\")\n",
    "        print(f\"  Original: {len(df)} tickers\")\n",
    "        print(f\"  Valid: {df['is_valid'].sum()} tickers\")\n",
    "        print(f\"  After filters: {len(filtered)} tickers\")\n",
    "        \n",
    "        return filtered\n",
    "    \n",
    "    def build_universe(self, ticker_labels: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Build complete universe.\n",
    "        \n",
    "        Args:\n",
    "            ticker_labels: DataFrame from SEC enforcement scraping\n",
    "            \n",
    "        Returns:\n",
    "            Final universe DataFrame with metadata\n",
    "        \"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"BUILDING STOCK UNIVERSE\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Step 1: Add SEC enforcement tickers\n",
    "        self.add_sec_enforcement_tickers(ticker_labels)\n",
    "        \n",
    "        # Step 2: Add known meme/pump stocks\n",
    "        self.add_known_meme_stocks()\n",
    "        \n",
    "        # Step 3: Validate all tickers\n",
    "        print(f\"\\nTotal candidate tickers: {len(self.universe)}\")\n",
    "        metadata_df = self.validate_tickers_with_yfinance(self.universe)\n",
    "        \n",
    "        # Step 4: Filter universe\n",
    "        final_universe = self.filter_universe(metadata_df)\n",
    "        \n",
    "        # Step 5: Add source information\n",
    "        final_universe['source'] = final_universe['ticker'].map(\n",
    "            lambda x: self.ticker_metadata.get(x, {}).get('source', 'other')\n",
    "        )\n",
    "        final_universe['is_confirmed_manipulation'] = final_universe['ticker'].map(\n",
    "            lambda x: self.ticker_metadata.get(x, {}).get('is_confirmed_manipulation', False)\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"UNIVERSE CONSTRUCTION COMPLETE\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Final universe size: {len(final_universe)} tickers\")\n",
    "        print(f\"Confirmed manipulation: {final_universe['is_confirmed_manipulation'].sum()} tickers\")\n",
    "        \n",
    "        return final_universe\n",
    "\n",
    "\n",
    "# Initialize builder\n",
    "universe_builder = UniverseBuilder(config)\n",
    "print(\"Universe Builder initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BUILD THE UNIVERSE\n",
    "# =============================================================================\n",
    "\n",
    "# Build universe using SEC labels\n",
    "universe_df = universe_builder.build_universe(ticker_labels)\n",
    "\n",
    "# Display universe summary\n",
    "print(\"\\nUniverse Summary:\")\n",
    "print(universe_df.describe())\n",
    "\n",
    "print(\"\\nSample of universe:\")\n",
    "print(universe_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Expand Universe with Additional Volatile Small-Caps\n",
    "\n",
    "To ensure we capture potential pump-and-dump candidates not yet in SEC enforcement, we add high-volatility small-caps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ADD HIGH-VOLATILITY PENNY STOCKS\n",
    "# =============================================================================\n",
    "\n",
    "# Additional small-cap/penny stocks known for high volatility\n",
    "# These are stocks commonly discussed in pump-and-dump contexts\n",
    "\n",
    "additional_volatile_stocks = [\n",
    "    # Recent high-volatility small caps\n",
    "    'MULN', 'BBIG', 'ATER', 'PROG', 'CENN', 'GNUS', 'SAVA', 'PHUN',\n",
    "    'DWAC', 'IRNT', 'OPAD', 'TMC', 'LIDR', 'PTRA', 'GOEV', 'ARVL',\n",
    "    'LCID', 'RIVN', 'FSR', 'HYLN', 'XL', 'BLNK', 'CHPT', 'QS',\n",
    "    \n",
    "    # OTC/Pink Sheet frequent movers (tickers may vary)\n",
    "    'EEENF', 'OZSC', 'ALPP', 'ABML', 'USMJ', 'HCMC', 'AITX', 'DPLS',\n",
    "    \n",
    "    # Cannabis sector (frequent pump targets)\n",
    "    'CGC', 'ACB', 'TLRY', 'HEXO', 'OGI', 'VFF', 'GRWG',\n",
    "    \n",
    "    # Biotech small caps\n",
    "    'OCGN', 'VXRT', 'INO', 'NVAX', 'SRNE', 'ATOS', 'CTRM',\n",
    "    \n",
    "    # SPACs and De-SPACs (common pump targets)\n",
    "    'PSTH', 'CCIV', 'IPOE', 'SOFI', 'IPOF', 'PSFE', 'UWMC',\n",
    "]\n",
    "\n",
    "print(f\"Adding {len(additional_volatile_stocks)} additional volatile stocks...\")\n",
    "\n",
    "# Validate and add to universe\n",
    "additional_metadata = universe_builder.validate_tickers_with_yfinance(additional_volatile_stocks)\n",
    "additional_filtered = universe_builder.filter_universe(additional_metadata)\n",
    "additional_filtered['source'] = 'volatile_smallcap'\n",
    "additional_filtered['is_confirmed_manipulation'] = False\n",
    "\n",
    "# Combine with main universe\n",
    "universe_df = pd.concat([universe_df, additional_filtered], ignore_index=True)\n",
    "universe_df = universe_df.drop_duplicates(subset=['ticker'], keep='first')\n",
    "\n",
    "print(f\"\\nExpanded universe size: {len(universe_df)} tickers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE OUTPUTS\n",
    "# =============================================================================\n",
    "\n",
    "def save_outputs(universe_df: pd.DataFrame, \n",
    "                 enforcement_df: pd.DataFrame,\n",
    "                 ticker_labels: pd.DataFrame,\n",
    "                 output_dir: str):\n",
    "    \"\"\"Save all outputs from Notebook 1.\"\"\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save universe\n",
    "    universe_path = os.path.join(output_dir, 'stock_universe.parquet')\n",
    "    universe_df.to_parquet(universe_path, index=False)\n",
    "    print(f\"Saved universe: {universe_path}\")\n",
    "    \n",
    "    # Save as CSV for inspection\n",
    "    universe_csv = os.path.join(output_dir, 'stock_universe.csv')\n",
    "    universe_df.to_csv(universe_csv, index=False)\n",
    "    print(f\"Saved universe CSV: {universe_csv}\")\n",
    "    \n",
    "    # Save SEC enforcement cases\n",
    "    if len(enforcement_df) > 0:\n",
    "        enforcement_path = os.path.join(output_dir, 'sec_enforcement_cases.parquet')\n",
    "        enforcement_df.to_parquet(enforcement_path, index=False)\n",
    "        print(f\"Saved enforcement cases: {enforcement_path}\")\n",
    "    \n",
    "    # Save ticker labels (ground truth)\n",
    "    labels_path = os.path.join(output_dir, 'ticker_manipulation_labels.parquet')\n",
    "    ticker_labels.to_parquet(labels_path, index=False)\n",
    "    print(f\"Saved ticker labels: {labels_path}\")\n",
    "    \n",
    "    # Save summary statistics\n",
    "    summary = {\n",
    "        'universe_size': len(universe_df),\n",
    "        'confirmed_manipulation_tickers': int(universe_df['is_confirmed_manipulation'].sum()),\n",
    "        'sec_enforcement_cases': len(enforcement_df) if len(enforcement_df) > 0 else 0,\n",
    "        'unique_labeled_tickers': ticker_labels['ticker'].nunique(),\n",
    "        'sources': universe_df['source'].value_counts().to_dict(),\n",
    "        'created_at': datetime.now().isoformat(),\n",
    "        'config': {\n",
    "            'start_date': config.START_DATE,\n",
    "            'end_date': config.END_DATE,\n",
    "            'max_market_cap': config.MAX_MARKET_CAP,\n",
    "            'max_price': config.MAX_PRICE\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    summary_path = os.path.join(output_dir, 'notebook01_summary.json')\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(f\"Saved summary: {summary_path}\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "# Save all outputs\n",
    "summary = save_outputs(\n",
    "    universe_df=universe_df,\n",
    "    enforcement_df=enforcement_df if 'enforcement_df' in dir() and len(enforcement_df) > 0 else pd.DataFrame(),\n",
    "    ticker_labels=ticker_labels,\n",
    "    output_dir=config.PROCESSED_DATA_PATH\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Summary:\")\n",
    "print(json.dumps(summary, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# NOTEBOOK 1 SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════════════════╗\n",
    "║         NOTEBOOK 1: UNIVERSE CONSTRUCTION & SEC SCRAPING COMPLETE            ║\n",
    "╚══════════════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "OUTPUT FILES:\n",
    "─────────────\n",
    "• stock_universe.parquet          - Complete ticker universe with metadata\n",
    "• stock_universe.csv              - CSV for inspection\n",
    "• sec_enforcement_cases.parquet   - SEC litigation releases (manipulation cases)\n",
    "• ticker_manipulation_labels.parquet - Ground truth labels (ticker, date, label)\n",
    "• notebook01_summary.json         - Summary statistics\n",
    "\n",
    "UNIVERSE COMPOSITION:\n",
    "─────────────────────\n",
    "• SEC enforcement tickers (confirmed manipulation)\n",
    "• Known meme stocks (potential manipulation)\n",
    "• High-volatility small caps (control group candidates)\n",
    "\n",
    "GROUND TRUTH LABELS:\n",
    "────────────────────\n",
    "• Label 1: Ticker + date range from SEC enforcement action\n",
    "• Label 0: To be assigned in Notebook 4 (high-volatility without enforcement)\n",
    "\n",
    "NEXT STEPS:\n",
    "───────────\n",
    "→ Notebook 2: Yahoo Finance Market Data Collection\n",
    "  - Scrape daily OHLCV data for universe\n",
    "  - Compute baseline statistics\n",
    "  - Identify price-volume anomalies\n",
    "\n",
    "IMPORTANT NOTES:\n",
    "────────────────\n",
    "1. SEC scraping respects rate limits - may take 1-2 hours\n",
    "2. Some tickers may be delisted - handle gracefully in downstream analysis\n",
    "3. Ground truth is incomplete - SEC enforcement is tip of iceberg\n",
    "4. Use PLS (Pump Likelihood Score) as continuous proxy in final analysis\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ENVIRONMENT INFO FOR REPRODUCIBILITY\n",
    "# =============================================================================\n",
    "\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "print(\"Environment Information:\")\n",
    "print(f\"  Python: {sys.version}\")\n",
    "print(f\"  Platform: {platform.platform()}\")\n",
    "print(f\"  Pandas: {pd.__version__}\")\n",
    "print(f\"  NumPy: {np.__version__}\")\n",
    "print(f\"  yfinance: {yf.__version__}\")\n",
    "print(f\"  Timestamp: {datetime.now().isoformat()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}